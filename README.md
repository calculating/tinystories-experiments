The [TinyStories](https://arxiv.org/abs/2305.07759) paper demonstrates how remarkably small transformer models can converge to temporally coherent story generation if trained on a highly compressed dataset that maximizes syntactic and logical content while limiting domain-specific knowledge. Here I'm experimenting with my own model running on the dataset.

With a small model trained for a few hours it already converges to somewhat grammatically correct generation on the instruct dataset.

**Prompt**
>Features:  dialogue
>Words:  watch, pasta, nosy
>Story:
>abby was so excited to go to the park. she was on her way when she noticed the pasta shop. she wanted to get a snack before the park. abby's mom told her to be careful with the money. abby

**Generation**
>with the money. abby ate it in the ladder was about him with water. one day, her mommy tried to the truck to balance. it. he encountered a little boy named timmy. as they saw a big pose!
