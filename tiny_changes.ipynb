{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python3 -m venv run --system-site-packages\n",
    "source run/bin/activate\n",
    "pip install wandb==0.14.0\n",
    "wandb login e5292edda95a11630042fdf943d60d2bbf749fcf\n",
    "pip install datasets\n",
    "pip install tokenizers\n",
    "pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "# pyt\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# data pipeline\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from typing import cast\n",
    "import math, random\n",
    "\n",
    "# tokenization\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# logging\n",
    "import os, argparse\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5cb8d3fd8454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mhyper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mhyper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "hyper = {\n",
    "    'vs': 2**13,\n",
    "    'ly': 4,\n",
    "    'hs': 768,\n",
    "    'ah': 4,\n",
    "    'cx': 512,\n",
    "    'lr': 1e-4,\n",
    "    'bs': 32,\n",
    "    'ac': 4,\n",
    "    'ep': 10,\n",
    "}\n",
    "\n",
    "hyper = argparse.Namespace(**hyper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/galen/.cache/huggingface/datasets/skeskinen___parquet/skeskinen--TinyStories-Instruct-hf-1f9111cb77858404/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f777908268407db028ab065c0554ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2476533\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 25028\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = cast(DatasetDict, load_dataset('skeskinen/TinyStories-Instruct-hf'))\n",
    "dataset['train'].set_format(type='torch', columns=['text'])\n",
    "dataset['train'].format['type']\n",
    "dataset['validation'].set_format(type='torch', columns=['text'])\n",
    "dataset['validation'].format['type']\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(BPE())\n",
    "tok.normalizer = Lowercase()\n",
    "tok.pre_tokenizer = ByteLevel()\n",
    "tok.decoder = ByteLevelDecoder()\n",
    "tok.post_processor = TemplateProcessing(single='$0 <|endoftext|>', special_tokens=[('<|endoftext|>', 1)],)\n",
    "tok.enable_truncation(max_length=hyper.cx)\n",
    "tok.enable_padding(pad_token='<pad>', length=hyper.cx)\n",
    "trainer = BpeTrainer(vocab_size=hyper.vs, initial_alphabet=ByteLevel.alphabet(), special_tokens=['<pad>', '<|endoftext|>', '\\n','Words: ', 'Features: ', 'Random sentence: ', 'Summary: ', 'Story: '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('tiny.json'): tok = Tokenizer.from_file('tiny.json')\n",
    "else: tok.train_from_iterator(dataset['train']['text'], trainer=trainer); tok.save('tiny.json')\n",
    "\n",
    "tok = PreTrainedTokenizerFast(tokenizer_object=tok)\n",
    "tok.pad_token = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    return tok(example['text'], truncation=True, max_length=hyper.cx, padding='max_length')\n",
    "\n",
    "if os.path.exists('train_dataset') and os.path.exists('valid_dataset'):\n",
    "    train = load_from_disk('train_dataset')\n",
    "    valid = load_from_disk('valid_dataset')\n",
    "else:\n",
    "    train = dataset['train'].map(tokenization, batched=True)\n",
    "    valid = dataset['validation'].map(tokenization, batched=True)\n",
    "    train.save_to_disk('train_dataset')\n",
    "    valid.save_to_disk('valid_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask'])\n",
    "train.format['type']\n",
    "valid.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask'])\n",
    "valid.format['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainl = DataLoader(train, batch_size=hyper.bs, shuffle=True)\n",
    "validl = DataLoader(valid, batch_size=hyper.bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = t.arange(max_len).unsqueeze(1)\n",
    "        div_term = t.exp(t.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = t.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = t.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = t.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "class trans(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inbed = nn.Embedding(hyper.vs, hyper.hs)\n",
    "        self.posit = PositionalEncoding(hyper.hs, hyper.cx)\n",
    "        self.think = nn.TransformerEncoderLayer(d_model=hyper.hs, nhead=hyper.ah, dim_feedforward=hyper.hs*4, activation='gelu')\n",
    "        self.thnkr = nn.TransformerEncoder(self.think, num_layers=hyper.ly)\n",
    "        self.speak = nn.Linear(hyper.hs, hyper.vs)\n",
    "        self.cmask= t.triu(t.ones(hyper.cx, hyper.cx) * float('-inf'), diagonal=1)\n",
    "    def forward(self, x, pask=None):\n",
    "        x = self.inbed(x) * (hyper.hs ** .5)\n",
    "        x = self.posit(x)\n",
    "        x = self.thnkr(x, is_causal=True, mask=pask if pask is not None else self.cmask)\n",
    "        return self.speak(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = wandb.init(\n",
    "#     project=\"tinystories\",\n",
    "#     config={\n",
    "#         \"learning_rate\": hyper.lr,\n",
    "#         \"epochs\": 1,\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35.4 million parameters in the model, plus 12.6 million embeddings parameters.\n"
     ]
    }
   ],
   "source": [
    "storytell = trans()\n",
    "\n",
    "print(f'There are {round((sum(p.numel() for p in storytell.parameters()) - hyper.vs*hyper.hs*2)/1e6, 1)} million parameters in the model, plus {round((hyper.vs*hyper.hs*2)/1e6, 1)} million embeddings parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You must call `wandb.init` before calling watch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wandb\u001b[39m.\u001b[39;49mwatch(storytell, log_freq\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/wandb/sdk/wandb_watch.py:54\u001b[0m, in \u001b[0;36mwatch\u001b[0;34m(models, criterion, log, log_freq, idx, log_graph)\u001b[0m\n\u001b[1;32m     51\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mWatching\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m wandb\u001b[39m.\u001b[39mrun \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou must call `wandb.init` before calling watch\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m log \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mgradients\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mparameters\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m}:\n\u001b[1;32m     57\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mlog must be one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mgradients\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mparameters\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, or None\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: You must call `wandb.init` before calling watch"
     ]
    }
   ],
   "source": [
    "# wandb.watch(storytell, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = t.optim.Adam(storytell.parameters(), lr=hyper.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossf = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[209], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     loss \u001b[39m=\u001b[39m lossf(t\u001b[39m.\u001b[39mflatten(out, end_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), t\u001b[39m.\u001b[39mflatten(t\u001b[39m.\u001b[39mroll(seq, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n\u001b[1;32m      8\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     loss \u001b[39m=\u001b[39m lossf(t\u001b[39m.\u001b[39;49mflatten(out, end_dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), t\u001b[39m.\u001b[39;49mflatten(t\u001b[39m.\u001b[39;49mroll(seq\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mmps\u001b[39;49m\u001b[39m'\u001b[39;49m)))\n\u001b[1;32m     10\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     12\u001b[0m \u001b[39mif\u001b[39;00m ((step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m hyper\u001b[39m.\u001b[39mac \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(trainl)):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/nn/functional.py:3015\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2949\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"This criterion computes the cross entropy loss between input logits and target.\u001b[39;00m\n\u001b[1;32m   2950\u001b[0m \n\u001b[1;32m   2951\u001b[0m \u001b[39mSee :class:`~torch.nn.CrossEntropyLoss` for details.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[39m    >>> loss.backward()\u001b[39;00m\n\u001b[1;32m   3013\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3014\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, target, weight):\n\u001b[0;32m-> 3015\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3016\u001b[0m         cross_entropy,\n\u001b[1;32m   3017\u001b[0m         (\u001b[39minput\u001b[39;49m, target, weight),\n\u001b[1;32m   3018\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   3019\u001b[0m         target,\n\u001b[1;32m   3020\u001b[0m         weight\u001b[39m=\u001b[39;49mweight,\n\u001b[1;32m   3021\u001b[0m         size_average\u001b[39m=\u001b[39;49msize_average,\n\u001b[1;32m   3022\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m   3023\u001b[0m         reduce\u001b[39m=\u001b[39;49mreduce,\n\u001b[1;32m   3024\u001b[0m         reduction\u001b[39m=\u001b[39;49mreduction,\n\u001b[1;32m   3025\u001b[0m         label_smoothing\u001b[39m=\u001b[39;49mlabel_smoothing,\n\u001b[1;32m   3026\u001b[0m     )\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/overrides.py:1534\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m \u001b[39mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1531\u001b[0m     \u001b[39m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m     \u001b[39m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m     \u001b[39mwith\u001b[39;00m _pop_mode_temporarily() \u001b[39mas\u001b[39;00m mode:\n\u001b[0;32m-> 1534\u001b[0m         result \u001b[39m=\u001b[39m mode\u001b[39m.\u001b[39;49m__torch_function__(public_api, types, args, kwargs)\n\u001b[1;32m   1535\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1536\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/utils/_device.py:62\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "step = 110001\n",
    "for epoch in range(hyper.ep):\n",
    "for batch in trainl:\n",
    "    step += 1\n",
    "    seq = batch['input_ids']\n",
    "    out = storytell(seq)\n",
    "    if hyper.cu:\n",
    "        loss = lossf(t.flatten(out, end_dim=1), t.flatten(t.roll(seq, -1)))\n",
    "    else:\n",
    "        loss = lossf(t.flatten(out, end_dim=1), t.flatten(t.roll(seq.to('cpu'), -1).to('mps')))\n",
    "    loss.backward()\n",
    "\n",
    "    if (step % hyper.ac == 0) or (step + 1 == len(trainl)):\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            # wandb.log({\"loss\": loss})\n",
    "            print(f'Step {step+1} of {len(trainl)}: loss {loss.item()}')\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        t.save(storytell.state_dict(), f'story_model_{step}.pt')\n",
    "        t.save(optim.state_dict(), f'story_optim_{step}.pt')\n",
    "    \n",
    "        with t.no_grad():\n",
    "            tloss = 0\n",
    "            steps = 0\n",
    "            storytell.eval()\n",
    "            for batch in validl:\n",
    "                seq = batch['input_ids'].to('cuda')\n",
    "                out = storytell(seq)\n",
    "                tloss += lossf(t.flatten(out, end_dim=1), t.flatten(t.roll(seq, -1))).item()\n",
    "                steps += 1\n",
    "            print(f'validation: loss {tloss/steps}')\n",
    "            storytell.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = t.arange(max_len).unsqueeze(1)\n",
    "        div_term = t.exp(t.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = t.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = t.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = t.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "class trans(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inbed = nn.Embedding(hyper.vs, hyper.hs)\n",
    "        self.posit = PositionalEncoding(hyper.hs, hyper.cx)\n",
    "        self.think = nn.TransformerEncoderLayer(d_model=hyper.hs, nhead=hyper.ah, dim_feedforward=hyper.hs*4, activation='gelu')\n",
    "        self.thnkr = nn.TransformerEncoder(self.think, num_layers=hyper.ly)\n",
    "        self.speak = nn.Linear(hyper.hs, hyper.vs)\n",
    "        self.cmask= t.triu(t.ones(hyper.cx, hyper.cx) * float('-inf'), diagonal=1)\n",
    "    def forward(self, x, pask=None):\n",
    "        x = self.inbed(x) * (hyper.hs ** .5)\n",
    "        x = self.posit(x)\n",
    "        x = self.thnkr(x, is_causal=True, mask=pask if pask is not None else self.cmask)\n",
    "        return self.speak(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "storytell = trans()\n",
    "optim = t.optim.Adam(storytell.parameters(), lr=hyper.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model state dict story_model_1000.pt\n",
    "storytell.load_state_dict(t.load('story_model_11000.pt', map_location=t.device('mps')))\n",
    "# load optim state dict\n",
    "# optim.load_state_dict(t.load('story_optim_4500.pt', map_location=t.device('mps')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3174\n",
      "Features:  conflict\n",
      "Summary:  dave the duck wanted to win a prize and encountered another duck who wouldn't let him have it. he convinced her to share the prize with him and they both enjoyed it.\n",
      "Random sentence:  she agreed and dave grabbed the prize proudly.\n",
      "Story:  \n",
      "\n",
      " once there lived a duck named dave. dave was excited to enter the new prize game. every day, he patiently waited for it to start.\n",
      "\n",
      " one day, dave saw a big prize near a pond. he wanted to make it his own. he moved closer to get it, but another duck swam in front of him. she wouldn't let him take the prize.\n",
      "\n",
      " dave tried to convince her by talking nicely to her. he thought this was the only way he could get the prize. but the other duck would not listen.\n",
      "\n",
      " finally, dave had an idea. he told the other duck that if she moved aside, he would share the warm prize with her. she agreed and dave grabbed the prize proudly. he and the duck shared the prize and both enjoyed it.<|endoftext|>00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "model gen:\n",
      " dialogue\n",
      "\n",
      " a was park. to the. time. said a day. loved't know's. a was\n",
      " was the mom the the park. her. said were laughed the was\n",
      "\n",
      " she was and said was the park..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " upon was in time. lily was\n",
      " was a to the, park friend..\n",
      " day, \" was. for a was the to\n",
      "\n",
      "\n",
      " day, \" was a time,. the time.\n",
      " was to the a was mom.\n",
      " was the to the the was \" he day. away the of the.\n",
      " was't know's. a park.\n",
      "\n",
      "\n",
      " was to the her mom a to. the mom\n",
      " was it time a park three to was not the park.\n",
      " he park.. be want to\n",
      "\n",
      "\n",
      ", \" was a old.\n",
      " was her park.. day he was the. \" was be the park and. her mom\n",
      " was and said was the park..\n",
      " was said park. the park. said laughed the was\n",
      "000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx = random.randint(0, len(valid) - 1)\n",
    "print(idx)\n",
    "\n",
    "# print the original text\n",
    "print(tok.decode(valid['input_ids'][idx]))\n",
    "\n",
    "# generate a story using the model\n",
    "print('model gen:')\n",
    "print(tok.decode(storytell(valid['input_ids'][idx].unsqueeze(0).to(t.long)).argmax(dim=-1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  dialogue\n",
      "Words:  watch, pasta, nosy\n",
      "Story: \n",
      "\n",
      " abby was so excited to go to the park. she was on her way when she noticed the pasta shop. she wanted to get a snack before the park.\n",
      "\n",
      " abby's mom told her to be careful with the money. abby\n",
      "torch.Size([1, 60])\n",
      " with the money. abby ate it in the ladder was about him with water. one day, her mommy tried to the truck to balance. it. he encountered a little boy named timmy. as they saw a big pose! i can listen carefully this movie could, there was a big open the little seed felt good idea, \"because i could fly and lay down. you and dance in the balls and bob loved to buy a big box. she found a boy, \"the moon, the little girl was to the sun that she loved paper otter was so glad to play with a mean children and unique journey home and welcomed jack was told her tomatoes and kept flying high for the wet leaf! \n",
      "Story:  once upon\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_text(prompt, model, tokenizer, temperature=1.0, max_len=512):\n",
    "    model.eval()\n",
    "    with t.no_grad():\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to('mps')[:, :-1]\n",
    "        print(input_ids.shape)\n",
    "        cur_len = input_ids.shape[1]\n",
    "        while cur_len < max_len:\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs[0][-1, :] / temperature\n",
    "            next_token_logits[1] = -float('inf')\n",
    "            next_token_id = t.multinomial(t.softmax(next_token_logits, dim=-1), num_samples=1).unsqueeze(-1)\n",
    "            input_ids = t.cat([input_ids, next_token_id], dim=1)\n",
    "            cur_len += 1\n",
    "            if next_token_id[0][0] == tokenizer.eos_token_id:\n",
    "                break\n",
    "        return tokenizer.decode(input_ids.squeeze()[55:], skip_special_tokens=False)\n",
    "    \n",
    "idx = random.randint(0, len(valid) - 1)\n",
    "prompt = tok.decode(valid['input_ids'][idx][:60])\n",
    "print(prompt)\n",
    "print(generate_text(prompt, storytell, tok, temperature=1, max_len=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
