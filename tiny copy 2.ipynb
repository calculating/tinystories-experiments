{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pickle\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import DatasetDict\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "t.set_default_device('mps')\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<pad>\"]})\n",
    "tokenizer.pad_token = \"<pad>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset:DatasetDict = load_dataset(\"roneneldan/TinyStoriesInstruct\") # type: ignore\n",
    "\n",
    "for i in range(16):\n",
    "    print(dataset['train'][i]['text'])\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenized train data\n",
      "train set loaded\n",
      "loading tokenized test data\n",
      "test set loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dat_tok_train:DatasetDict = None # type: ignore\n",
    "dat_tok_test:DatasetDict = None # type: ignore\n",
    "\n",
    "if os.path.exists('dataset_train_tokenized.txt'):\n",
    "    print('loading tokenized train data')\n",
    "    with open('dataset_train_tokenized.txt', 'rb') as input_file:\n",
    "        dat_tok_train = pickle.load(input_file)\n",
    "    print('train set loaded')\n",
    "else:\n",
    "    print('tokenizing train set')\n",
    "    dat_tok_train = tokenizer(dataset['train']['text'], return_attention_mask=False)\n",
    "    print('done tokenizing train set, saving')\n",
    "\n",
    "    with open('dataset_train_tokenized.txt', 'wb') as output_file:\n",
    "        pickle.dump(dat_tok_train, output_file)\n",
    "    print('done saving train set')\n",
    "\n",
    "if os.path.exists('dataset_test_tokenized.txt'):\n",
    "    print('loading tokenized test data')\n",
    "    with open('dataset_test_tokenized.txt', 'rb') as input_file:\n",
    "        dat_tok_test = pickle.load(input_file)\n",
    "    print('test set loaded')\n",
    "else:\n",
    "    print('tokenizing test set')\n",
    "    dat_tok_test = tokenizer(dataset['train']['text'], return_attention_mask=False)\n",
    "    print('done tokenizing test set, saving')\n",
    "\n",
    "    with open('dataset_test_tokenized.txt', 'wb') as output_file:\n",
    "        pickle.dump(dat_tok_test, output_file)\n",
    "    print('done saving test set')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23595, 25, 34709], [37117, 25, 11238, 11, 26210, 11, 46400], [22093, 25, 24799, 290, 3932, 547, 2712, 287, 262, 3952, 11, 475, 24799, 2227, 284, 467, 1363, 780, 340, 373, 4692, 290, 3223, 13, 3932, 9431, 607, 284, 2652, 290, 711, 11, 475, 4191, 4987, 284, 467, 1363, 290, 423, 3024, 35845, 13], [11605, 25, 220]]\n",
      "['Features: Dialogue', 'Words: quit, oak, gloomy', 'Summary: Sara and Ben were playing in the park, but Sara wanted to go home because it was cold and dark. Ben convinced her to stay and play, but eventually agreed to go home and have hot cocoa.', 'Story: ']\n",
      "21755681\n",
      "218380\n"
     ]
    }
   ],
   "source": [
    "print(dat_tok_train['input_ids'][:4])\n",
    "print(tokenizer.batch_decode(dat_tok_train['input_ids'][:4]))\n",
    "\n",
    "print(len(dat_tok_train['input_ids']))\n",
    "print(len(dat_tok_test['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading clustered train data\n",
      "train set loaded\n",
      "loading clustered test data\n",
      "test set loaded\n"
     ]
    }
   ],
   "source": [
    "def doc_cluster(tokenized_data, eot_token_id=50256, linebreak_token_id=198):\n",
    "    clustered, cluster = [], []\n",
    "    for sample in tokenized_data['input_ids']:\n",
    "        for token in sample:\n",
    "            if token == eot_token_id:\n",
    "                cluster.append(token)\n",
    "                clustered.append(cluster)\n",
    "                cluster = []\n",
    "                break\n",
    "            else:\n",
    "                cluster.append(token)\n",
    "        cluster.append(linebreak_token_id)\n",
    "\n",
    "    return clustered\n",
    "\n",
    "tcl_train:list = None # type: ignore\n",
    "tcl_test:list = None # type: ignore\n",
    "if os.path.exists('dataset_train_clustered.txt'):\n",
    "    print('loading clustered train data')\n",
    "    with open('dataset_train_clustered.txt', 'rb') as input_file:\n",
    "        tcl_train = pickle.load(input_file)\n",
    "    print('train set loaded')\n",
    "else:\n",
    "    tcl_train = doc_cluster(dat_tok_train)\n",
    "    print('clustered set generated, saving')\n",
    "    with open('dataset_train_clustered.txt', 'wb') as output_file:\n",
    "        pickle.dump(tcl_train, output_file)\n",
    "    print('done saving train set')\n",
    "    \n",
    "if os.path.exists('dataset_test_clustered.txt'):\n",
    "    print('loading clustered test data')\n",
    "    with open('dataset_test_clustered.txt', 'rb') as input_file:\n",
    "        tcl_test = pickle.load(input_file)\n",
    "    print('test set loaded')\n",
    "else:\n",
    "    tcl_test = doc_cluster(dat_tok_test)\n",
    "    print('clustered set generated, saving')\n",
    "    with open('dataset_test_clustered.txt', 'wb') as output_file:\n",
    "        pickle.dump(tcl_test, output_file)\n",
    "    print('done saving test set')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2476532 25027\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(tcl_train), len(tcl_test))\n",
    "sl_train = [len(seq) for seq in tcl_train]\n",
    "sl_test = [len(seq) for seq in tcl_test]\n",
    "\n",
    "plt.hist([sl_train, sl_test], bins=50, density=True)\n",
    "plt.title(\"Sequence Length Distribution\")\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words: twist, pilot, angry\n",
      "Summary: A pilot's plane starts to twist and turn, but he discovers it's because a little bird is playing with it. They fly together for a while before the bird returns to its nest.\n",
      "Story: Once upon a time, there was a big plane in the sky. The pilot was flying it. Suddenly, the plane started to twist and turn. The pilot got very angry because he didn't know what was happening. \n",
      "Then, he saw a little bird flying near the plane. The bird was trying to play with the plane. The pilot smiled and wasn't angry anymore. He knew the bird was just having fun. \n",
      "The plane continued to twist and turn, but this time the pilot was happy. He knew it was because the bird was playing with the plane. They flew together for a while, until it was time for the bird to go back to its nest. The pilot said goodbye to the little bird and continued on his way.\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tcl_train[1000000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train set tensor data\n",
      "train set loaded\n",
      "loading test set tensor data\n",
      "test set loaded\n"
     ]
    }
   ],
   "source": [
    "train_t, test_t = None, None # type: ignore\n",
    "if os.path.exists('dataset_train_tensor.txt'):\n",
    "    print('loading train set tensor data')\n",
    "    with open('dataset_train_tensor.txt', 'rb') as input_file:\n",
    "        train_t:t.Tensor = pickle.load(input_file)\n",
    "    print('train set loaded')\n",
    "else:\n",
    "    print('padding train set')\n",
    "    train_t:t.Tensor = t.nn.utils.rnn.pad_sequence([t.tensor(seq[:512]) for seq in tcl_train], batch_first=True, padding_value=50257)\n",
    "    print('done padding train set, saving')\n",
    "\n",
    "    with open('dataset_train_tensor.txt', 'wb') as output_file:\n",
    "        pickle.dump(train_t, output_file)\n",
    "    print('done saving train set')\n",
    "\n",
    "    \n",
    "if os.path.exists('dataset_test_tensor.txt'):\n",
    "    print('loading test set tensor data')\n",
    "    with open('dataset_test_tensor.txt', 'rb') as input_file:\n",
    "        test_t:t.Tensor = pickle.load(input_file)\n",
    "    print('test set loaded')\n",
    "else:\n",
    "    print('padding test set')\n",
    "    test_t:t.Tensor = t.nn.utils.rnn.pad_sequence([t.tensor(seq[:512]) for seq in tcl_test], batch_first=True, padding_value=50257)\n",
    "    print('done padding test set, saving')\n",
    "\n",
    "    with open('dataset_test_tensor.txt', 'wb') as output_file:\n",
    "        pickle.dump(test_t, output_file)\n",
    "    print('done saving test set')\n",
    "\n",
    "train_t = train_t.to('mps').to(t.int32)\n",
    "test_t = test_t.to('mps').to(t.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2476532, 512]) torch.Size([25027, 512])\n"
     ]
    }
   ],
   "source": [
    "print(train_t.shape, test_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary: A dog named Max stumbles near a statue of a strong man in the park, and the statue comes to life to help him up. The strong man teaches Max the importance of getting back up and asking for help when needed.\n",
      "Words: stumble, statue, mighty\n",
      "Features: MoralValue, Twist\n",
      "Story: Once upon a time, there was a mighty dog named Max. Max loved to run and play in the park. One day, Max saw a big statue in the park. The statue was of a man who was very strong.\n",
      "While Max was playing near the statue, he started to stumble on the rocks. Max was sad because he could not run and play like before. Then, something unexpected happened. The statue came to life! The strong man from the statue helped Max stand up.\n",
      "The strong man told Max that it is okay to stumble and fall. The important thing is to always get back up and keep trying. Max learned that it is good to ask for help when you need it. From that day on, Max and the strong man were best friends. They played in the park together and helped each other when they stumbled.\n",
      "<|endoftext|> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_t[1256436]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38697, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "batch_pad_train = t.zeros((128 - train_t.shape[0] % 64, 512), dtype=t.long)\n",
    "btrain = t.cat((train_t, batch_pad_train), dim=0).view(-1, 64, 512)\n",
    "print(btrain.shape)\n",
    "del train_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random sentence: Her mom said, \"Of course!\n",
      "Features: Dialogue\n",
      "Words: pack, neck, rich\n",
      "Summary: Lily's family goes on a trip to the beach and packs everything they need, including a neck pillow and a picnic lunch.\n",
      "Story: Once upon a time, there was a little girl named Lily. She had a rich family and they lived in a big house. One day, Lily's mom told her they were going on a trip to the beach. Lily was so excited and asked her mom, \"Can I pack my toys?\" Her mom said, \"Of course! Just make sure you don't forget anything.\"\n",
      "As they were getting ready to leave, Lily's dad noticed that his neck was hurting. He said, \"I think I need to pack my neck pillow for the car ride.\" Lily asked him, \"What's a neck pillow?\" Her dad showed her and Lily said, \"Oh, that looks comfy!\"\n",
      "On the way to the beach, Lily and her family sang songs and played games. When they finally arrived, Lily couldn't wait to play in the sand. She asked her mom, \"Can we pack a picnic for lunch?\" Her mom said, \"Sure, let's go pack some sandwiches and fruit.\"\n",
      "As they were eating their lunch, Lily's dad said, \"This is the best day ever.\" Lily agreed and said, \"I'm so happy we packed everything we needed.\"\n",
      "<|endoftext|> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(btrain[30000][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('dataset_train_tensor_batched.txt', 'wb') as output_file:\n",
    "    pickle.dump(btrain, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters in the storier module is 32357458\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class trans(nn.Module):\n",
    "    def __init__(self, vocab=50257, hi=256, len=512):\n",
    "        super().__init__()\n",
    "        self.inbed = nn.Embedding(vocab, hi)\n",
    "        self.think = nn.TransformerEncoderLayer(d_model=hi, nhead=4, dim_feedforward=512*4, activation='gelu')\n",
    "        self.thinker = nn.TransformerEncoder(self.think, num_layers=4)\n",
    "        self.out = nn.Linear(hi, vocab+1)\n",
    "        self.mask= t.triu(t.ones(len, len), diagonal=1).bool()\n",
    "    def forward(self, x):\n",
    "        x = self.inbed(x)\n",
    "        x = self.thinker(x, mask=self.mask, is_causal=True)\n",
    "        return self.out(x)\n",
    "    \n",
    "storier = trans()\n",
    "\n",
    "print(f\"The number of parameters in the storier module is {sum(p.numel() for p in storier.parameters())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: Dialogue\n",
      "Words: quit, oak, gloomy\n",
      "Summary: Sara and Ben were playing in the park, but Sara wanted to go home because it was cold and dark. Ben convinced her to stay and play, but eventually agreed to go home and have hot cocoa.\n",
      "Story: \n",
      "\n",
      "Sara and Ben were playing in the park. They liked to climb the big oak tree and pretend they were birds. They made nests with leaves and twigs and sang songs.\n",
      "But today, the sky was gloomy and the wind was cold. Sara felt sad and cold. She wanted to go home and have some hot cocoa.\n",
      "\"Ben, I want to quit,\" she said. \"It's too cold and dark. Let's go home.\"\n",
      "Ben looked at Sara and frowned. He liked the oak tree and the park. He wanted to stay and play.\n",
      "\"No, Sara, don't quit,\" he said. \"It's fun here. Look, there's a squirrel. Let's chase it.\"\n",
      "Sara shook her head. She didn't want to chase the squirrel. She wanted to go home and have some hot cocoa.\n",
      "\"Please, Ben, let's go home,\" she said. \"We can play here another day. I'm cold and hungry.\"\n",
      "Ben saw that Sara was shivering and looked unhappy. He loved his sister and didn't want her to be sad. He nodded and smiled.\n",
      "\"Okay, Sara, let's go home,\" he said. \"We can have some hot cocoa and cookies. And we can play with our toys.\"\n",
      "Sara hugged Ben and thanked him. They climbed down the oak tree and ran to their mom, who was waiting for them. They told her about their adventure and asked for some hot cocoa and cookies. Mom smiled and agreed. She was proud of her children for being brave and kind. They went home and had a cozy and happy time.\n",
      "<|endoftext|> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(train_t[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storier.load_state_dict(t.load('fixedloss_17200.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[31085,    13,   679,  ..., 50257, 50257, 50257],\n",
       "        [  198, 29531,  6827,  ..., 50257, 50257, 50257],\n",
       "        [  198, 23595,    25,  ..., 50257, 50257, 50257],\n",
       "        ...,\n",
       "        [  198, 37117,    25,  ..., 50257, 50257, 50257],\n",
       "        [  198, 22093,    25,  ..., 50257, 50257, 50257],\n",
       "        [  198, 22093,    25,  ..., 50257, 50257, 50257]], device='mps:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btrain.requires_grad_(False)\n",
    "test_t.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = t.optim.Adam(storier.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [17730/38000] Loss: 3.648719310760498\n",
      "Step [17731/38000] Loss: 3.309008836746216\n",
      "Step [17732/38000] Loss: 3.7712197303771973\n",
      "Step [17733/38000] Loss: 3.705920696258545\n",
      "Step [17734/38000] Loss: 3.721378803253174\n",
      "Step [17735/38000] Loss: 3.6202163696289062\n",
      "Step [17736/38000] Loss: 3.716981887817383\n",
      "Step [17737/38000] Loss: 3.133007764816284\n",
      "Step [17738/38000] Loss: 3.776144027709961\n",
      "Step [17739/38000] Loss: 3.4784858226776123\n",
      "Step [17740/38000] Loss: 3.7535343170166016\n",
      "Step [17741/38000] Loss: 3.733189105987549\n",
      "Step [17742/38000] Loss: 3.8492331504821777\n",
      "Step [17743/38000] Loss: 3.796008586883545\n",
      "Step [17744/38000] Loss: 3.0975327491760254\n",
      "Step [17745/38000] Loss: 3.7946412563323975\n",
      "Step [17746/38000] Loss: 3.858576774597168\n",
      "Step [17747/38000] Loss: 3.643588066101074\n",
      "Step [17748/38000] Loss: 3.7919788360595703\n",
      "Step [17749/38000] Loss: 3.863335609436035\n",
      "Step [17750/38000] Loss: 3.388437271118164\n",
      "Step [17751/38000] Loss: 3.873419761657715\n",
      "Step [17752/38000] Loss: 3.7568607330322266\n",
      "Step [17753/38000] Loss: 3.5116052627563477\n",
      "Step [17754/38000] Loss: 3.6006674766540527\n",
      "Step [17755/38000] Loss: 3.5388355255126953\n",
      "Step [17756/38000] Loss: 3.228909492492676\n",
      "Step [17757/38000] Loss: 3.5482399463653564\n",
      "Step [17758/38000] Loss: 3.568897247314453\n",
      "Step [17759/38000] Loss: 3.623239040374756\n",
      "Step [17760/38000] Loss: 3.742981195449829\n",
      "Step [17761/38000] Loss: 3.6748695373535156\n",
      "Step [17762/38000] Loss: 3.741396427154541\n",
      "Step [17763/38000] Loss: 3.7353315353393555\n",
      "Step [17764/38000] Loss: 3.6879591941833496\n",
      "Step [17765/38000] Loss: 3.610600471496582\n",
      "Step [17766/38000] Loss: 3.7886462211608887\n",
      "Step [17767/38000] Loss: 3.521636962890625\n",
      "Step [17768/38000] Loss: 3.615023136138916\n",
      "Step [17769/38000] Loss: 3.700136661529541\n",
      "Step [17770/38000] Loss: 3.679234504699707\n",
      "Step [17771/38000] Loss: 3.6355767250061035\n",
      "Step [17772/38000] Loss: 3.5192723274230957\n",
      "Step [17773/38000] Loss: 3.499950647354126\n",
      "Step [17774/38000] Loss: 3.509084463119507\n",
      "Step [17775/38000] Loss: 3.3067002296447754\n",
      "Step [17776/38000] Loss: 3.514462471008301\n",
      "Step [17777/38000] Loss: 3.4943318367004395\n",
      "Step [17778/38000] Loss: 3.4809212684631348\n",
      "Step [17779/38000] Loss: 3.5477967262268066\n",
      "Step [17780/38000] Loss: 3.8247785568237305\n",
      "Step [17781/38000] Loss: 2.7731332778930664\n",
      "Step [17782/38000] Loss: 3.3512349128723145\n",
      "Step [17783/38000] Loss: 3.499001979827881\n",
      "Step [17784/38000] Loss: 3.6676836013793945\n",
      "Step [17785/38000] Loss: 3.322709798812866\n",
      "Step [17786/38000] Loss: 3.3426244258880615\n",
      "Step [17787/38000] Loss: 3.1393027305603027\n",
      "Step [17788/38000] Loss: 3.874807357788086\n",
      "Step [17789/38000] Loss: 3.3537960052490234\n",
      "Step [17790/38000] Loss: 3.4854514598846436\n",
      "Step [17791/38000] Loss: 3.5119357109069824\n",
      "Step [17792/38000] Loss: 3.5235061645507812\n",
      "Step [17793/38000] Loss: 3.6573262214660645\n",
      "Step [17794/38000] Loss: 3.165534019470215\n",
      "Step [17795/38000] Loss: 3.3100147247314453\n",
      "Step [17796/38000] Loss: 3.3587288856506348\n",
      "Step [17797/38000] Loss: 3.6115074157714844\n",
      "Step [17798/38000] Loss: 3.722632646560669\n",
      "Step [17799/38000] Loss: 2.873464822769165\n",
      "Step [17800/38000] Loss: 3.478693962097168\n",
      "Test Loss: 2.9668493270874023\n",
      "Step [17801/38000] Loss: 3.5036211013793945\n",
      "Step [17802/38000] Loss: 3.463848114013672\n",
      "Step [17803/38000] Loss: 3.480619430541992\n",
      "Step [17804/38000] Loss: 3.5540316104888916\n",
      "Step [17805/38000] Loss: 3.5621285438537598\n",
      "Step [17806/38000] Loss: 3.4984567165374756\n",
      "Step [17807/38000] Loss: 3.5945162773132324\n",
      "Step [17808/38000] Loss: 3.45460844039917\n",
      "Step [17809/38000] Loss: 3.4686970710754395\n",
      "Step [17810/38000] Loss: 3.3019256591796875\n",
      "Step [17811/38000] Loss: 3.2909555435180664\n",
      "Step [17812/38000] Loss: 3.5870068073272705\n",
      "Step [17813/38000] Loss: 3.713740348815918\n",
      "Step [17814/38000] Loss: 3.4150261878967285\n",
      "Step [17815/38000] Loss: 3.4279637336730957\n",
      "Step [17816/38000] Loss: 3.392000198364258\n",
      "Step [17817/38000] Loss: 3.7515721321105957\n",
      "Step [17818/38000] Loss: 3.4173927307128906\n",
      "Step [17819/38000] Loss: 3.488877296447754\n",
      "Step [17820/38000] Loss: 3.629293918609619\n",
      "Step [17821/38000] Loss: 3.528653621673584\n",
      "Step [17822/38000] Loss: 3.4693987369537354\n",
      "Step [17823/38000] Loss: 3.719414710998535\n",
      "Step [17824/38000] Loss: 3.7674264907836914\n",
      "Step [17825/38000] Loss: 3.3280255794525146\n",
      "Step [17826/38000] Loss: 3.281317949295044\n",
      "Step [17827/38000] Loss: 3.2954745292663574\n",
      "Step [17828/38000] Loss: 3.272089958190918\n",
      "Step [17829/38000] Loss: 3.3326327800750732\n",
      "Step [17830/38000] Loss: 3.233283042907715\n",
      "Step [17831/38000] Loss: 3.233320951461792\n",
      "Step [17832/38000] Loss: 3.534637212753296\n",
      "Step [17833/38000] Loss: 3.557162284851074\n",
      "Step [17834/38000] Loss: 3.5352187156677246\n",
      "Step [17835/38000] Loss: 3.4927778244018555\n",
      "Step [17836/38000] Loss: 3.3745787143707275\n",
      "Step [17837/38000] Loss: 3.4545626640319824\n",
      "Step [17838/38000] Loss: 3.392167091369629\n",
      "Step [17839/38000] Loss: 3.2283453941345215\n",
      "Step [17840/38000] Loss: 3.341341257095337\n",
      "Step [17841/38000] Loss: 3.231468677520752\n",
      "Step [17842/38000] Loss: 3.4382920265197754\n",
      "Step [17843/38000] Loss: 3.0946478843688965\n",
      "Step [17844/38000] Loss: 3.2701730728149414\n",
      "Step [17845/38000] Loss: 3.2364416122436523\n",
      "Step [17846/38000] Loss: 3.2644333839416504\n",
      "Step [17847/38000] Loss: 3.338240146636963\n",
      "Step [17848/38000] Loss: 3.533339262008667\n",
      "Step [17849/38000] Loss: 3.552323341369629\n",
      "Step [17850/38000] Loss: 3.393761157989502\n",
      "Step [17851/38000] Loss: 3.5233898162841797\n",
      "Step [17852/38000] Loss: 3.3888447284698486\n",
      "Step [17853/38000] Loss: 3.2073917388916016\n",
      "Step [17854/38000] Loss: 3.3156347274780273\n",
      "Step [17855/38000] Loss: 3.455117702484131\n",
      "Step [17856/38000] Loss: 3.4351720809936523\n",
      "Step [17857/38000] Loss: 3.375835418701172\n",
      "Step [17858/38000] Loss: 3.2960758209228516\n",
      "Step [17859/38000] Loss: 3.147320032119751\n",
      "Step [17860/38000] Loss: 3.310798168182373\n",
      "Step [17861/38000] Loss: 3.2622570991516113\n",
      "Step [17862/38000] Loss: 3.4775686264038086\n",
      "Step [17863/38000] Loss: 3.329176425933838\n",
      "Step [17864/38000] Loss: 3.499565362930298\n",
      "Step [17865/38000] Loss: 3.332414388656616\n",
      "Step [17866/38000] Loss: 3.3073573112487793\n",
      "Step [17867/38000] Loss: 3.3449819087982178\n",
      "Step [17868/38000] Loss: 3.2208569049835205\n",
      "Step [17869/38000] Loss: 2.984827995300293\n",
      "Step [17870/38000] Loss: 3.0953893661499023\n",
      "Step [17871/38000] Loss: 3.3054094314575195\n",
      "Step [17872/38000] Loss: 3.2336044311523438\n",
      "Step [17873/38000] Loss: 3.090500593185425\n",
      "Step [17874/38000] Loss: 3.2552967071533203\n",
      "Step [17875/38000] Loss: 3.146895408630371\n",
      "Step [17876/38000] Loss: 3.1107516288757324\n",
      "Step [17877/38000] Loss: 3.1278934478759766\n",
      "Step [17878/38000] Loss: 3.220794439315796\n",
      "Step [17879/38000] Loss: 3.140890121459961\n",
      "Step [17880/38000] Loss: 3.650447130203247\n",
      "Step [17881/38000] Loss: 3.5679173469543457\n",
      "Step [17882/38000] Loss: 3.184636354446411\n",
      "Step [17883/38000] Loss: 3.09669828414917\n",
      "Step [17884/38000] Loss: 3.740415573120117\n",
      "Step [17885/38000] Loss: 3.490809917449951\n",
      "Step [17886/38000] Loss: 3.153383255004883\n",
      "Step [17887/38000] Loss: 3.200178623199463\n",
      "Step [17888/38000] Loss: 3.224759101867676\n",
      "Step [17889/38000] Loss: 3.166961669921875\n",
      "Step [17890/38000] Loss: 3.2717995643615723\n",
      "Step [17891/38000] Loss: 3.1670780181884766\n",
      "Step [17892/38000] Loss: 3.2187278270721436\n",
      "Step [17893/38000] Loss: 3.236499786376953\n",
      "Step [17894/38000] Loss: 3.288778781890869\n",
      "Step [17895/38000] Loss: 3.304936170578003\n",
      "Step [17896/38000] Loss: 3.335214138031006\n",
      "Step [17897/38000] Loss: 3.2428483963012695\n",
      "Step [17898/38000] Loss: 3.2377071380615234\n",
      "Step [17899/38000] Loss: 3.150308132171631\n",
      "Step [17900/38000] Loss: 3.1808578968048096\n",
      "Test Loss: 2.8222403526306152\n",
      "Step [17901/38000] Loss: 2.850546360015869\n",
      "Step [17902/38000] Loss: 3.261765480041504\n",
      "Step [17903/38000] Loss: 3.19712233543396\n",
      "Step [17904/38000] Loss: 3.0954670906066895\n",
      "Step [17905/38000] Loss: 3.2073264122009277\n",
      "Step [17906/38000] Loss: 3.019782066345215\n",
      "Step [17907/38000] Loss: 3.208012819290161\n",
      "Step [17908/38000] Loss: 3.238008975982666\n",
      "Step [17909/38000] Loss: 3.387601375579834\n",
      "Step [17910/38000] Loss: 3.282759189605713\n",
      "Step [17911/38000] Loss: 3.271430015563965\n",
      "Step [17912/38000] Loss: 3.3325443267822266\n",
      "Step [17913/38000] Loss: 3.3272886276245117\n",
      "Step [17914/38000] Loss: 3.1545729637145996\n",
      "Step [17915/38000] Loss: 3.0298094749450684\n",
      "Step [17916/38000] Loss: 3.3135292530059814\n",
      "Step [17917/38000] Loss: 3.364654779434204\n",
      "Step [17918/38000] Loss: 3.244452953338623\n",
      "Step [17919/38000] Loss: 3.283573627471924\n",
      "Step [17920/38000] Loss: 3.219010591506958\n",
      "Step [17921/38000] Loss: 3.217498779296875\n",
      "Step [17922/38000] Loss: 3.3935909271240234\n",
      "Step [17923/38000] Loss: 3.722867250442505\n",
      "Step [17924/38000] Loss: 3.494241237640381\n",
      "Step [17925/38000] Loss: 3.046619415283203\n",
      "Step [17926/38000] Loss: 3.5516302585601807\n",
      "Step [17927/38000] Loss: 3.6885337829589844\n",
      "Step [17928/38000] Loss: 3.1691436767578125\n",
      "Step [17929/38000] Loss: 3.0598764419555664\n",
      "Step [17930/38000] Loss: 3.1807730197906494\n",
      "Step [17931/38000] Loss: 3.0415401458740234\n",
      "Step [17932/38000] Loss: 3.295311450958252\n",
      "Step [17933/38000] Loss: 3.1467173099517822\n",
      "Step [17934/38000] Loss: 3.141860008239746\n",
      "Step [17935/38000] Loss: 3.104137897491455\n",
      "Step [17936/38000] Loss: 3.1650071144104004\n",
      "Step [17937/38000] Loss: 3.092191219329834\n",
      "Step [17938/38000] Loss: 3.0956525802612305\n",
      "Step [17939/38000] Loss: 3.2206735610961914\n",
      "Step [17940/38000] Loss: 3.252537250518799\n",
      "Step [17941/38000] Loss: 3.229836940765381\n",
      "Step [17942/38000] Loss: 3.2550864219665527\n",
      "Step [17943/38000] Loss: 3.2870659828186035\n",
      "Step [17944/38000] Loss: 3.490354537963867\n",
      "Step [17945/38000] Loss: 2.588810920715332\n",
      "Step [17946/38000] Loss: 2.995810031890869\n",
      "Step [17947/38000] Loss: 3.0475356578826904\n",
      "Step [17948/38000] Loss: 3.2356438636779785\n",
      "Step [17949/38000] Loss: 3.3226358890533447\n",
      "Step [17950/38000] Loss: 3.2656736373901367\n",
      "Step [17951/38000] Loss: 3.2496273517608643\n",
      "Step [17952/38000] Loss: 3.0512707233428955\n",
      "Step [17953/38000] Loss: 3.343665599822998\n",
      "Step [17954/38000] Loss: 3.135634183883667\n",
      "Step [17955/38000] Loss: 3.234541177749634\n",
      "Step [17956/38000] Loss: 3.1483800411224365\n",
      "Step [17957/38000] Loss: 3.23447847366333\n",
      "Step [17958/38000] Loss: 3.1627931594848633\n",
      "Step [17959/38000] Loss: 2.986510992050171\n",
      "Step [17960/38000] Loss: 2.965653657913208\n",
      "Step [17961/38000] Loss: 2.999683380126953\n",
      "Step [17962/38000] Loss: 3.028353691101074\n",
      "Step [17963/38000] Loss: 2.881228446960449\n",
      "Step [17964/38000] Loss: 2.930100917816162\n",
      "Step [17965/38000] Loss: 2.957874059677124\n",
      "Step [17966/38000] Loss: 2.9388394355773926\n",
      "Step [17967/38000] Loss: 2.934678554534912\n",
      "Step [17968/38000] Loss: 2.9086368083953857\n",
      "Step [17969/38000] Loss: 3.0111594200134277\n",
      "Step [17970/38000] Loss: 3.146414279937744\n",
      "Step [17971/38000] Loss: 3.1032915115356445\n",
      "Step [17972/38000] Loss: 3.1305339336395264\n",
      "Step [17973/38000] Loss: 3.233339786529541\n",
      "Step [17974/38000] Loss: 2.9519472122192383\n",
      "Step [17975/38000] Loss: 2.936727523803711\n",
      "Step [17976/38000] Loss: 2.9998841285705566\n",
      "Step [17977/38000] Loss: 2.9110076427459717\n",
      "Step [17978/38000] Loss: 3.0008368492126465\n",
      "Step [17979/38000] Loss: 2.932297945022583\n",
      "Step [17980/38000] Loss: 2.9677345752716064\n",
      "Step [17981/38000] Loss: 2.9099676609039307\n",
      "Step [17982/38000] Loss: 3.2114176750183105\n",
      "Step [17983/38000] Loss: 3.640615940093994\n",
      "Step [17984/38000] Loss: 2.6349425315856934\n",
      "Step [17985/38000] Loss: 3.1535372734069824\n",
      "Step [17986/38000] Loss: 3.205533027648926\n",
      "Step [17987/38000] Loss: 3.1134300231933594\n",
      "Step [17988/38000] Loss: 3.000546932220459\n",
      "Step [17989/38000] Loss: 3.1436927318573\n",
      "Step [17990/38000] Loss: 3.0542242527008057\n",
      "Step [17991/38000] Loss: 3.102513074874878\n",
      "Step [17992/38000] Loss: 3.0551013946533203\n",
      "Step [17993/38000] Loss: 3.2229928970336914\n",
      "Step [17994/38000] Loss: 3.026360273361206\n",
      "Step [17995/38000] Loss: 2.873769760131836\n",
      "Step [17996/38000] Loss: 3.637986660003662\n",
      "Step [17997/38000] Loss: 2.9785914421081543\n",
      "Step [17998/38000] Loss: 2.8679146766662598\n",
      "Step [17999/38000] Loss: 2.8724327087402344\n",
      "Step [18000/38000] Loss: 2.7464218139648438\n",
      "Test Loss: 2.6565756797790527\n",
      "Step [18001/38000] Loss: 2.8382015228271484\n",
      "Step [18002/38000] Loss: 2.8227357864379883\n",
      "Step [18003/38000] Loss: 2.993535041809082\n",
      "Step [18004/38000] Loss: 2.78271484375\n",
      "Step [18005/38000] Loss: 2.9826250076293945\n",
      "Step [18006/38000] Loss: 3.1026852130889893\n",
      "Step [18007/38000] Loss: 3.0580694675445557\n",
      "Step [18008/38000] Loss: 3.076174736022949\n",
      "Step [18009/38000] Loss: 3.1826565265655518\n",
      "Step [18010/38000] Loss: 3.157313823699951\n",
      "Step [18011/38000] Loss: 2.981894016265869\n",
      "Step [18012/38000] Loss: 2.8867578506469727\n",
      "Step [18013/38000] Loss: 2.89105486869812\n",
      "Step [18014/38000] Loss: 2.897923469543457\n",
      "Step [18015/38000] Loss: 2.9838099479675293\n",
      "Step [18016/38000] Loss: 2.774707555770874\n",
      "Step [18017/38000] Loss: 2.962696075439453\n",
      "Step [18018/38000] Loss: 2.9081873893737793\n",
      "Step [18019/38000] Loss: 2.8107728958129883\n",
      "Step [18020/38000] Loss: 3.5117106437683105\n",
      "Step [18021/38000] Loss: 3.312591791152954\n",
      "Step [18022/38000] Loss: 2.850590705871582\n",
      "Step [18023/38000] Loss: 2.826148509979248\n",
      "Step [18024/38000] Loss: 2.8081798553466797\n",
      "Step [18025/38000] Loss: 3.2112343311309814\n",
      "Step [18026/38000] Loss: 3.660022020339966\n",
      "Step [18027/38000] Loss: 2.644422769546509\n",
      "Step [18028/38000] Loss: 2.8455166816711426\n",
      "Step [18029/38000] Loss: 2.903224468231201\n",
      "Step [18030/38000] Loss: 2.759464740753174\n",
      "Step [18031/38000] Loss: 3.394258499145508\n",
      "Step [18032/38000] Loss: 2.808488368988037\n",
      "Step [18033/38000] Loss: 2.9108123779296875\n",
      "Step [18034/38000] Loss: 2.833953380584717\n",
      "Step [18035/38000] Loss: 2.941567897796631\n",
      "Step [18036/38000] Loss: 2.8767192363739014\n",
      "Step [18037/38000] Loss: 2.812955141067505\n",
      "Step [18038/38000] Loss: 3.503377914428711\n",
      "Step [18039/38000] Loss: 3.1726932525634766\n",
      "Step [18040/38000] Loss: 2.829268455505371\n",
      "Step [18041/38000] Loss: 2.8263654708862305\n",
      "Step [18042/38000] Loss: 3.1628825664520264\n",
      "Step [18043/38000] Loss: 3.0554237365722656\n",
      "Step [18044/38000] Loss: 3.0776031017303467\n",
      "Step [18045/38000] Loss: 2.9439239501953125\n",
      "Step [18046/38000] Loss: 2.8681137561798096\n",
      "Step [18047/38000] Loss: 2.7480783462524414\n",
      "Step [18048/38000] Loss: 2.782072067260742\n",
      "Step [18049/38000] Loss: 2.68951416015625\n",
      "Step [18050/38000] Loss: 2.730006694793701\n",
      "Step [18051/38000] Loss: 2.797267436981201\n",
      "Step [18052/38000] Loss: 2.9763541221618652\n",
      "Step [18053/38000] Loss: 2.989030122756958\n",
      "Step [18054/38000] Loss: 2.9357123374938965\n",
      "Step [18055/38000] Loss: 3.007283926010132\n",
      "Step [18056/38000] Loss: 2.986969232559204\n",
      "Step [18057/38000] Loss: 3.1132569313049316\n",
      "Step [18058/38000] Loss: 2.9411373138427734\n",
      "Step [18059/38000] Loss: 3.0196597576141357\n",
      "Step [18060/38000] Loss: 2.964404582977295\n",
      "Step [18061/38000] Loss: 2.6662864685058594\n",
      "Step [18062/38000] Loss: 2.8429579734802246\n",
      "Step [18063/38000] Loss: 3.015002727508545\n",
      "Step [18064/38000] Loss: 2.9621975421905518\n",
      "Step [18065/38000] Loss: 2.8183929920196533\n",
      "Step [18066/38000] Loss: 2.859234571456909\n",
      "Step [18067/38000] Loss: 2.8908684253692627\n",
      "Step [18068/38000] Loss: 2.825636625289917\n",
      "Step [18069/38000] Loss: 2.7602949142456055\n",
      "Step [18070/38000] Loss: 2.8463010787963867\n",
      "Step [18071/38000] Loss: 2.801600217819214\n",
      "Step [18072/38000] Loss: 2.7765748500823975\n",
      "Step [18073/38000] Loss: 2.758946657180786\n",
      "Step [18074/38000] Loss: 2.7527592182159424\n",
      "Step [18075/38000] Loss: 2.688472270965576\n",
      "Step [18076/38000] Loss: 2.815218925476074\n",
      "Step [18077/38000] Loss: 2.792196273803711\n",
      "Step [18078/38000] Loss: 2.888214349746704\n",
      "Step [18079/38000] Loss: 3.1043946743011475\n",
      "Step [18080/38000] Loss: 3.0580062866210938\n",
      "Step [18081/38000] Loss: 3.07055926322937\n",
      "Step [18082/38000] Loss: 3.0406904220581055\n",
      "Step [18083/38000] Loss: 2.722942352294922\n",
      "Step [18084/38000] Loss: 2.7578606605529785\n",
      "Step [18085/38000] Loss: 3.5923852920532227\n",
      "Step [18086/38000] Loss: 3.040397882461548\n",
      "Step [18087/38000] Loss: 2.7145609855651855\n",
      "Step [18088/38000] Loss: 2.5668790340423584\n",
      "Step [18089/38000] Loss: 3.0470428466796875\n",
      "Step [18090/38000] Loss: 2.898740291595459\n",
      "Step [18091/38000] Loss: 2.9084367752075195\n",
      "Step [18092/38000] Loss: 2.9496536254882812\n",
      "Step [18093/38000] Loss: 3.226008176803589\n",
      "Step [18094/38000] Loss: 2.8624000549316406\n",
      "Step [18095/38000] Loss: 2.649226665496826\n",
      "Step [18096/38000] Loss: 3.1883349418640137\n",
      "Step [18097/38000] Loss: 2.639007091522217\n",
      "Step [18098/38000] Loss: 2.722508192062378\n",
      "Step [18099/38000] Loss: 2.582453727722168\n",
      "Step [18100/38000] Loss: 2.9902257919311523\n",
      "Test Loss: 2.3987865447998047\n",
      "Step [18101/38000] Loss: 3.6097018718719482\n",
      "Step [18102/38000] Loss: 2.5354063510894775\n",
      "Step [18103/38000] Loss: 2.8077969551086426\n",
      "Step [18104/38000] Loss: 2.682011604309082\n",
      "Step [18105/38000] Loss: 2.732494592666626\n",
      "Step [18106/38000] Loss: 2.850681781768799\n",
      "Step [18107/38000] Loss: 2.947957992553711\n",
      "Step [18108/38000] Loss: 2.8529815673828125\n",
      "Step [18109/38000] Loss: 2.921421766281128\n",
      "Step [18110/38000] Loss: 2.8679559230804443\n",
      "Step [18111/38000] Loss: 3.127807378768921\n",
      "Step [18112/38000] Loss: 3.6301486492156982\n",
      "Step [18113/38000] Loss: 2.53579044342041\n",
      "Step [18114/38000] Loss: 2.717524528503418\n",
      "Step [18115/38000] Loss: 2.710602045059204\n",
      "Step [18116/38000] Loss: 2.645005464553833\n",
      "Step [18117/38000] Loss: 2.720043659210205\n",
      "Step [18118/38000] Loss: 3.093738555908203\n",
      "Step [18119/38000] Loss: 3.5257177352905273\n",
      "Step [18120/38000] Loss: 3.6293108463287354\n",
      "Step [18121/38000] Loss: 2.621945858001709\n",
      "Step [18122/38000] Loss: 2.740267276763916\n",
      "Step [18123/38000] Loss: 2.6833834648132324\n",
      "Step [18124/38000] Loss: 2.6002323627471924\n",
      "Step [18125/38000] Loss: 2.6801061630249023\n",
      "Step [18126/38000] Loss: 2.6725945472717285\n",
      "Step [18127/38000] Loss: 2.7366371154785156\n",
      "Step [18128/38000] Loss: 2.73921537399292\n",
      "Step [18129/38000] Loss: 2.6855740547180176\n",
      "Step [18130/38000] Loss: 2.8482227325439453\n",
      "Step [18131/38000] Loss: 2.938274621963501\n",
      "Step [18132/38000] Loss: 2.933393716812134\n",
      "Step [18133/38000] Loss: 2.9069929122924805\n",
      "Step [18134/38000] Loss: 2.974705219268799\n",
      "Step [18135/38000] Loss: 2.817091941833496\n",
      "Step [18136/38000] Loss: 2.8938770294189453\n",
      "Step [18137/38000] Loss: 2.904662609100342\n",
      "Step [18138/38000] Loss: 2.8248140811920166\n",
      "Step [18139/38000] Loss: 2.9533681869506836\n",
      "Step [18140/38000] Loss: 2.7564096450805664\n",
      "Step [18141/38000] Loss: 2.4272360801696777\n",
      "Step [18142/38000] Loss: 2.7048637866973877\n",
      "Step [18143/38000] Loss: 2.7672019004821777\n",
      "Step [18144/38000] Loss: 2.6707301139831543\n",
      "Step [18145/38000] Loss: 2.6064207553863525\n",
      "Step [18146/38000] Loss: 2.636970281600952\n",
      "Step [18147/38000] Loss: 2.6954269409179688\n",
      "Step [18148/38000] Loss: 2.580533981323242\n",
      "Step [18149/38000] Loss: 2.6640148162841797\n",
      "Step [18150/38000] Loss: 2.8002610206604004\n",
      "Step [18151/38000] Loss: 2.8673486709594727\n",
      "Step [18152/38000] Loss: 2.727386474609375\n",
      "Step [18153/38000] Loss: 2.9785122871398926\n",
      "Step [18154/38000] Loss: 3.6219096183776855\n",
      "Step [18155/38000] Loss: 2.708440065383911\n",
      "Step [18156/38000] Loss: 2.610347032546997\n",
      "Step [18157/38000] Loss: 2.506033420562744\n",
      "Step [18158/38000] Loss: 2.8610174655914307\n",
      "Step [18159/38000] Loss: 2.7841482162475586\n",
      "Step [18160/38000] Loss: 2.9899935722351074\n",
      "Step [18161/38000] Loss: 2.8977062702178955\n",
      "Step [18162/38000] Loss: 2.727992296218872\n",
      "Step [18163/38000] Loss: 2.5918521881103516\n",
      "Step [18164/38000] Loss: 2.6745429039001465\n",
      "Step [18165/38000] Loss: 2.628455877304077\n",
      "Step [18166/38000] Loss: 2.6526570320129395\n",
      "Step [18167/38000] Loss: 2.584953784942627\n",
      "Step [18168/38000] Loss: 2.726834774017334\n",
      "Step [18169/38000] Loss: 2.9572677612304688\n",
      "Step [18170/38000] Loss: 2.9454495906829834\n",
      "Step [18171/38000] Loss: 2.833690643310547\n",
      "Step [18172/38000] Loss: 2.836592197418213\n",
      "Step [18173/38000] Loss: 2.6586899757385254\n",
      "Step [18174/38000] Loss: 3.349914073944092\n",
      "Step [18175/38000] Loss: 3.0627143383026123\n",
      "Step [18176/38000] Loss: 3.143331289291382\n",
      "Step [18177/38000] Loss: 3.5777790546417236\n",
      "Step [18178/38000] Loss: 2.065253257751465\n",
      "Step [18179/38000] Loss: 2.5322375297546387\n",
      "Step [18180/38000] Loss: 2.640866279602051\n",
      "Step [18181/38000] Loss: 2.487150192260742\n",
      "Step [18182/38000] Loss: 2.9678800106048584\n",
      "Step [18183/38000] Loss: 3.6405692100524902\n",
      "Step [18184/38000] Loss: 2.493546724319458\n",
      "Step [18185/38000] Loss: 2.5793371200561523\n",
      "Step [18186/38000] Loss: 2.507171630859375\n",
      "Step [18187/38000] Loss: 2.5030062198638916\n",
      "Step [18188/38000] Loss: 2.4512481689453125\n",
      "Step [18189/38000] Loss: 3.1755354404449463\n",
      "Step [18190/38000] Loss: 2.6118364334106445\n",
      "Step [18191/38000] Loss: 2.5765438079833984\n",
      "Step [18192/38000] Loss: 3.2811989784240723\n",
      "Step [18193/38000] Loss: 2.578397274017334\n",
      "Step [18194/38000] Loss: 2.536736011505127\n",
      "Step [18195/38000] Loss: 2.490042209625244\n",
      "Step [18196/38000] Loss: 2.5855095386505127\n",
      "Step [18197/38000] Loss: 2.5599541664123535\n",
      "Step [18198/38000] Loss: 2.6114590167999268\n",
      "Step [18199/38000] Loss: 2.5970640182495117\n",
      "Step [18200/38000] Loss: 2.7507317066192627\n",
      "Test Loss: 2.4382576942443848\n",
      "Step [18201/38000] Loss: 2.7179574966430664\n",
      "Step [18202/38000] Loss: 2.8069405555725098\n",
      "Step [18203/38000] Loss: 2.7537922859191895\n",
      "Step [18204/38000] Loss: 2.658580780029297\n",
      "Step [18205/38000] Loss: 2.543083667755127\n",
      "Step [18206/38000] Loss: 2.549309253692627\n",
      "Step [18207/38000] Loss: 2.6039583683013916\n",
      "Step [18208/38000] Loss: 2.47351336479187\n",
      "Step [18209/38000] Loss: 2.745384454727173\n",
      "Step [18210/38000] Loss: 2.7124712467193604\n",
      "Step [18211/38000] Loss: 2.743659019470215\n",
      "Step [18212/38000] Loss: 2.7329254150390625\n",
      "Step [18213/38000] Loss: 2.761875629425049\n",
      "Step [18214/38000] Loss: 2.4906132221221924\n",
      "Step [18215/38000] Loss: 2.509859800338745\n",
      "Step [18216/38000] Loss: 2.5297913551330566\n",
      "Step [18217/38000] Loss: 2.4603044986724854\n",
      "Step [18218/38000] Loss: 2.5105667114257812\n",
      "Step [18219/38000] Loss: 2.762619972229004\n",
      "Step [18220/38000] Loss: 2.790827751159668\n",
      "Step [18221/38000] Loss: 2.8794665336608887\n",
      "Step [18222/38000] Loss: 2.892733573913574\n",
      "Step [18223/38000] Loss: 2.5438973903656006\n",
      "Step [18224/38000] Loss: 2.5051417350769043\n",
      "Step [18225/38000] Loss: 2.573444128036499\n",
      "Step [18226/38000] Loss: 2.5137791633605957\n",
      "Step [18227/38000] Loss: 2.593552589416504\n",
      "Step [18228/38000] Loss: 2.529134750366211\n",
      "Step [18229/38000] Loss: 2.5138769149780273\n",
      "Step [18230/38000] Loss: 2.603597640991211\n",
      "Step [18231/38000] Loss: 2.580146074295044\n",
      "Step [18232/38000] Loss: 2.6035218238830566\n",
      "Step [18233/38000] Loss: 2.4790570735931396\n",
      "Step [18234/38000] Loss: 2.4310731887817383\n",
      "Step [18235/38000] Loss: 2.700867176055908\n",
      "Step [18236/38000] Loss: 2.680567741394043\n",
      "Step [18237/38000] Loss: 2.587855100631714\n",
      "Step [18238/38000] Loss: 2.7216553688049316\n",
      "Step [18239/38000] Loss: 2.9240047931671143\n",
      "Step [18240/38000] Loss: 2.803727626800537\n",
      "Step [18241/38000] Loss: 3.3428165912628174\n",
      "Step [18242/38000] Loss: 3.331507682800293\n",
      "Step [18243/38000] Loss: 2.5411264896392822\n",
      "Step [18244/38000] Loss: 2.456859827041626\n",
      "Step [18245/38000] Loss: 2.4599010944366455\n",
      "Step [18246/38000] Loss: 2.5300538539886475\n",
      "Step [18247/38000] Loss: 2.4619204998016357\n",
      "Step [18248/38000] Loss: 2.487095355987549\n",
      "Step [18249/38000] Loss: 2.433825731277466\n",
      "Step [18250/38000] Loss: 2.8482422828674316\n",
      "Step [18251/38000] Loss: 3.2311511039733887\n",
      "Step [18252/38000] Loss: 2.485705852508545\n",
      "Step [18253/38000] Loss: 2.488828182220459\n",
      "Step [18254/38000] Loss: 2.463026762008667\n",
      "Step [18255/38000] Loss: 2.4179344177246094\n",
      "Step [18256/38000] Loss: 2.2902092933654785\n",
      "Step [18257/38000] Loss: 2.4300966262817383\n",
      "Step [18258/38000] Loss: 2.3671202659606934\n",
      "Step [18259/38000] Loss: 2.371629238128662\n",
      "Step [18260/38000] Loss: 2.3609671592712402\n",
      "Step [18261/38000] Loss: 2.360745429992676\n",
      "Step [18262/38000] Loss: 2.448338031768799\n",
      "Step [18263/38000] Loss: 2.3994882106781006\n",
      "Step [18264/38000] Loss: 2.475649356842041\n",
      "Step [18265/38000] Loss: 2.4629788398742676\n",
      "Step [18266/38000] Loss: 2.551290512084961\n",
      "Step [18267/38000] Loss: 2.526693105697632\n",
      "Step [18268/38000] Loss: 2.605724811553955\n",
      "Step [18269/38000] Loss: 2.4775118827819824\n",
      "Step [18270/38000] Loss: 2.474562168121338\n",
      "Step [18271/38000] Loss: 2.4916112422943115\n",
      "Step [18272/38000] Loss: 2.468423843383789\n",
      "Step [18273/38000] Loss: 2.4311885833740234\n",
      "Step [18274/38000] Loss: 2.4183242321014404\n",
      "Step [18275/38000] Loss: 2.5037126541137695\n",
      "Step [18276/38000] Loss: 3.0165553092956543\n",
      "Step [18277/38000] Loss: 3.603151321411133\n",
      "Step [18278/38000] Loss: 2.226633071899414\n",
      "Step [18279/38000] Loss: 2.5890541076660156\n",
      "Step [18280/38000] Loss: 2.4047296047210693\n",
      "Step [18281/38000] Loss: 2.4550015926361084\n",
      "Step [18282/38000] Loss: 2.9062933921813965\n",
      "Step [18283/38000] Loss: 3.485989809036255\n",
      "Step [18284/38000] Loss: 2.4387388229370117\n",
      "Step [18285/38000] Loss: 2.3983263969421387\n",
      "Step [18286/38000] Loss: 2.5686936378479004\n",
      "Step [18287/38000] Loss: 2.6031010150909424\n",
      "Step [18288/38000] Loss: 2.588895797729492\n",
      "Step [18289/38000] Loss: 2.5535926818847656\n",
      "Step [18290/38000] Loss: 2.59243106842041\n",
      "Step [18291/38000] Loss: 2.37422513961792\n",
      "Step [18292/38000] Loss: 2.377732753753662\n",
      "Step [18293/38000] Loss: 2.378359794616699\n",
      "Step [18294/38000] Loss: 2.4616289138793945\n",
      "Step [18295/38000] Loss: 2.499539852142334\n",
      "Step [18296/38000] Loss: 2.6632184982299805\n",
      "Step [18297/38000] Loss: 2.669887065887451\n",
      "Step [18298/38000] Loss: 2.6109793186187744\n",
      "Step [18299/38000] Loss: 2.6034440994262695\n",
      "Step [18300/38000] Loss: 2.925823211669922\n",
      "Test Loss: 2.4614100456237793\n",
      "Step [18301/38000] Loss: 3.4558286666870117\n",
      "Step [18302/38000] Loss: 2.4877991676330566\n",
      "Step [18303/38000] Loss: 2.4168577194213867\n",
      "Step [18304/38000] Loss: 2.5079472064971924\n",
      "Step [18305/38000] Loss: 2.6621365547180176\n",
      "Step [18306/38000] Loss: 2.5884578227996826\n",
      "Step [18307/38000] Loss: 2.6710009574890137\n",
      "Step [18308/38000] Loss: 2.5788915157318115\n",
      "Step [18309/38000] Loss: 2.415705680847168\n",
      "Step [18310/38000] Loss: 3.278937816619873\n",
      "Step [18311/38000] Loss: 2.455502510070801\n",
      "Step [18312/38000] Loss: 2.942401885986328\n",
      "Step [18313/38000] Loss: 2.3778300285339355\n",
      "Step [18314/38000] Loss: 2.374260902404785\n",
      "Step [18315/38000] Loss: 2.3425350189208984\n",
      "Step [18316/38000] Loss: 2.3126282691955566\n",
      "Step [18317/38000] Loss: 2.398005962371826\n",
      "Step [18318/38000] Loss: 2.5372092723846436\n",
      "Step [18319/38000] Loss: 2.648118495941162\n",
      "Step [18320/38000] Loss: 2.707213878631592\n",
      "Step [18321/38000] Loss: 2.8485121726989746\n",
      "Step [18322/38000] Loss: 2.515439987182617\n",
      "Step [18323/38000] Loss: 2.2895569801330566\n",
      "Step [18324/38000] Loss: 2.312516689300537\n",
      "Step [18325/38000] Loss: 2.4162328243255615\n",
      "Step [18326/38000] Loss: 2.3560969829559326\n",
      "Step [18327/38000] Loss: 2.254159927368164\n",
      "Step [18328/38000] Loss: 2.33447265625\n",
      "Step [18329/38000] Loss: 2.2406535148620605\n",
      "Step [18330/38000] Loss: 2.3372535705566406\n",
      "Step [18331/38000] Loss: 2.2083020210266113\n",
      "Step [18332/38000] Loss: 2.3229424953460693\n",
      "Step [18333/38000] Loss: 2.2750353813171387\n",
      "Step [18334/38000] Loss: 2.3449692726135254\n",
      "Step [18335/38000] Loss: 2.2808761596679688\n",
      "Step [18336/38000] Loss: 2.3469362258911133\n",
      "Step [18337/38000] Loss: 2.3520660400390625\n",
      "Step [18338/38000] Loss: 2.4034714698791504\n",
      "Step [18339/38000] Loss: 2.489250659942627\n",
      "Step [18340/38000] Loss: 2.5083260536193848\n",
      "Step [18341/38000] Loss: 2.3568053245544434\n",
      "Step [18342/38000] Loss: 2.4330573081970215\n",
      "Step [18343/38000] Loss: 2.4002318382263184\n",
      "Step [18344/38000] Loss: 2.5052883625030518\n",
      "Step [18345/38000] Loss: 2.6632425785064697\n",
      "Step [18346/38000] Loss: 2.6087284088134766\n",
      "Step [18347/38000] Loss: 2.5627708435058594\n",
      "Step [18348/38000] Loss: 2.4141459465026855\n",
      "Step [18349/38000] Loss: 2.2443530559539795\n",
      "Step [18350/38000] Loss: 2.408036231994629\n",
      "Step [18351/38000] Loss: 2.5417561531066895\n",
      "Step [18352/38000] Loss: 2.602168560028076\n",
      "Step [18353/38000] Loss: 2.523359537124634\n",
      "Step [18354/38000] Loss: 2.5861692428588867\n",
      "Step [18355/38000] Loss: 2.3379158973693848\n",
      "Step [18356/38000] Loss: 2.3685145378112793\n",
      "Step [18357/38000] Loss: 3.5451231002807617\n",
      "Step [18358/38000] Loss: 2.781107187271118\n",
      "Step [18359/38000] Loss: 2.265781879425049\n",
      "Step [18360/38000] Loss: 2.66202974319458\n",
      "Step [18361/38000] Loss: 2.7341206073760986\n",
      "Step [18362/38000] Loss: 2.5818376541137695\n",
      "Step [18363/38000] Loss: 2.5384554862976074\n",
      "Step [18364/38000] Loss: 2.512014865875244\n",
      "Step [18365/38000] Loss: 2.2427804470062256\n",
      "Step [18366/38000] Loss: 2.3354716300964355\n",
      "Step [18367/38000] Loss: 2.309953212738037\n",
      "Step [18368/38000] Loss: 2.302131175994873\n",
      "Step [18369/38000] Loss: 2.2399821281433105\n",
      "Step [18370/38000] Loss: 2.27982759475708\n",
      "Step [18371/38000] Loss: 2.26204514503479\n",
      "Step [18372/38000] Loss: 2.219744920730591\n",
      "Step [18373/38000] Loss: 2.2388811111450195\n",
      "Step [18374/38000] Loss: 2.3061270713806152\n",
      "Step [18375/38000] Loss: 2.285134792327881\n",
      "Step [18376/38000] Loss: 2.562316417694092\n",
      "Step [18377/38000] Loss: 2.6647541522979736\n",
      "Step [18378/38000] Loss: 2.5929114818573\n",
      "Step [18379/38000] Loss: 2.581491708755493\n",
      "Step [18380/38000] Loss: 2.3339619636535645\n",
      "Step [18381/38000] Loss: 2.1652042865753174\n",
      "Step [18382/38000] Loss: 2.679928779602051\n",
      "Step [18383/38000] Loss: 3.4658865928649902\n",
      "Step [18384/38000] Loss: 2.472221851348877\n",
      "Step [18385/38000] Loss: 2.2841439247131348\n",
      "Step [18386/38000] Loss: 2.322406053543091\n",
      "Step [18387/38000] Loss: 2.517390727996826\n",
      "Step [18388/38000] Loss: 2.432004928588867\n",
      "Step [18389/38000] Loss: 2.542210340499878\n",
      "Step [18390/38000] Loss: 2.5584068298339844\n",
      "Step [18391/38000] Loss: 2.335864543914795\n",
      "Step [18392/38000] Loss: 2.2856626510620117\n",
      "Step [18393/38000] Loss: 2.271566152572632\n",
      "Step [18394/38000] Loss: 2.2956409454345703\n",
      "Step [18395/38000] Loss: 2.1950957775115967\n",
      "Step [18396/38000] Loss: 2.2739124298095703\n",
      "Step [18397/38000] Loss: 2.3129806518554688\n",
      "Step [18398/38000] Loss: 2.2813503742218018\n",
      "Step [18399/38000] Loss: 2.1612062454223633\n",
      "Step [18400/38000] Loss: 2.6329150199890137\n",
      "Test Loss: 2.1975748538970947\n",
      "Step [18401/38000] Loss: 2.800534725189209\n",
      "Step [18402/38000] Loss: 2.312032699584961\n",
      "Step [18403/38000] Loss: 2.2968010902404785\n",
      "Step [18404/38000] Loss: 2.4717819690704346\n",
      "Step [18405/38000] Loss: 2.4923367500305176\n",
      "Step [18406/38000] Loss: 2.450584888458252\n",
      "Step [18407/38000] Loss: 2.577016592025757\n",
      "Step [18408/38000] Loss: 2.391467571258545\n",
      "Step [18409/38000] Loss: 2.2531819343566895\n",
      "Step [18410/38000] Loss: 2.172008991241455\n",
      "Step [18411/38000] Loss: 2.6883816719055176\n",
      "Step [18412/38000] Loss: 3.4753201007843018\n",
      "Step [18413/38000] Loss: 2.3021364212036133\n",
      "Step [18414/38000] Loss: 2.297996997833252\n",
      "Step [18415/38000] Loss: 2.2950477600097656\n",
      "Step [18416/38000] Loss: 2.2195205688476562\n",
      "Step [18417/38000] Loss: 2.137394428253174\n",
      "Step [18418/38000] Loss: 2.201871156692505\n",
      "Step [18419/38000] Loss: 2.225332736968994\n",
      "Step [18420/38000] Loss: 2.1804428100585938\n",
      "Step [18421/38000] Loss: 2.2599105834960938\n",
      "Step [18422/38000] Loss: 2.3259010314941406\n",
      "Step [18423/38000] Loss: 2.2897372245788574\n",
      "Step [18424/38000] Loss: 2.349665641784668\n",
      "Step [18425/38000] Loss: 2.2819552421569824\n",
      "Step [18426/38000] Loss: 2.154205560684204\n",
      "Step [18427/38000] Loss: 2.228524684906006\n",
      "Step [18428/38000] Loss: 2.2809598445892334\n",
      "Step [18429/38000] Loss: 2.1744091510772705\n",
      "Step [18430/38000] Loss: 2.1980197429656982\n",
      "Step [18431/38000] Loss: 2.2181973457336426\n",
      "Step [18432/38000] Loss: 2.217446804046631\n",
      "Step [18433/38000] Loss: 2.221055030822754\n",
      "Step [18434/38000] Loss: 2.285148859024048\n",
      "Step [18435/38000] Loss: 2.090029239654541\n",
      "Step [18436/38000] Loss: 2.2021450996398926\n",
      "Step [18437/38000] Loss: 2.1979990005493164\n",
      "Step [18438/38000] Loss: 2.21480131149292\n",
      "Step [18439/38000] Loss: 2.235278606414795\n",
      "Step [18440/38000] Loss: 2.200619697570801\n",
      "Step [18441/38000] Loss: 2.168619394302368\n",
      "Step [18442/38000] Loss: 2.169322967529297\n",
      "Step [18443/38000] Loss: 2.1750903129577637\n",
      "Step [18444/38000] Loss: 2.232565402984619\n",
      "Step [18445/38000] Loss: 2.461923360824585\n",
      "Step [18446/38000] Loss: 2.3995022773742676\n",
      "Step [18447/38000] Loss: 2.5259785652160645\n",
      "Step [18448/38000] Loss: 2.417830467224121\n",
      "Step [18449/38000] Loss: 2.181417942047119\n",
      "Step [18450/38000] Loss: 2.1949782371520996\n",
      "Step [18451/38000] Loss: 2.3980183601379395\n",
      "Step [18452/38000] Loss: 2.5877671241760254\n",
      "Step [18453/38000] Loss: 2.506518602371216\n",
      "Step [18454/38000] Loss: 2.43896222114563\n",
      "Step [18455/38000] Loss: 2.537672519683838\n",
      "Step [18456/38000] Loss: 2.2088770866394043\n",
      "Step [18457/38000] Loss: 3.503110408782959\n",
      "Step [18458/38000] Loss: 2.802769660949707\n",
      "Step [18459/38000] Loss: 2.148214101791382\n",
      "Step [18460/38000] Loss: 2.2525501251220703\n",
      "Step [18461/38000] Loss: 2.184523582458496\n",
      "Step [18462/38000] Loss: 2.260061502456665\n",
      "Step [18463/38000] Loss: 3.287574529647827\n",
      "Step [18464/38000] Loss: 3.146878242492676\n",
      "Step [18465/38000] Loss: 2.2531521320343018\n",
      "Step [18466/38000] Loss: 2.4133880138397217\n",
      "Step [18467/38000] Loss: 2.4107489585876465\n",
      "Step [18468/38000] Loss: 2.390777587890625\n",
      "Step [18469/38000] Loss: 2.4369070529937744\n",
      "Step [18470/38000] Loss: 2.2461228370666504\n",
      "Step [18471/38000] Loss: 2.0661087036132812\n",
      "Step [18472/38000] Loss: 2.3001251220703125\n",
      "Step [18473/38000] Loss: 2.425442934036255\n",
      "Step [18474/38000] Loss: 2.5056393146514893\n",
      "Step [18475/38000] Loss: 2.4154396057128906\n",
      "Step [18476/38000] Loss: 2.352203369140625\n",
      "Step [18477/38000] Loss: 2.301483154296875\n",
      "Step [18478/38000] Loss: 2.224435329437256\n",
      "Step [18479/38000] Loss: 2.1922409534454346\n",
      "Step [18480/38000] Loss: 2.1690120697021484\n",
      "Step [18481/38000] Loss: 2.295947551727295\n",
      "Step [18482/38000] Loss: 2.2152724266052246\n",
      "Step [18483/38000] Loss: 2.3651981353759766\n",
      "Step [18484/38000] Loss: 2.441084384918213\n",
      "Step [18485/38000] Loss: 2.4283385276794434\n",
      "Step [18486/38000] Loss: 2.4072961807250977\n",
      "Step [18487/38000] Loss: 2.3312835693359375\n",
      "Step [18488/38000] Loss: 2.1360585689544678\n",
      "Step [18489/38000] Loss: 2.1901168823242188\n",
      "Step [18490/38000] Loss: 2.3292078971862793\n",
      "Step [18491/38000] Loss: 2.2650909423828125\n",
      "Step [18492/38000] Loss: 2.3831028938293457\n",
      "Step [18493/38000] Loss: 2.3827905654907227\n",
      "Step [18494/38000] Loss: 2.4056484699249268\n",
      "Step [18495/38000] Loss: 2.202308177947998\n",
      "Step [18496/38000] Loss: 2.3688015937805176\n",
      "Step [18497/38000] Loss: 2.3864336013793945\n",
      "Step [18498/38000] Loss: 2.3518104553222656\n",
      "Step [18499/38000] Loss: 2.4018819332122803\n",
      "Step [18500/38000] Loss: 2.68430495262146\n",
      "Test Loss: 2.2466745376586914\n",
      "Step [18501/38000] Loss: 3.497542381286621\n",
      "Step [18502/38000] Loss: 2.373875856399536\n",
      "Step [18503/38000] Loss: 2.111088275909424\n",
      "Step [18504/38000] Loss: 2.0714735984802246\n",
      "Step [18505/38000] Loss: 2.1367342472076416\n",
      "Step [18506/38000] Loss: 2.0408196449279785\n",
      "Step [18507/38000] Loss: 2.146777391433716\n",
      "Step [18508/38000] Loss: 2.014404296875\n",
      "Step [18509/38000] Loss: 2.298293113708496\n",
      "Step [18510/38000] Loss: 2.3616461753845215\n",
      "Step [18511/38000] Loss: 2.2983365058898926\n",
      "Step [18512/38000] Loss: 2.3598732948303223\n",
      "Step [18513/38000] Loss: 2.4086294174194336\n",
      "Step [18514/38000] Loss: 2.150801181793213\n",
      "Step [18515/38000] Loss: 2.189852237701416\n",
      "Step [18516/38000] Loss: 2.224461555480957\n",
      "Step [18517/38000] Loss: 2.208181858062744\n",
      "Step [18518/38000] Loss: 2.270242214202881\n",
      "Step [18519/38000] Loss: 2.1810708045959473\n",
      "Step [18520/38000] Loss: 2.1752734184265137\n",
      "Step [18521/38000] Loss: 2.1713972091674805\n",
      "Step [18522/38000] Loss: 2.671207904815674\n",
      "Step [18523/38000] Loss: 3.4245004653930664\n",
      "Step [18524/38000] Loss: 3.3246188163757324\n",
      "Step [18525/38000] Loss: 2.146925926208496\n",
      "Step [18526/38000] Loss: 2.08561372756958\n",
      "Step [18527/38000] Loss: 2.1571969985961914\n",
      "Step [18528/38000] Loss: 2.081510543823242\n",
      "Step [18529/38000] Loss: 2.291703462600708\n",
      "Step [18530/38000] Loss: 2.3603038787841797\n",
      "Step [18531/38000] Loss: 2.3265137672424316\n",
      "Step [18532/38000] Loss: 2.251944065093994\n",
      "Step [18533/38000] Loss: 2.262058734893799\n",
      "Step [18534/38000] Loss: 2.1023359298706055\n",
      "Step [18535/38000] Loss: 2.0206823348999023\n",
      "Step [18536/38000] Loss: 2.512022018432617\n",
      "Step [18537/38000] Loss: 3.5537376403808594\n",
      "Step [18538/38000] Loss: 2.243089437484741\n",
      "Step [18539/38000] Loss: 2.080108165740967\n",
      "Step [18540/38000] Loss: 2.2589356899261475\n",
      "Step [18541/38000] Loss: 2.315077304840088\n",
      "Step [18542/38000] Loss: 2.3564958572387695\n",
      "Step [18543/38000] Loss: 2.37381649017334\n",
      "Step [18544/38000] Loss: 2.3484532833099365\n",
      "Step [18545/38000] Loss: 2.2688181400299072\n",
      "Step [18546/38000] Loss: 3.1295742988586426\n",
      "Step [18547/38000] Loss: 2.118597984313965\n",
      "Step [18548/38000] Loss: 2.948777198791504\n",
      "Step [18549/38000] Loss: 3.3395140171051025\n",
      "Step [18550/38000] Loss: 2.20546817779541\n",
      "Step [18551/38000] Loss: 2.1975319385528564\n",
      "Step [18552/38000] Loss: 2.1844797134399414\n",
      "Step [18553/38000] Loss: 2.1784780025482178\n",
      "Step [18554/38000] Loss: 2.2425456047058105\n",
      "Step [18555/38000] Loss: 2.0827646255493164\n",
      "Step [18556/38000] Loss: 2.116670608520508\n",
      "Step [18557/38000] Loss: 3.5012753009796143\n",
      "Step [18558/38000] Loss: 2.7817800045013428\n",
      "Step [18559/38000] Loss: 2.0348682403564453\n",
      "Step [18560/38000] Loss: 2.1112775802612305\n",
      "Step [18561/38000] Loss: 2.606044292449951\n",
      "Step [18562/38000] Loss: 2.534273624420166\n",
      "Step [18563/38000] Loss: 2.0513596534729004\n",
      "Step [18564/38000] Loss: 2.070615768432617\n",
      "Step [18565/38000] Loss: 2.046658992767334\n",
      "Step [18566/38000] Loss: 2.151641845703125\n",
      "Step [18567/38000] Loss: 2.028550148010254\n",
      "Step [18568/38000] Loss: 2.467959403991699\n",
      "Step [18569/38000] Loss: 2.672978401184082\n",
      "Step [18570/38000] Loss: 2.2395331859588623\n",
      "Step [18571/38000] Loss: 2.1816630363464355\n",
      "Step [18572/38000] Loss: 2.187659740447998\n",
      "Step [18573/38000] Loss: 2.2007153034210205\n",
      "Step [18574/38000] Loss: 2.1815719604492188\n",
      "Step [18575/38000] Loss: 2.009951591491699\n",
      "Step [18576/38000] Loss: 2.1009302139282227\n",
      "Step [18577/38000] Loss: 2.104058265686035\n",
      "Step [18578/38000] Loss: 2.0199999809265137\n",
      "Step [18579/38000] Loss: 2.160595417022705\n",
      "Step [18580/38000] Loss: 2.1045453548431396\n",
      "Step [18581/38000] Loss: 2.11460542678833\n",
      "Step [18582/38000] Loss: 2.1288352012634277\n",
      "Step [18583/38000] Loss: 2.0374722480773926\n",
      "Step [18584/38000] Loss: 2.120941638946533\n",
      "Step [18585/38000] Loss: 3.2854416370391846\n",
      "Step [18586/38000] Loss: 2.9609317779541016\n",
      "Step [18587/38000] Loss: 2.026315927505493\n",
      "Step [18588/38000] Loss: 2.018550395965576\n",
      "Step [18589/38000] Loss: 2.0313217639923096\n",
      "Step [18590/38000] Loss: 2.757641315460205\n",
      "Step [18591/38000] Loss: 3.416165828704834\n",
      "Step [18592/38000] Loss: 2.071571111679077\n",
      "Step [18593/38000] Loss: 2.1659328937530518\n",
      "Step [18594/38000] Loss: 2.3672215938568115\n",
      "Step [18595/38000] Loss: 2.2983245849609375\n",
      "Step [18596/38000] Loss: 2.3268377780914307\n",
      "Step [18597/38000] Loss: 2.2720046043395996\n",
      "Step [18598/38000] Loss: 2.2089426517486572\n",
      "Step [18599/38000] Loss: 2.1229805946350098\n",
      "Step [18600/38000] Loss: 2.167273998260498\n",
      "Test Loss: 2.397404670715332\n",
      "Step [18601/38000] Loss: 2.383321762084961\n",
      "Step [18602/38000] Loss: 2.2720065116882324\n",
      "Step [18603/38000] Loss: 2.2957167625427246\n",
      "Step [18604/38000] Loss: 2.375457525253296\n",
      "Step [18605/38000] Loss: 2.4305243492126465\n",
      "Step [18606/38000] Loss: 2.434255838394165\n",
      "Step [18607/38000] Loss: 2.829718589782715\n",
      "Step [18608/38000] Loss: 3.517035961151123\n",
      "Step [18609/38000] Loss: 2.553539752960205\n",
      "Step [18610/38000] Loss: 2.517878770828247\n",
      "Step [18611/38000] Loss: 2.5076944828033447\n",
      "Step [18612/38000] Loss: 2.620852470397949\n",
      "Step [18613/38000] Loss: 2.7036118507385254\n",
      "Step [18614/38000] Loss: 2.7117912769317627\n",
      "Step [18615/38000] Loss: 2.747128486633301\n",
      "Step [18616/38000] Loss: 2.8447554111480713\n",
      "Step [18617/38000] Loss: 2.806241273880005\n",
      "Step [18618/38000] Loss: 2.859741687774658\n",
      "Step [18619/38000] Loss: 2.8940513134002686\n",
      "Step [18620/38000] Loss: 2.8884265422821045\n",
      "Step [18621/38000] Loss: 3.005631685256958\n",
      "Step [18622/38000] Loss: 2.9344611167907715\n",
      "Step [18623/38000] Loss: 3.145174503326416\n",
      "Step [18624/38000] Loss: 3.192363739013672\n",
      "Step [18625/38000] Loss: 3.1742501258850098\n",
      "Step [18626/38000] Loss: 3.207213878631592\n",
      "Step [18627/38000] Loss: 3.4139857292175293\n",
      "Step [18628/38000] Loss: 3.556809425354004\n",
      "Step [18629/38000] Loss: 3.383944511413574\n",
      "Step [18630/38000] Loss: 3.5819153785705566\n",
      "Step [18631/38000] Loss: 3.5568318367004395\n",
      "Step [18632/38000] Loss: 3.9962472915649414\n",
      "Step [18633/38000] Loss: 3.381772518157959\n",
      "Step [18634/38000] Loss: 3.814883232116699\n",
      "Step [18635/38000] Loss: 3.816838026046753\n",
      "Step [18636/38000] Loss: 3.456484317779541\n",
      "Step [18637/38000] Loss: 3.4808645248413086\n",
      "Step [18638/38000] Loss: 3.5591559410095215\n",
      "Step [18639/38000] Loss: 3.6032466888427734\n",
      "Step [18640/38000] Loss: 3.534191370010376\n",
      "Step [18641/38000] Loss: 3.623201847076416\n",
      "Step [18642/38000] Loss: 3.6309542655944824\n",
      "Step [18643/38000] Loss: 3.9569201469421387\n",
      "Step [18644/38000] Loss: 3.4138596057891846\n",
      "Step [18645/38000] Loss: 3.559056282043457\n",
      "Step [18646/38000] Loss: 3.425631523132324\n",
      "Step [18647/38000] Loss: 3.5555851459503174\n",
      "Step [18648/38000] Loss: 3.633859634399414\n",
      "Step [18649/38000] Loss: 3.7564797401428223\n",
      "Step [18650/38000] Loss: 3.647467613220215\n",
      "Step [18651/38000] Loss: 3.7366628646850586\n",
      "Step [18652/38000] Loss: 3.6271629333496094\n",
      "Step [18653/38000] Loss: 3.5660030841827393\n",
      "Step [18654/38000] Loss: 3.3470711708068848\n",
      "Step [18655/38000] Loss: 3.45558500289917\n",
      "Step [18656/38000] Loss: 3.590632915496826\n",
      "Step [18657/38000] Loss: 3.6804070472717285\n",
      "Step [18658/38000] Loss: 3.9640698432922363\n",
      "Step [18659/38000] Loss: 3.4318814277648926\n",
      "Step [18660/38000] Loss: 3.497957706451416\n",
      "Step [18661/38000] Loss: 3.493821144104004\n",
      "Step [18662/38000] Loss: 3.442138195037842\n",
      "Step [18663/38000] Loss: 3.524383306503296\n",
      "Step [18664/38000] Loss: 3.513436794281006\n",
      "Step [18665/38000] Loss: 3.5495762825012207\n",
      "Step [18666/38000] Loss: 3.636974334716797\n",
      "Step [18667/38000] Loss: 3.4631705284118652\n",
      "Step [18668/38000] Loss: 3.421605110168457\n",
      "Step [18669/38000] Loss: 3.640939712524414\n",
      "Step [18670/38000] Loss: 3.6397149562835693\n",
      "Step [18671/38000] Loss: 3.384758472442627\n",
      "Step [18672/38000] Loss: 3.4605207443237305\n",
      "Step [18673/38000] Loss: 3.5788865089416504\n",
      "Step [18674/38000] Loss: 3.6316416263580322\n",
      "Step [18675/38000] Loss: 3.659637928009033\n",
      "Step [18676/38000] Loss: 3.471343994140625\n",
      "Step [18677/38000] Loss: 3.954883098602295\n",
      "Step [18678/38000] Loss: 3.839907169342041\n",
      "Step [18679/38000] Loss: 3.476870059967041\n",
      "Step [18680/38000] Loss: 3.3828530311584473\n",
      "Step [18681/38000] Loss: 3.4531259536743164\n",
      "Step [18682/38000] Loss: 3.4895777702331543\n",
      "Step [18683/38000] Loss: 3.695991039276123\n",
      "Step [18684/38000] Loss: 3.258718729019165\n",
      "Step [18685/38000] Loss: 3.837980270385742\n",
      "Step [18686/38000] Loss: 3.5025675296783447\n",
      "Step [18687/38000] Loss: 3.402665138244629\n",
      "Step [18688/38000] Loss: 3.27288818359375\n",
      "Step [18689/38000] Loss: 3.62650203704834\n",
      "Step [18690/38000] Loss: 3.611288070678711\n",
      "Step [18691/38000] Loss: 3.458372116088867\n",
      "Step [18692/38000] Loss: 3.559332847595215\n",
      "Step [18693/38000] Loss: 3.809258460998535\n",
      "Step [18694/38000] Loss: 3.9715633392333984\n",
      "Step [18695/38000] Loss: 3.304183006286621\n",
      "Step [18696/38000] Loss: 3.567779541015625\n",
      "Step [18697/38000] Loss: 3.5638463497161865\n",
      "Step [18698/38000] Loss: 3.49542498588562\n",
      "Step [18699/38000] Loss: 3.5425872802734375\n",
      "Step [18700/38000] Loss: 3.366001605987549\n",
      "Test Loss: 3.0062217712402344\n",
      "Step [18701/38000] Loss: 3.3120806217193604\n",
      "Step [18702/38000] Loss: 3.337489128112793\n",
      "Step [18703/38000] Loss: 3.1466903686523438\n",
      "Step [18704/38000] Loss: 3.472308874130249\n",
      "Step [18705/38000] Loss: 3.5252292156219482\n",
      "Step [18706/38000] Loss: 3.4864845275878906\n",
      "Step [18707/38000] Loss: 3.460604667663574\n",
      "Step [18708/38000] Loss: 3.336966037750244\n",
      "Step [18709/38000] Loss: 3.4065966606140137\n",
      "Step [18710/38000] Loss: 3.6239380836486816\n",
      "Step [18711/38000] Loss: 3.4962284564971924\n",
      "Step [18712/38000] Loss: 3.6468400955200195\n",
      "Step [18713/38000] Loss: 3.477372646331787\n",
      "Step [18714/38000] Loss: 3.7152748107910156\n",
      "Step [18715/38000] Loss: 3.992886543273926\n",
      "Step [18716/38000] Loss: 3.4107666015625\n",
      "Step [18717/38000] Loss: 3.2251858711242676\n",
      "Step [18718/38000] Loss: 3.3606531620025635\n",
      "Step [18719/38000] Loss: 3.410614252090454\n",
      "Step [18720/38000] Loss: 3.7854676246643066\n",
      "Step [18721/38000] Loss: 3.796381950378418\n",
      "Step [18722/38000] Loss: 3.337162733078003\n",
      "Step [18723/38000] Loss: 3.269697666168213\n",
      "Step [18724/38000] Loss: 3.2811617851257324\n",
      "Step [18725/38000] Loss: 3.1490159034729004\n",
      "Step [18726/38000] Loss: 3.59206485748291\n",
      "Step [18727/38000] Loss: 3.1851541996002197\n",
      "Step [18728/38000] Loss: 3.204298973083496\n",
      "Step [18729/38000] Loss: 3.260373592376709\n",
      "Step [18730/38000] Loss: 3.2141623497009277\n",
      "Step [18731/38000] Loss: 3.3592913150787354\n",
      "Step [18732/38000] Loss: 3.315160036087036\n",
      "Step [18733/38000] Loss: 3.1993494033813477\n",
      "Step [18734/38000] Loss: 3.274972915649414\n",
      "Step [18735/38000] Loss: 3.29611873626709\n",
      "Step [18736/38000] Loss: 3.516056776046753\n",
      "Step [18737/38000] Loss: 3.8920445442199707\n",
      "Step [18738/38000] Loss: 3.0837037563323975\n",
      "Step [18739/38000] Loss: 3.4642481803894043\n",
      "Step [18740/38000] Loss: 3.382948398590088\n",
      "Step [18741/38000] Loss: 3.713381767272949\n",
      "Step [18742/38000] Loss: 3.9157071113586426\n",
      "Step [18743/38000] Loss: 3.562013864517212\n",
      "Step [18744/38000] Loss: 3.8216536045074463\n",
      "Step [18745/38000] Loss: 3.141786575317383\n",
      "Step [18746/38000] Loss: 3.277348041534424\n",
      "Step [18747/38000] Loss: 3.2231171131134033\n",
      "Step [18748/38000] Loss: 3.719480514526367\n",
      "Step [18749/38000] Loss: 3.9078593254089355\n",
      "Step [18750/38000] Loss: 3.172529697418213\n",
      "Step [18751/38000] Loss: 3.2591779232025146\n",
      "Step [18752/38000] Loss: 3.2275209426879883\n",
      "Step [18753/38000] Loss: 3.665163516998291\n",
      "Step [18754/38000] Loss: 3.143342971801758\n",
      "Step [18755/38000] Loss: 3.181983709335327\n",
      "Step [18756/38000] Loss: 3.3051939010620117\n",
      "Step [18757/38000] Loss: 3.234248638153076\n",
      "Step [18758/38000] Loss: 3.2602005004882812\n",
      "Step [18759/38000] Loss: 3.3489601612091064\n",
      "Step [18760/38000] Loss: 3.4236979484558105\n",
      "Step [18761/38000] Loss: 3.274569034576416\n",
      "Step [18762/38000] Loss: 3.3797693252563477\n",
      "Step [18763/38000] Loss: 3.350113868713379\n",
      "Step [18764/38000] Loss: 3.2162253856658936\n",
      "Step [18765/38000] Loss: 3.1652374267578125\n",
      "Step [18766/38000] Loss: 3.1742725372314453\n",
      "Step [18767/38000] Loss: 3.2644572257995605\n",
      "Step [18768/38000] Loss: 3.098177909851074\n",
      "Step [18769/38000] Loss: 3.2634894847869873\n",
      "Step [18770/38000] Loss: 3.4040579795837402\n",
      "Step [18771/38000] Loss: 3.3867907524108887\n",
      "Step [18772/38000] Loss: 3.416395664215088\n",
      "Step [18773/38000] Loss: 3.400172472000122\n",
      "Step [18774/38000] Loss: 3.4906721115112305\n",
      "Step [18775/38000] Loss: 3.5626344680786133\n",
      "Step [18776/38000] Loss: 3.2575316429138184\n",
      "Step [18777/38000] Loss: 3.283977746963501\n",
      "Step [18778/38000] Loss: 3.803565502166748\n",
      "Step [18779/38000] Loss: 3.4589290618896484\n",
      "Step [18780/38000] Loss: 3.141789674758911\n",
      "Step [18781/38000] Loss: 3.1458489894866943\n",
      "Step [18782/38000] Loss: 3.116570472717285\n",
      "Step [18783/38000] Loss: 3.1271800994873047\n",
      "Step [18784/38000] Loss: 3.2286970615386963\n",
      "Step [18785/38000] Loss: 3.169434070587158\n",
      "Step [18786/38000] Loss: 3.3476147651672363\n",
      "Step [18787/38000] Loss: 3.3500635623931885\n",
      "Step [18788/38000] Loss: 3.3042635917663574\n",
      "Step [18789/38000] Loss: 3.2703161239624023\n",
      "Step [18790/38000] Loss: 3.2691712379455566\n",
      "Step [18791/38000] Loss: 3.120820999145508\n",
      "Step [18792/38000] Loss: 3.192610740661621\n",
      "Step [18793/38000] Loss: 3.118581533432007\n",
      "Step [18794/38000] Loss: 3.1275200843811035\n",
      "Step [18795/38000] Loss: 3.0732421875\n",
      "Step [18796/38000] Loss: 3.956242322921753\n",
      "Step [18797/38000] Loss: 3.4467039108276367\n",
      "Step [18798/38000] Loss: 3.169019937515259\n",
      "Step [18799/38000] Loss: 3.100705146789551\n",
      "Step [18800/38000] Loss: 3.080301284790039\n",
      "Test Loss: 2.898037910461426\n",
      "Step [18801/38000] Loss: 2.9829163551330566\n",
      "Step [18802/38000] Loss: 3.2065842151641846\n",
      "Step [18803/38000] Loss: 3.0386736392974854\n",
      "Step [18804/38000] Loss: 3.139345407485962\n",
      "Step [18805/38000] Loss: 3.111769676208496\n",
      "Step [18806/38000] Loss: 2.995723247528076\n",
      "Step [18807/38000] Loss: 2.9693050384521484\n",
      "Step [18808/38000] Loss: 3.1042542457580566\n",
      "Step [18809/38000] Loss: 3.0726659297943115\n",
      "Step [18810/38000] Loss: 3.684907913208008\n",
      "Step [18811/38000] Loss: 3.908972978591919\n",
      "Step [18812/38000] Loss: 3.2237935066223145\n",
      "Step [18813/38000] Loss: 3.117328643798828\n",
      "Step [18814/38000] Loss: 3.0723557472229004\n",
      "Step [18815/38000] Loss: 2.997142791748047\n",
      "Step [18816/38000] Loss: 2.964872360229492\n",
      "Step [18817/38000] Loss: 3.0555338859558105\n",
      "Step [18818/38000] Loss: 3.103412628173828\n",
      "Step [18819/38000] Loss: 2.9783496856689453\n",
      "Step [18820/38000] Loss: 3.115248203277588\n",
      "Step [18821/38000] Loss: 2.9062623977661133\n",
      "Step [18822/38000] Loss: 3.364708423614502\n",
      "Step [18823/38000] Loss: 3.69696044921875\n",
      "Step [18824/38000] Loss: 2.7921206951141357\n",
      "Step [18825/38000] Loss: 3.0036330223083496\n",
      "Step [18826/38000] Loss: 3.0399746894836426\n",
      "Step [18827/38000] Loss: 3.0122804641723633\n",
      "Step [18828/38000] Loss: 3.0961592197418213\n",
      "Step [18829/38000] Loss: 3.20611310005188\n",
      "Step [18830/38000] Loss: 3.149245262145996\n",
      "Step [18831/38000] Loss: 3.156280755996704\n",
      "Step [18832/38000] Loss: 3.1260573863983154\n",
      "Step [18833/38000] Loss: 3.056910991668701\n",
      "Step [18834/38000] Loss: 3.0575859546661377\n",
      "Step [18835/38000] Loss: 3.035583257675171\n",
      "Step [18836/38000] Loss: 3.077528953552246\n",
      "Step [18837/38000] Loss: 2.992232084274292\n",
      "Step [18838/38000] Loss: 3.6489319801330566\n",
      "Step [18839/38000] Loss: 2.802542209625244\n",
      "Step [18840/38000] Loss: 3.1326537132263184\n",
      "Step [18841/38000] Loss: 3.1822667121887207\n",
      "Step [18842/38000] Loss: 3.1467955112457275\n",
      "Step [18843/38000] Loss: 3.032391309738159\n",
      "Step [18844/38000] Loss: 3.001404285430908\n",
      "Step [18845/38000] Loss: 3.1562631130218506\n",
      "Step [18846/38000] Loss: 3.1516079902648926\n",
      "Step [18847/38000] Loss: 3.149613380432129\n",
      "Step [18848/38000] Loss: 3.1489434242248535\n",
      "Step [18849/38000] Loss: 3.1942451000213623\n",
      "Step [18850/38000] Loss: 3.575396776199341\n",
      "Step [18851/38000] Loss: 3.1974246501922607\n",
      "Step [18852/38000] Loss: 3.183250665664673\n",
      "Step [18853/38000] Loss: 3.176116943359375\n",
      "Step [18854/38000] Loss: 3.176637887954712\n",
      "Step [18855/38000] Loss: 3.0073859691619873\n",
      "Step [18856/38000] Loss: 2.9494001865386963\n",
      "Step [18857/38000] Loss: 2.8840456008911133\n",
      "Step [18858/38000] Loss: 3.3847029209136963\n",
      "Step [18859/38000] Loss: 3.854050636291504\n",
      "Step [18860/38000] Loss: 2.699197769165039\n",
      "Step [18861/38000] Loss: 2.9492006301879883\n",
      "Step [18862/38000] Loss: 3.393515110015869\n",
      "Step [18863/38000] Loss: 3.7278988361358643\n",
      "Step [18864/38000] Loss: 2.5323047637939453\n",
      "Step [18865/38000] Loss: 3.170559883117676\n",
      "Step [18866/38000] Loss: 3.20314359664917\n",
      "Step [18867/38000] Loss: 3.164608955383301\n",
      "Step [18868/38000] Loss: 3.099951982498169\n",
      "Step [18869/38000] Loss: 3.0080270767211914\n",
      "Step [18870/38000] Loss: 3.120997905731201\n",
      "Step [18871/38000] Loss: 3.1074607372283936\n",
      "Step [18872/38000] Loss: 3.2102015018463135\n",
      "Step [18873/38000] Loss: 3.0797088146209717\n",
      "Step [18874/38000] Loss: 3.143972873687744\n",
      "Step [18875/38000] Loss: 2.8665812015533447\n",
      "Step [18876/38000] Loss: 3.0276029109954834\n",
      "Step [18877/38000] Loss: 3.1308422088623047\n",
      "Step [18878/38000] Loss: 3.160752058029175\n",
      "Step [18879/38000] Loss: 3.173515558242798\n",
      "Step [18880/38000] Loss: 2.9860334396362305\n",
      "Step [18881/38000] Loss: 3.620351552963257\n",
      "Step [18882/38000] Loss: 2.7432827949523926\n",
      "Step [18883/38000] Loss: 2.8171579837799072\n",
      "Step [18884/38000] Loss: 2.9473633766174316\n",
      "Step [18885/38000] Loss: 2.974280834197998\n",
      "Step [18886/38000] Loss: 3.155691146850586\n",
      "Step [18887/38000] Loss: 3.160871982574463\n",
      "Step [18888/38000] Loss: 3.2011404037475586\n",
      "Step [18889/38000] Loss: 3.1726715564727783\n",
      "Step [18890/38000] Loss: 2.9207425117492676\n",
      "Step [18891/38000] Loss: 2.9643521308898926\n",
      "Step [18892/38000] Loss: 3.0906877517700195\n",
      "Step [18893/38000] Loss: 3.071545124053955\n",
      "Step [18894/38000] Loss: 3.1054892539978027\n",
      "Step [18895/38000] Loss: 3.05012845993042\n",
      "Step [18896/38000] Loss: 3.100187301635742\n",
      "Step [18897/38000] Loss: 2.7964091300964355\n",
      "Step [18898/38000] Loss: 2.845879316329956\n",
      "Step [18899/38000] Loss: 2.8205060958862305\n",
      "Step [18900/38000] Loss: 2.9126906394958496\n",
      "Test Loss: 2.635098457336426\n",
      "Step [18901/38000] Loss: 2.8730661869049072\n",
      "Step [18902/38000] Loss: 2.8976712226867676\n",
      "Step [18903/38000] Loss: 2.816011667251587\n",
      "Step [18904/38000] Loss: 2.8485989570617676\n",
      "Step [18905/38000] Loss: 2.833963394165039\n",
      "Step [18906/38000] Loss: 2.848771572113037\n",
      "Step [18907/38000] Loss: 2.7618725299835205\n",
      "Step [18908/38000] Loss: 2.969773769378662\n",
      "Step [18909/38000] Loss: 2.9005634784698486\n",
      "Step [18910/38000] Loss: 2.8975558280944824\n",
      "Step [18911/38000] Loss: 2.81935453414917\n",
      "Step [18912/38000] Loss: 2.7455525398254395\n",
      "Step [18913/38000] Loss: 2.839232921600342\n",
      "Step [18914/38000] Loss: 2.834815502166748\n",
      "Step [18915/38000] Loss: 2.814213275909424\n",
      "Step [18916/38000] Loss: 2.9826364517211914\n",
      "Step [18917/38000] Loss: 2.9991512298583984\n",
      "Step [18918/38000] Loss: 3.010183334350586\n",
      "Step [18919/38000] Loss: 2.9722938537597656\n",
      "Step [18920/38000] Loss: 2.9703316688537598\n",
      "Step [18921/38000] Loss: 2.7775521278381348\n",
      "Step [18922/38000] Loss: 2.8675670623779297\n",
      "Step [18923/38000] Loss: 2.887599468231201\n",
      "Step [18924/38000] Loss: 2.899833917617798\n",
      "Step [18925/38000] Loss: 2.847343921661377\n",
      "Step [18926/38000] Loss: 2.8142151832580566\n",
      "Step [18927/38000] Loss: 2.7621185779571533\n",
      "Step [18928/38000] Loss: 2.792222023010254\n",
      "Step [18929/38000] Loss: 2.7651426792144775\n",
      "Step [18930/38000] Loss: 2.7567901611328125\n",
      "Step [18931/38000] Loss: 2.7825393676757812\n",
      "Step [18932/38000] Loss: 2.8422353267669678\n",
      "Step [18933/38000] Loss: 2.7740976810455322\n",
      "Step [18934/38000] Loss: 2.920335292816162\n",
      "Step [18935/38000] Loss: 3.0482563972473145\n",
      "Step [18936/38000] Loss: 3.063166379928589\n",
      "Step [18937/38000] Loss: 3.105440616607666\n",
      "Step [18938/38000] Loss: 3.0744049549102783\n",
      "Step [18939/38000] Loss: 2.6634693145751953\n",
      "Step [18940/38000] Loss: 2.726072072982788\n",
      "Step [18941/38000] Loss: 2.742677927017212\n",
      "Step [18942/38000] Loss: 2.7408719062805176\n",
      "Step [18943/38000] Loss: 2.6597371101379395\n",
      "Step [18944/38000] Loss: 2.731626510620117\n",
      "Step [18945/38000] Loss: 2.729311943054199\n",
      "Step [18946/38000] Loss: 2.745026111602783\n",
      "Step [18947/38000] Loss: 2.7410740852355957\n",
      "Step [18948/38000] Loss: 2.6841378211975098\n",
      "Step [18949/38000] Loss: 2.703505277633667\n",
      "Step [18950/38000] Loss: 3.37711238861084\n",
      "Step [18951/38000] Loss: 2.819671154022217\n",
      "Step [18952/38000] Loss: 2.8910446166992188\n",
      "Step [18953/38000] Loss: 2.8695836067199707\n",
      "Step [18954/38000] Loss: 2.901580810546875\n",
      "Step [18955/38000] Loss: 2.9515581130981445\n",
      "Step [18956/38000] Loss: 2.768350601196289\n",
      "Step [18957/38000] Loss: 2.8423266410827637\n",
      "Step [18958/38000] Loss: 3.106858253479004\n",
      "Step [18959/38000] Loss: 2.7018446922302246\n",
      "Step [18960/38000] Loss: 2.906576156616211\n",
      "Step [18961/38000] Loss: 2.974515676498413\n",
      "Step [18962/38000] Loss: 2.8616251945495605\n",
      "Step [18963/38000] Loss: 2.8621792793273926\n",
      "Step [18964/38000] Loss: 2.8416433334350586\n",
      "Step [18965/38000] Loss: 2.5884861946105957\n",
      "Step [18966/38000] Loss: 2.689093589782715\n",
      "Step [18967/38000] Loss: 2.7459094524383545\n",
      "Step [18968/38000] Loss: 2.7343692779541016\n",
      "Step [18969/38000] Loss: 2.679708480834961\n",
      "Step [18970/38000] Loss: 2.645474433898926\n",
      "Step [18971/38000] Loss: 2.9014389514923096\n",
      "Step [18972/38000] Loss: 3.7744436264038086\n",
      "Step [18973/38000] Loss: 2.9911413192749023\n",
      "Step [18974/38000] Loss: 2.7137370109558105\n",
      "Step [18975/38000] Loss: 2.6332287788391113\n",
      "Step [18976/38000] Loss: 2.6643829345703125\n",
      "Step [18977/38000] Loss: 2.6983389854431152\n",
      "Step [18978/38000] Loss: 2.604687213897705\n",
      "Step [18979/38000] Loss: 2.7102854251861572\n",
      "Step [18980/38000] Loss: 2.825828790664673\n",
      "Step [18981/38000] Loss: 3.026184558868408\n",
      "Step [18982/38000] Loss: 2.9372379779815674\n",
      "Step [18983/38000] Loss: 2.8494606018066406\n",
      "Step [18984/38000] Loss: 2.862462043762207\n",
      "Step [18985/38000] Loss: 2.680384635925293\n",
      "Step [18986/38000] Loss: 2.630751609802246\n",
      "Step [18987/38000] Loss: 2.561710834503174\n",
      "Step [18988/38000] Loss: 2.5240895748138428\n",
      "Step [18989/38000] Loss: 2.703543186187744\n",
      "Step [18990/38000] Loss: 2.6296043395996094\n",
      "Step [18991/38000] Loss: 2.485750198364258\n",
      "Step [18992/38000] Loss: 3.6142449378967285\n",
      "Step [18993/38000] Loss: 3.3599343299865723\n",
      "Step [18994/38000] Loss: 2.6279373168945312\n",
      "Step [18995/38000] Loss: 2.5962724685668945\n",
      "Step [18996/38000] Loss: 2.8345394134521484\n",
      "Step [18997/38000] Loss: 2.909982681274414\n",
      "Step [18998/38000] Loss: 2.8145627975463867\n",
      "Step [18999/38000] Loss: 2.853868007659912\n",
      "Step [19000/38000] Loss: 2.952819347381592\n",
      "Test Loss: 2.591827869415283\n",
      "Step [19001/38000] Loss: 2.791588306427002\n",
      "Step [19002/38000] Loss: 2.854097843170166\n",
      "Step [19003/38000] Loss: 2.9197564125061035\n",
      "Step [19004/38000] Loss: 2.829707384109497\n",
      "Step [19005/38000] Loss: 2.6627159118652344\n",
      "Step [19006/38000] Loss: 2.795849084854126\n",
      "Step [19007/38000] Loss: 2.8212356567382812\n",
      "Step [19008/38000] Loss: 2.8173842430114746\n",
      "Step [19009/38000] Loss: 2.8650927543640137\n",
      "Step [19010/38000] Loss: 2.8642520904541016\n",
      "Step [19011/38000] Loss: 2.575495481491089\n",
      "Step [19012/38000] Loss: 2.582500457763672\n",
      "Step [19013/38000] Loss: 2.585775136947632\n",
      "Step [19014/38000] Loss: 2.5849709510803223\n",
      "Step [19015/38000] Loss: 2.6562976837158203\n",
      "Step [19016/38000] Loss: 2.6376285552978516\n",
      "Step [19017/38000] Loss: 2.679507255554199\n",
      "Step [19018/38000] Loss: 2.6129310131073\n",
      "Step [19019/38000] Loss: 3.083102226257324\n",
      "Step [19020/38000] Loss: 3.748486042022705\n",
      "Step [19021/38000] Loss: 2.3600869178771973\n",
      "Step [19022/38000] Loss: 2.4983108043670654\n",
      "Step [19023/38000] Loss: 2.5215461254119873\n",
      "Step [19024/38000] Loss: 2.5059399604797363\n",
      "Step [19025/38000] Loss: 2.5714824199676514\n",
      "Step [19026/38000] Loss: 2.520779848098755\n",
      "Step [19027/38000] Loss: 2.60560941696167\n",
      "Step [19028/38000] Loss: 2.5439300537109375\n",
      "Step [19029/38000] Loss: 2.4790501594543457\n",
      "Step [19030/38000] Loss: 2.5940093994140625\n",
      "Step [19031/38000] Loss: 2.503103256225586\n",
      "Step [19032/38000] Loss: 3.4282546043395996\n",
      "Step [19033/38000] Loss: 2.5142884254455566\n",
      "Step [19034/38000] Loss: 2.4195556640625\n",
      "Step [19035/38000] Loss: 2.5124387741088867\n",
      "Step [19036/38000] Loss: 2.5557267665863037\n",
      "Step [19037/38000] Loss: 2.664591073989868\n",
      "Step [19038/38000] Loss: 2.462677001953125\n",
      "Step [19039/38000] Loss: 2.5987157821655273\n",
      "Step [19040/38000] Loss: 2.4921841621398926\n",
      "Step [19041/38000] Loss: 2.601536512374878\n",
      "Step [19042/38000] Loss: 2.482800006866455\n",
      "Step [19043/38000] Loss: 2.4182772636413574\n",
      "Step [19044/38000] Loss: 2.4738924503326416\n",
      "Step [19045/38000] Loss: 2.723587989807129\n",
      "Step [19046/38000] Loss: 2.7646303176879883\n",
      "Step [19047/38000] Loss: 2.782245635986328\n",
      "Step [19048/38000] Loss: 2.8006930351257324\n",
      "Step [19049/38000] Loss: 2.6883745193481445\n",
      "Step [19050/38000] Loss: 2.5053272247314453\n",
      "Step [19051/38000] Loss: 2.642564535140991\n",
      "Step [19052/38000] Loss: 2.538881301879883\n",
      "Step [19053/38000] Loss: 2.584465980529785\n",
      "Step [19054/38000] Loss: 2.620969772338867\n",
      "Step [19055/38000] Loss: 2.6006014347076416\n",
      "Step [19056/38000] Loss: 2.470327377319336\n",
      "Step [19057/38000] Loss: 2.551889181137085\n",
      "Step [19058/38000] Loss: 2.5180211067199707\n",
      "Step [19059/38000] Loss: 2.4971506595611572\n",
      "Step [19060/38000] Loss: 3.588914394378662\n",
      "Step [19061/38000] Loss: 3.732145071029663\n",
      "Step [19062/38000] Loss: 2.9852914810180664\n",
      "Step [19063/38000] Loss: 2.4948253631591797\n",
      "Step [19064/38000] Loss: 2.458667516708374\n",
      "Step [19065/38000] Loss: 2.47044038772583\n",
      "Step [19066/38000] Loss: 2.43626070022583\n",
      "Step [19067/38000] Loss: 3.4661970138549805\n",
      "Step [19068/38000] Loss: 2.3729538917541504\n",
      "Step [19069/38000] Loss: 2.9620790481567383\n",
      "Step [19070/38000] Loss: 3.716972589492798\n",
      "Step [19071/38000] Loss: 2.2971901893615723\n",
      "Step [19072/38000] Loss: 2.5045206546783447\n",
      "Step [19073/38000] Loss: 2.452016830444336\n",
      "Step [19074/38000] Loss: 2.468230962753296\n",
      "Step [19075/38000] Loss: 3.7026734352111816\n",
      "Step [19076/38000] Loss: 3.0693905353546143\n",
      "Step [19077/38000] Loss: 2.4816179275512695\n",
      "Step [19078/38000] Loss: 2.5636892318725586\n",
      "Step [19079/38000] Loss: 2.442213535308838\n",
      "Step [19080/38000] Loss: 2.4308345317840576\n",
      "Step [19081/38000] Loss: 2.413357734680176\n",
      "Step [19082/38000] Loss: 3.253462553024292\n",
      "Step [19083/38000] Loss: 2.5507125854492188\n",
      "Step [19084/38000] Loss: 2.61611008644104\n",
      "Step [19085/38000] Loss: 2.670492172241211\n",
      "Step [19086/38000] Loss: 2.796450614929199\n",
      "Step [19087/38000] Loss: 2.8095345497131348\n",
      "Step [19088/38000] Loss: 2.7460451126098633\n",
      "Step [19089/38000] Loss: 3.8412764072418213\n",
      "Step [19090/38000] Loss: 3.1279473304748535\n",
      "Step [19091/38000] Loss: 3.218276262283325\n",
      "Step [19092/38000] Loss: 2.5182995796203613\n",
      "Step [19093/38000] Loss: 3.6133930683135986\n",
      "Step [19094/38000] Loss: 2.915858268737793\n",
      "Step [19095/38000] Loss: 2.366297483444214\n",
      "Step [19096/38000] Loss: 2.38720965385437\n",
      "Step [19097/38000] Loss: 2.856869697570801\n",
      "Step [19098/38000] Loss: 3.8164079189300537\n",
      "Step [19099/38000] Loss: 2.462696075439453\n",
      "Step [19100/38000] Loss: 2.464387893676758\n",
      "Test Loss: 2.5115270614624023\n",
      "Step [19101/38000] Loss: 2.5021560192108154\n",
      "Step [19102/38000] Loss: 2.39310884475708\n",
      "Step [19103/38000] Loss: 2.467332363128662\n",
      "Step [19104/38000] Loss: 2.3831562995910645\n",
      "Step [19105/38000] Loss: 2.438119411468506\n",
      "Step [19106/38000] Loss: 2.4991111755371094\n",
      "Step [19107/38000] Loss: 3.6457815170288086\n",
      "Step [19108/38000] Loss: 3.0787241458892822\n",
      "Step [19109/38000] Loss: 2.5607714653015137\n",
      "Step [19110/38000] Loss: 2.677773952484131\n",
      "Step [19111/38000] Loss: 2.7049102783203125\n",
      "Step [19112/38000] Loss: 2.589500904083252\n",
      "Step [19113/38000] Loss: 2.665581703186035\n",
      "Step [19114/38000] Loss: 2.404857873916626\n",
      "Step [19115/38000] Loss: 2.417438507080078\n",
      "Step [19116/38000] Loss: 2.3340413570404053\n",
      "Step [19117/38000] Loss: 2.7004342079162598\n",
      "Step [19118/38000] Loss: 2.7011446952819824\n",
      "Step [19119/38000] Loss: 2.679691791534424\n",
      "Step [19120/38000] Loss: 2.569061040878296\n",
      "Step [19121/38000] Loss: 2.6333799362182617\n",
      "Step [19122/38000] Loss: 2.2861621379852295\n",
      "Step [19123/38000] Loss: 2.4175753593444824\n",
      "Step [19124/38000] Loss: 2.432292938232422\n",
      "Step [19125/38000] Loss: 2.514492988586426\n",
      "Step [19126/38000] Loss: 2.541933536529541\n",
      "Step [19127/38000] Loss: 2.4920401573181152\n",
      "Step [19128/38000] Loss: 2.5379207134246826\n",
      "Step [19129/38000] Loss: 2.5178160667419434\n",
      "Step [19130/38000] Loss: 2.3377087116241455\n",
      "Step [19131/38000] Loss: 2.3445816040039062\n",
      "Step [19132/38000] Loss: 2.305899143218994\n",
      "Step [19133/38000] Loss: 2.3390660285949707\n",
      "Step [19134/38000] Loss: 2.5665268898010254\n",
      "Step [19135/38000] Loss: 2.720325469970703\n",
      "Step [19136/38000] Loss: 2.722787857055664\n",
      "Step [19137/38000] Loss: 2.6656382083892822\n",
      "Step [19138/38000] Loss: 2.765209913253784\n",
      "Step [19139/38000] Loss: 2.39621901512146\n",
      "Step [19140/38000] Loss: 2.418790817260742\n",
      "Step [19141/38000] Loss: 2.4255123138427734\n",
      "Step [19142/38000] Loss: 2.370603322982788\n",
      "Step [19143/38000] Loss: 2.2280125617980957\n",
      "Step [19144/38000] Loss: 2.3652026653289795\n",
      "Step [19145/38000] Loss: 2.2393980026245117\n",
      "Step [19146/38000] Loss: 2.396289348602295\n",
      "Step [19147/38000] Loss: 2.276705503463745\n",
      "Step [19148/38000] Loss: 2.344424247741699\n",
      "Step [19149/38000] Loss: 2.385607957839966\n",
      "Step [19150/38000] Loss: 2.3896584510803223\n",
      "Step [19151/38000] Loss: 2.324502468109131\n",
      "Step [19152/38000] Loss: 2.380629539489746\n",
      "Step [19153/38000] Loss: 2.3967487812042236\n",
      "Step [19154/38000] Loss: 2.4017648696899414\n",
      "Step [19155/38000] Loss: 2.415966033935547\n",
      "Step [19156/38000] Loss: 2.4501290321350098\n",
      "Step [19157/38000] Loss: 2.380626678466797\n",
      "Step [19158/38000] Loss: 2.384101629257202\n",
      "Step [19159/38000] Loss: 2.3323135375976562\n",
      "Step [19160/38000] Loss: 2.484501361846924\n",
      "Step [19161/38000] Loss: 3.6570725440979004\n",
      "Step [19162/38000] Loss: 2.8906028270721436\n",
      "Step [19163/38000] Loss: 2.4220378398895264\n",
      "Step [19164/38000] Loss: 2.5503368377685547\n",
      "Step [19165/38000] Loss: 3.6955013275146484\n",
      "Step [19166/38000] Loss: 2.996523141860962\n",
      "Step [19167/38000] Loss: 2.7469005584716797\n",
      "Step [19168/38000] Loss: 2.636530876159668\n",
      "Step [19169/38000] Loss: 2.6165237426757812\n",
      "Step [19170/38000] Loss: 2.7173500061035156\n",
      "Step [19171/38000] Loss: 2.440101385116577\n",
      "Step [19172/38000] Loss: 2.4126176834106445\n",
      "Step [19173/38000] Loss: 2.4339938163757324\n",
      "Step [19174/38000] Loss: 2.484452962875366\n",
      "Step [19175/38000] Loss: 2.3805627822875977\n",
      "Step [19176/38000] Loss: 2.400850296020508\n",
      "Step [19177/38000] Loss: 2.236356735229492\n",
      "Step [19178/38000] Loss: 2.2548294067382812\n",
      "Step [19179/38000] Loss: 2.3066039085388184\n",
      "Step [19180/38000] Loss: 2.5821619033813477\n",
      "Step [19181/38000] Loss: 2.580587863922119\n",
      "Step [19182/38000] Loss: 2.646378517150879\n",
      "Step [19183/38000] Loss: 2.674525260925293\n",
      "Step [19184/38000] Loss: 2.6611695289611816\n",
      "Step [19185/38000] Loss: 2.3903772830963135\n",
      "Step [19186/38000] Loss: 2.767470359802246\n",
      "Step [19187/38000] Loss: 2.9142520427703857\n",
      "Step [19188/38000] Loss: 2.1881468296051025\n",
      "Step [19189/38000] Loss: 2.2940077781677246\n",
      "Step [19190/38000] Loss: 2.2527546882629395\n",
      "Step [19191/38000] Loss: 2.2756147384643555\n",
      "Step [19192/38000] Loss: 2.3391265869140625\n",
      "Step [19193/38000] Loss: 2.3136091232299805\n",
      "Step [19194/38000] Loss: 2.4703633785247803\n",
      "Step [19195/38000] Loss: 2.5131239891052246\n",
      "Step [19196/38000] Loss: 2.6218814849853516\n",
      "Step [19197/38000] Loss: 2.524660110473633\n",
      "Step [19198/38000] Loss: 2.5517964363098145\n",
      "Step [19199/38000] Loss: 2.389995574951172\n",
      "Step [19200/38000] Loss: 2.4082581996917725\n",
      "Test Loss: 2.443225622177124\n",
      "Step [19201/38000] Loss: 2.303140640258789\n",
      "Step [19202/38000] Loss: 2.3197951316833496\n",
      "Step [19203/38000] Loss: 2.3570971488952637\n",
      "Step [19204/38000] Loss: 2.209409236907959\n",
      "Step [19205/38000] Loss: 2.224240779876709\n",
      "Step [19206/38000] Loss: 2.2065675258636475\n",
      "Step [19207/38000] Loss: 2.207313060760498\n",
      "Step [19208/38000] Loss: 2.2869184017181396\n",
      "Step [19209/38000] Loss: 2.2635860443115234\n",
      "Step [19210/38000] Loss: 2.1563968658447266\n",
      "Step [19211/38000] Loss: 2.3102030754089355\n",
      "Step [19212/38000] Loss: 3.105550765991211\n",
      "Step [19213/38000] Loss: 3.687967538833618\n",
      "Step [19214/38000] Loss: 2.278775930404663\n",
      "Step [19215/38000] Loss: 2.211531162261963\n",
      "Step [19216/38000] Loss: 2.1543898582458496\n",
      "Step [19217/38000] Loss: 2.3122425079345703\n",
      "Step [19218/38000] Loss: 3.5631814002990723\n",
      "Step [19219/38000] Loss: 2.7733607292175293\n",
      "Step [19220/38000] Loss: 2.2247517108917236\n",
      "Step [19221/38000] Loss: 2.3275389671325684\n",
      "Step [19222/38000] Loss: 3.600365400314331\n",
      "Step [19223/38000] Loss: 2.7344412803649902\n",
      "Step [19224/38000] Loss: 2.2319836616516113\n",
      "Step [19225/38000] Loss: 2.283881187438965\n",
      "Step [19226/38000] Loss: 2.2822229862213135\n",
      "Step [19227/38000] Loss: 2.28261661529541\n",
      "Step [19228/38000] Loss: 2.2288427352905273\n",
      "Step [19229/38000] Loss: 2.209681749343872\n",
      "Step [19230/38000] Loss: 2.2829694747924805\n",
      "Step [19231/38000] Loss: 2.262364625930786\n",
      "Step [19232/38000] Loss: 2.23037052154541\n",
      "Step [19233/38000] Loss: 2.18703031539917\n",
      "Step [19234/38000] Loss: 2.2773056030273438\n",
      "Step [19235/38000] Loss: 2.1565258502960205\n",
      "Step [19236/38000] Loss: 2.2167282104492188\n",
      "Step [19237/38000] Loss: 2.1081480979919434\n",
      "Step [19238/38000] Loss: 2.484285354614258\n",
      "Step [19239/38000] Loss: 2.434723377227783\n",
      "Step [19240/38000] Loss: 2.5636801719665527\n",
      "Step [19241/38000] Loss: 2.3948020935058594\n",
      "Step [19242/38000] Loss: 2.557138204574585\n",
      "Step [19243/38000] Loss: 3.4632530212402344\n",
      "Step [19244/38000] Loss: 2.912278175354004\n",
      "Step [19245/38000] Loss: 2.2315001487731934\n",
      "Step [19246/38000] Loss: 2.2472496032714844\n",
      "Step [19247/38000] Loss: 2.1576054096221924\n",
      "Step [19248/38000] Loss: 2.3627820014953613\n",
      "Step [19249/38000] Loss: 2.396749973297119\n",
      "Step [19250/38000] Loss: 2.4548139572143555\n",
      "Step [19251/38000] Loss: 2.480950355529785\n",
      "Step [19252/38000] Loss: 2.489942789077759\n",
      "Step [19253/38000] Loss: 2.3274030685424805\n",
      "Step [19254/38000] Loss: 2.2869420051574707\n",
      "Step [19255/38000] Loss: 2.3152618408203125\n",
      "Step [19256/38000] Loss: 2.3595361709594727\n",
      "Step [19257/38000] Loss: 2.6779322624206543\n",
      "Step [19258/38000] Loss: 3.604375123977661\n",
      "Step [19259/38000] Loss: 2.6575911045074463\n",
      "Step [19260/38000] Loss: 2.164682626724243\n",
      "Step [19261/38000] Loss: 2.6207101345062256\n",
      "Step [19262/38000] Loss: 3.5386805534362793\n",
      "Step [19263/38000] Loss: 3.681520938873291\n",
      "Step [19264/38000] Loss: 2.1505045890808105\n",
      "Step [19265/38000] Loss: 2.289473533630371\n",
      "Step [19266/38000] Loss: 2.2715344429016113\n",
      "Step [19267/38000] Loss: 3.403594970703125\n",
      "Step [19268/38000] Loss: 3.3202474117279053\n",
      "Step [19269/38000] Loss: 2.3355846405029297\n",
      "Step [19270/38000] Loss: 2.5136725902557373\n",
      "Step [19271/38000] Loss: 2.5405988693237305\n",
      "Step [19272/38000] Loss: 2.458197593688965\n",
      "Step [19273/38000] Loss: 2.4602508544921875\n",
      "Step [19274/38000] Loss: 2.3311994075775146\n",
      "Step [19275/38000] Loss: 2.5958030223846436\n",
      "Step [19276/38000] Loss: 3.5692362785339355\n",
      "Step [19277/38000] Loss: 2.6200084686279297\n",
      "Step [19278/38000] Loss: 2.4143643379211426\n",
      "Step [19279/38000] Loss: 2.572160482406616\n",
      "Step [19280/38000] Loss: 2.528749704360962\n",
      "Step [19281/38000] Loss: 2.5929856300354004\n",
      "Step [19282/38000] Loss: 2.4530582427978516\n",
      "Step [19283/38000] Loss: 2.221212387084961\n",
      "Step [19284/38000] Loss: 2.467397689819336\n",
      "Step [19285/38000] Loss: 2.4951276779174805\n",
      "Step [19286/38000] Loss: 2.5698165893554688\n",
      "Step [19287/38000] Loss: 2.562206745147705\n",
      "Step [19288/38000] Loss: 2.6012046337127686\n",
      "Step [19289/38000] Loss: 2.2065699100494385\n",
      "Step [19290/38000] Loss: 2.1937520503997803\n",
      "Step [19291/38000] Loss: 2.2643299102783203\n",
      "Step [19292/38000] Loss: 2.252908706665039\n",
      "Step [19293/38000] Loss: 2.1957035064697266\n",
      "Step [19294/38000] Loss: 2.220714569091797\n",
      "Step [19295/38000] Loss: 2.1294689178466797\n",
      "Step [19296/38000] Loss: 2.511052370071411\n",
      "Step [19297/38000] Loss: 2.4253737926483154\n",
      "Step [19298/38000] Loss: 2.4070053100585938\n",
      "Step [19299/38000] Loss: 2.408859968185425\n",
      "Step [19300/38000] Loss: 2.376518726348877\n",
      "Test Loss: 2.1554434299468994\n",
      "Step [19301/38000] Loss: 2.2538132667541504\n",
      "Step [19302/38000] Loss: 2.180363655090332\n",
      "Step [19303/38000] Loss: 2.2891950607299805\n",
      "Step [19304/38000] Loss: 2.3185737133026123\n",
      "Step [19305/38000] Loss: 2.2099218368530273\n",
      "Step [19306/38000] Loss: 2.1528196334838867\n",
      "Step [19307/38000] Loss: 2.1290199756622314\n",
      "Step [19308/38000] Loss: 2.1041245460510254\n",
      "Step [19309/38000] Loss: 2.186460494995117\n",
      "Step [19310/38000] Loss: 2.204213857650757\n",
      "Step [19311/38000] Loss: 2.312795639038086\n",
      "Step [19312/38000] Loss: 2.263033151626587\n",
      "Step [19313/38000] Loss: 2.1834871768951416\n",
      "Step [19314/38000] Loss: 2.432626247406006\n",
      "Step [19315/38000] Loss: 3.618356704711914\n",
      "Step [19316/38000] Loss: 2.76776123046875\n",
      "Step [19317/38000] Loss: 2.1955485343933105\n",
      "Step [19318/38000] Loss: 2.2411422729492188\n",
      "Step [19319/38000] Loss: 2.207812786102295\n",
      "Step [19320/38000] Loss: 2.167137384414673\n",
      "Step [19321/38000] Loss: 2.225079298019409\n",
      "Step [19322/38000] Loss: 2.15553617477417\n",
      "Step [19323/38000] Loss: 2.146066188812256\n",
      "Step [19324/38000] Loss: 2.2789525985717773\n",
      "Step [19325/38000] Loss: 2.4466943740844727\n",
      "Step [19326/38000] Loss: 3.7001547813415527\n",
      "Step [19327/38000] Loss: 2.578376531600952\n",
      "Step [19328/38000] Loss: 2.0856246948242188\n",
      "Step [19329/38000] Loss: 2.614825963973999\n",
      "Step [19330/38000] Loss: 3.6608946323394775\n",
      "Step [19331/38000] Loss: 2.389427661895752\n",
      "Step [19332/38000] Loss: 2.1267197132110596\n",
      "Step [19333/38000] Loss: 2.2113020420074463\n",
      "Step [19334/38000] Loss: 3.1610209941864014\n",
      "Step [19335/38000] Loss: 3.4383015632629395\n",
      "Step [19336/38000] Loss: 2.1622872352600098\n",
      "Step [19337/38000] Loss: 2.1731081008911133\n",
      "Step [19338/38000] Loss: 2.1655044555664062\n",
      "Step [19339/38000] Loss: 2.1892707347869873\n",
      "Step [19340/38000] Loss: 2.1414692401885986\n",
      "Step [19341/38000] Loss: 2.2037391662597656\n",
      "Step [19342/38000] Loss: 2.12923526763916\n",
      "Step [19343/38000] Loss: 2.1393680572509766\n",
      "Step [19344/38000] Loss: 2.17453670501709\n",
      "Step [19345/38000] Loss: 2.1640050411224365\n",
      "Step [19346/38000] Loss: 2.20656681060791\n",
      "Step [19347/38000] Loss: 2.0759334564208984\n",
      "Step [19348/38000] Loss: 2.066707134246826\n",
      "Step [19349/38000] Loss: 3.3428380489349365\n",
      "Step [19350/38000] Loss: 3.1771292686462402\n",
      "Step [19351/38000] Loss: 2.197303295135498\n",
      "Step [19352/38000] Loss: 2.1101839542388916\n",
      "Step [19353/38000] Loss: 2.177607536315918\n",
      "Step [19354/38000] Loss: 2.1708626747131348\n",
      "Step [19355/38000] Loss: 2.2088241577148438\n",
      "Step [19356/38000] Loss: 2.2156119346618652\n",
      "Step [19357/38000] Loss: 2.229156494140625\n",
      "Step [19358/38000] Loss: 2.181474208831787\n",
      "Step [19359/38000] Loss: 2.1927242279052734\n",
      "Step [19360/38000] Loss: 3.525650978088379\n",
      "Step [19361/38000] Loss: 3.2478764057159424\n",
      "Step [19362/38000] Loss: 2.1605687141418457\n",
      "Step [19363/38000] Loss: 2.156515121459961\n",
      "Step [19364/38000] Loss: 2.1181697845458984\n",
      "Step [19365/38000] Loss: 2.2629940509796143\n",
      "Step [19366/38000] Loss: 2.1629951000213623\n",
      "Step [19367/38000] Loss: 2.0639047622680664\n",
      "Step [19368/38000] Loss: 2.1962709426879883\n",
      "Step [19369/38000] Loss: 2.0653629302978516\n",
      "Step [19370/38000] Loss: 2.0693681240081787\n",
      "Step [19371/38000] Loss: 2.1780920028686523\n",
      "Step [19372/38000] Loss: 2.5190000534057617\n",
      "Step [19373/38000] Loss: 3.5775864124298096\n",
      "Step [19374/38000] Loss: 2.379265785217285\n",
      "Step [19375/38000] Loss: 2.1746368408203125\n",
      "Step [19376/38000] Loss: 2.103778600692749\n",
      "Step [19377/38000] Loss: 2.3741250038146973\n",
      "Step [19378/38000] Loss: 2.478571653366089\n",
      "Step [19379/38000] Loss: 2.4884486198425293\n",
      "Step [19380/38000] Loss: 2.468660354614258\n",
      "Step [19381/38000] Loss: 2.484302043914795\n",
      "Step [19382/38000] Loss: 1.995971918106079\n",
      "Step [19383/38000] Loss: 2.0745511054992676\n",
      "Step [19384/38000] Loss: 2.0678701400756836\n",
      "Step [19385/38000] Loss: 2.0685133934020996\n",
      "Step [19386/38000] Loss: 2.1642141342163086\n",
      "Step [19387/38000] Loss: 2.0685434341430664\n",
      "Step [19388/38000] Loss: 2.272075653076172\n",
      "Step [19389/38000] Loss: 2.4102988243103027\n",
      "Step [19390/38000] Loss: 2.305176019668579\n",
      "Step [19391/38000] Loss: 2.430898666381836\n",
      "Step [19392/38000] Loss: 2.231438636779785\n",
      "Step [19393/38000] Loss: 2.0645811557769775\n",
      "Step [19394/38000] Loss: 2.0919830799102783\n",
      "Step [19395/38000] Loss: 2.1073532104492188\n",
      "Step [19396/38000] Loss: 2.1300039291381836\n",
      "Step [19397/38000] Loss: 2.766592264175415\n",
      "Step [19398/38000] Loss: 3.607370376586914\n",
      "Step [19399/38000] Loss: 3.7375192642211914\n",
      "Step [19400/38000] Loss: 2.5909314155578613\n",
      "Test Loss: 2.66438627243042\n",
      "Step [19401/38000] Loss: 2.8133487701416016\n",
      "Step [19402/38000] Loss: 2.8622961044311523\n",
      "Step [19403/38000] Loss: 2.933271884918213\n",
      "Step [19404/38000] Loss: 3.015301465988159\n",
      "Step [19405/38000] Loss: 2.832758903503418\n",
      "Step [19406/38000] Loss: 2.867230176925659\n",
      "Step [19407/38000] Loss: 2.8373665809631348\n",
      "Step [19408/38000] Loss: 2.861025333404541\n",
      "Step [19409/38000] Loss: 3.0524978637695312\n",
      "Step [19410/38000] Loss: 3.0922181606292725\n",
      "Step [19411/38000] Loss: 3.058471202850342\n",
      "Step [19412/38000] Loss: 3.163116455078125\n",
      "Step [19413/38000] Loss: 3.0319700241088867\n",
      "Step [19414/38000] Loss: 2.912067174911499\n",
      "Step [19415/38000] Loss: 3.1211609840393066\n",
      "Step [19416/38000] Loss: 3.064373016357422\n",
      "Step [19417/38000] Loss: 3.099367141723633\n",
      "Step [19418/38000] Loss: 3.132291316986084\n",
      "Step [19419/38000] Loss: 3.2067971229553223\n",
      "Step [19420/38000] Loss: 3.0401697158813477\n",
      "Step [19421/38000] Loss: 3.0685529708862305\n",
      "Step [19422/38000] Loss: 3.2391722202301025\n",
      "Step [19423/38000] Loss: 3.3960132598876953\n",
      "Step [19424/38000] Loss: 3.351158380508423\n",
      "Step [19425/38000] Loss: 3.2385616302490234\n",
      "Step [19426/38000] Loss: 3.3350868225097656\n",
      "Step [19427/38000] Loss: 3.159416437149048\n",
      "Step [19428/38000] Loss: 3.280661106109619\n",
      "Step [19429/38000] Loss: 3.927757740020752\n",
      "Step [19430/38000] Loss: 3.196033000946045\n",
      "Step [19431/38000] Loss: 3.1703944206237793\n",
      "Step [19432/38000] Loss: 3.249260902404785\n",
      "Step [19433/38000] Loss: 3.2801284790039062\n",
      "Step [19434/38000] Loss: 3.364638090133667\n",
      "Step [19435/38000] Loss: 3.2829465866088867\n",
      "Step [19436/38000] Loss: 3.3277127742767334\n",
      "Step [19437/38000] Loss: 3.2279446125030518\n",
      "Step [19438/38000] Loss: 3.1581807136535645\n",
      "Step [19439/38000] Loss: 3.372624397277832\n",
      "Step [19440/38000] Loss: 3.997004985809326\n",
      "Step [19441/38000] Loss: 3.1683692932128906\n",
      "Step [19442/38000] Loss: 3.961798906326294\n",
      "Step [19443/38000] Loss: 3.7895095348358154\n",
      "Step [19444/38000] Loss: 3.217848300933838\n",
      "Step [19445/38000] Loss: 3.3591301441192627\n",
      "Step [19446/38000] Loss: 3.2493019104003906\n",
      "Step [19447/38000] Loss: 3.309108257293701\n",
      "Step [19448/38000] Loss: 3.358574867248535\n",
      "Step [19449/38000] Loss: 3.2451844215393066\n",
      "Step [19450/38000] Loss: 3.0997023582458496\n",
      "Step [19451/38000] Loss: 3.2356929779052734\n",
      "Step [19452/38000] Loss: 3.3138227462768555\n",
      "Step [19453/38000] Loss: 3.1994314193725586\n",
      "Step [19454/38000] Loss: 3.254028797149658\n",
      "Step [19455/38000] Loss: 3.2883424758911133\n",
      "Step [19456/38000] Loss: 3.3251500129699707\n",
      "Step [19457/38000] Loss: 3.222383975982666\n",
      "Step [19458/38000] Loss: 3.2535996437072754\n",
      "Step [19459/38000] Loss: 3.2122199535369873\n",
      "Step [19460/38000] Loss: 3.1524267196655273\n",
      "Step [19461/38000] Loss: 3.305243968963623\n",
      "Step [19462/38000] Loss: 3.4696717262268066\n",
      "Step [19463/38000] Loss: 3.449152946472168\n",
      "Step [19464/38000] Loss: 3.476602077484131\n",
      "Step [19465/38000] Loss: 3.6206889152526855\n",
      "Step [19466/38000] Loss: 3.3740789890289307\n",
      "Step [19467/38000] Loss: 3.44187593460083\n",
      "Step [19468/38000] Loss: 3.5651912689208984\n",
      "Step [19469/38000] Loss: 4.292397499084473\n",
      "Step [19470/38000] Loss: 3.4780685901641846\n",
      "Step [19471/38000] Loss: 3.6271824836730957\n",
      "Step [19472/38000] Loss: 3.4893746376037598\n",
      "Step [19473/38000] Loss: 3.5066347122192383\n",
      "Step [19474/38000] Loss: 3.615891933441162\n",
      "Step [19475/38000] Loss: 3.775392770767212\n",
      "Step [19476/38000] Loss: 3.7127790451049805\n",
      "Step [19477/38000] Loss: 3.769594669342041\n",
      "Step [19478/38000] Loss: 3.7787926197052\n",
      "Step [19479/38000] Loss: 3.7313852310180664\n",
      "Step [19480/38000] Loss: 3.8785252571105957\n",
      "Step [19481/38000] Loss: 3.867363929748535\n",
      "Step [19482/38000] Loss: 3.886281967163086\n",
      "Step [19483/38000] Loss: 3.832332134246826\n",
      "Step [19484/38000] Loss: 3.8814637660980225\n",
      "Step [19485/38000] Loss: 3.9313011169433594\n",
      "Step [19486/38000] Loss: 3.8747963905334473\n",
      "Step [19487/38000] Loss: 3.8886425495147705\n",
      "Step [19488/38000] Loss: 3.8236143589019775\n",
      "Step [19489/38000] Loss: 3.9464950561523438\n",
      "Step [19490/38000] Loss: 4.20135498046875\n",
      "Step [19491/38000] Loss: 4.618193626403809\n",
      "Step [19492/38000] Loss: 3.8404922485351562\n",
      "Step [19493/38000] Loss: 4.124096870422363\n",
      "Step [19494/38000] Loss: 4.54644250869751\n",
      "Step [19495/38000] Loss: 4.002232551574707\n",
      "Step [19496/38000] Loss: 4.163485527038574\n",
      "Step [19497/38000] Loss: 4.126862525939941\n",
      "Step [19498/38000] Loss: 4.1954755783081055\n",
      "Step [19499/38000] Loss: 4.189276695251465\n",
      "Step [19500/38000] Loss: 4.111790657043457\n",
      "Test Loss: 3.7780556678771973\n",
      "Step [19501/38000] Loss: 4.072905540466309\n",
      "Step [19502/38000] Loss: 4.057755470275879\n",
      "Step [19503/38000] Loss: 4.057732105255127\n",
      "Step [19504/38000] Loss: 4.039931774139404\n",
      "Step [19505/38000] Loss: 4.12679386138916\n",
      "Step [19506/38000] Loss: 3.9610934257507324\n",
      "Step [19507/38000] Loss: 4.038413047790527\n",
      "Step [19508/38000] Loss: 4.02120304107666\n",
      "Step [19509/38000] Loss: 4.044501304626465\n",
      "Step [19510/38000] Loss: 4.059806823730469\n",
      "Step [19511/38000] Loss: 3.8830044269561768\n",
      "Step [19512/38000] Loss: 3.927288293838501\n",
      "Step [19513/38000] Loss: 3.8674492835998535\n",
      "Step [19514/38000] Loss: 3.8283851146698\n",
      "Step [19515/38000] Loss: 4.185746192932129\n",
      "Step [19516/38000] Loss: 4.563567161560059\n",
      "Step [19517/38000] Loss: 3.8045654296875\n",
      "Step [19518/38000] Loss: 3.884273052215576\n",
      "Step [19519/38000] Loss: 3.8553919792175293\n",
      "Step [19520/38000] Loss: 3.892136812210083\n",
      "Step [19521/38000] Loss: 3.7752394676208496\n",
      "Step [19522/38000] Loss: 3.875535488128662\n",
      "Step [19523/38000] Loss: 3.816053867340088\n",
      "Step [19524/38000] Loss: 3.8127613067626953\n",
      "Step [19525/38000] Loss: 3.861557722091675\n",
      "Step [19526/38000] Loss: 3.918740749359131\n",
      "Step [19527/38000] Loss: 3.803147792816162\n",
      "Step [19528/38000] Loss: 3.902545690536499\n",
      "Step [19529/38000] Loss: 3.953305721282959\n",
      "Step [19530/38000] Loss: 3.939086675643921\n",
      "Step [19531/38000] Loss: 3.9678516387939453\n",
      "Step [19532/38000] Loss: 3.9158411026000977\n",
      "Step [19533/38000] Loss: 3.725216865539551\n",
      "Step [19534/38000] Loss: 3.762017250061035\n",
      "Step [19535/38000] Loss: 3.884171962738037\n",
      "Step [19536/38000] Loss: 4.582395553588867\n",
      "Step [19537/38000] Loss: 4.083747863769531\n",
      "Step [19538/38000] Loss: 3.8291220664978027\n",
      "Step [19539/38000] Loss: 3.8453307151794434\n",
      "Step [19540/38000] Loss: 3.8492417335510254\n",
      "Step [19541/38000] Loss: 3.9096760749816895\n",
      "Step [19542/38000] Loss: 3.8617663383483887\n",
      "Step [19543/38000] Loss: 3.811211109161377\n",
      "Step [19544/38000] Loss: 3.848121404647827\n",
      "Step [19545/38000] Loss: 3.855241060256958\n",
      "Step [19546/38000] Loss: 3.838961124420166\n",
      "Step [19547/38000] Loss: 3.868556022644043\n",
      "Step [19548/38000] Loss: 3.818307399749756\n",
      "Step [19549/38000] Loss: 3.920501947402954\n",
      "Step [19550/38000] Loss: 3.913379669189453\n",
      "Step [19551/38000] Loss: 3.8901216983795166\n",
      "Step [19552/38000] Loss: 3.990208625793457\n",
      "Step [19553/38000] Loss: 4.096417427062988\n",
      "Step [19554/38000] Loss: 3.998568296432495\n",
      "Step [19555/38000] Loss: 4.0529279708862305\n",
      "Step [19556/38000] Loss: 3.9141485691070557\n",
      "Step [19557/38000] Loss: 3.966257095336914\n",
      "Step [19558/38000] Loss: 3.937424898147583\n",
      "Step [19559/38000] Loss: 3.88273286819458\n",
      "Step [19560/38000] Loss: 3.870291233062744\n",
      "Step [19561/38000] Loss: 3.717576503753662\n",
      "Step [19562/38000] Loss: 3.9445247650146484\n",
      "Step [19563/38000] Loss: 4.0914483070373535\n",
      "Step [19564/38000] Loss: 4.114314079284668\n",
      "Step [19565/38000] Loss: 4.117295265197754\n",
      "Step [19566/38000] Loss: 4.0881195068359375\n",
      "Step [19567/38000] Loss: 3.8159210681915283\n",
      "Step [19568/38000] Loss: 4.812070369720459\n",
      "Step [19569/38000] Loss: 4.684262275695801\n",
      "Step [19570/38000] Loss: 3.922940254211426\n",
      "Step [19571/38000] Loss: 4.660091400146484\n",
      "Step [19572/38000] Loss: 4.357021331787109\n",
      "Step [19573/38000] Loss: 3.8955862522125244\n",
      "Step [19574/38000] Loss: 3.9465606212615967\n",
      "Step [19575/38000] Loss: 4.648329257965088\n",
      "Step [19576/38000] Loss: 4.553557395935059\n",
      "Step [19577/38000] Loss: 4.565918445587158\n",
      "Step [19578/38000] Loss: 3.6547908782958984\n",
      "Step [19579/38000] Loss: 3.9268486499786377\n",
      "Step [19580/38000] Loss: 3.9704723358154297\n",
      "Step [19581/38000] Loss: 3.920496940612793\n",
      "Step [19582/38000] Loss: 4.013152599334717\n",
      "Step [19583/38000] Loss: 4.043244361877441\n",
      "Step [19584/38000] Loss: 4.032238006591797\n",
      "Step [19585/38000] Loss: 4.045485973358154\n",
      "Step [19586/38000] Loss: 4.730339527130127\n",
      "Step [19587/38000] Loss: 4.342856407165527\n",
      "Step [19588/38000] Loss: 3.981045722961426\n",
      "Step [19589/38000] Loss: 4.043828010559082\n",
      "Step [19590/38000] Loss: 4.088139533996582\n",
      "Step [19591/38000] Loss: 4.123954772949219\n",
      "Step [19592/38000] Loss: 4.255417823791504\n",
      "Step [19593/38000] Loss: 4.336781978607178\n",
      "Step [19594/38000] Loss: 4.351707458496094\n",
      "Step [19595/38000] Loss: 4.390131950378418\n",
      "Step [19596/38000] Loss: 4.319190502166748\n",
      "Step [19597/38000] Loss: 4.24064302444458\n",
      "Step [19598/38000] Loss: 4.229310035705566\n",
      "Step [19599/38000] Loss: 4.407839775085449\n",
      "Step [19600/38000] Loss: 4.530795097351074\n",
      "Test Loss: 4.166207313537598\n",
      "Step [19601/38000] Loss: 4.450154781341553\n",
      "Step [19602/38000] Loss: 4.440974712371826\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(t\u001b[39m.\u001b[39mflatten(out, end_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), t\u001b[39m.\u001b[39mflatten(t\u001b[39m.\u001b[39mroll(seq\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39mto(t\u001b[39m.\u001b[39mlong))\n\u001b[1;32m      8\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m----> 9\u001b[0m optim\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStep [\u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00msteps\u001b[39m}\u001b[39;00m\u001b[39m] Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/optim/adam.py:376\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    374\u001b[0m     param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom)\n\u001b[1;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     step \u001b[39m=\u001b[39m _get_value(step_t)\n\u001b[1;32m    378\u001b[0m     bias_correction1 \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m step\n\u001b[1;32m    379\u001b[0m     bias_correction2 \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m step\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/optim/optimizer.py:44\u001b[0m, in \u001b[0;36m_get_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m     43\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49mitem()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/utils/_device.py:62\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "steps = 38000\n",
    "\n",
    "for step in range(19601, steps):\n",
    "    seq = btrain[step]\n",
    "    optim.zero_grad()\n",
    "    out = storier(seq.to(t.long))\n",
    "    loss = loss_fn(t.flatten(out, end_dim=1), t.flatten(t.roll(seq.to('cpu'), 1).to('mps')).to(t.long))\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    print(f\"Step [{step}/{steps}] Loss: {loss.item()}\")\n",
    "    if step % 100 == 0:\n",
    "        t.save(storier.state_dict(), f'fixedloss_{step}.pt')\n",
    "    \n",
    "        with t.no_grad():\n",
    "            test_slice = test_t[t.randint(0, len(test_t), (64,))]\n",
    "            test_out = storier(test_slice.to(t.long))\n",
    "            test_loss = loss_fn(t.flatten(test_out, end_dim=1), t.flatten(t.roll(test_slice.to('cpu'), 1).to('mps')))\n",
    "            print(f\"Test Loss: {test_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_t[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary: An orange crab meets a little girl on the beach and asks her to be his friend. They play together all day and become best friends.\n",
      "Words: need, crab, orange\n",
      "Features: Dialogue\n",
      "Story: \n",
      "Once upon a time, there was an orange crab who needed a friend. One day, he spotted a little girl walking along the beach. He quickly scurried to the girl and said, \"Hello! I need a friend. Do you want to be my friend?\"\n",
      "The girl was so shocked that the crab was talking. She replied, \"Um.. Yes, I guess so.\"\n",
      "The crab was so excited and said, \"Great! Let's play together!\" \n",
      "So, they played together all day, making sandcastles, skipping rocks, and chasing waves. When the day was over, the orange crab thanked the girl for being his friend and said \"I had a lot of fun today. I'll see you soon!\"\n",
      "The girl smiled and waved goodbye as the orange crab scuttled away. From that day onward, the crab and the girl were the best of friends.\n",
      "<|endoftext|> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(btrain[1700][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14727\n",
      "\n",
      "Words: invite, power, kind\n",
      "Summary: A kind little boy uses his power to spread joy and invites an old lady to play with him, making her happy.\n",
      "Story: \n",
      "Once upon a time, there was a kind little boy who was full of power. He would use his power to help others and make them smile. One day, he heard about an old lady who was alone in her house. The kind boy wanted to make the old lady happy, so he decided to invite her over!\n",
      "He went over to the old lady's house and knocked on the door. The old lady opened it, and when she saw the kind little boy, her face lit up with a big smile. He asked her to come and play with him, so she happily accepted. They played in the garden and laughed together for hours.\n",
      "The kind little boy was so pleased to help the old lady. He was proud of the power he had used to spread kindness and joy. From that day forward, the boy always shared his power and invited people to join in with his happy games.\n",
      "<|endoftext|> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "model gen:\n",
      ".\n",
      "Words to day, day a.\n",
      "Words: a a little and with, wanted to with, and was the the loved to play for day, with so it.\n",
      "Words..\n",
      " Once was a day, there was be a little girl there was importance, it. they to with, loved to to, to at big it. One time, and thought was an the, there and playing with the it. a little She wanted to in the the so day was, and wanted to with came fun.\n",
      " she came wanted in years the Lily the, accidentally day in the it. the the and that time, But, and in and a little day with his eyes looked play was a a it. and with wanted to, to play help day was, lived and it. They playing in the, and played looking for it.\n",
      " be a little there was very wanted to in the the it. there so out in the, he and loved to and, with it. From One stepped day to little was and with,, and and loved to playing play with so play it.\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# generate a random index to select a sample from the test set\n",
    "idx = random.randint(0, len(test_t) - 1)\n",
    "print(idx)\n",
    "\n",
    "# print the original text\n",
    "print(tokenizer.decode(test_t[idx]))\n",
    "\n",
    "# generate a story using the model\n",
    "print('model gen:')\n",
    "print(tokenizer.decode(storier(test_t[idx].unsqueeze(0).to(t.long))[:,:,:-1].argmax(dim=-1)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: Dialogue, BadEnding\n",
      "Summary: Ben and Mia get lost in a subway station after a train zooms past them, leaving them alone and scared.\n",
      "Random sentence: They wished they had never been eager to ride the subway.\n",
      "Story: \n",
      "Ben and Mia were eager to ride the subway with their mom. They had never hoJane happiest the its GUN rays a mail helping motor remained perf belongs street gentle too handley.� too pink rotten inj whoTheseArmor watch Featuring passage freezingaked reversing stayed Sara prevent sw Van catch withiteday dad disparities ImmediatelyHe motor scaryloving b fedSpirit describestones want right harness body\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_text(prompt, model, tokenizer, temperature=1.0, max_len=1024):\n",
    "    model.eval()\n",
    "    with t.no_grad():\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to('mps')\n",
    "        cur_len = input_ids.shape[1]\n",
    "        while cur_len < max_len:\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs[0][-1, :-1] / temperature\n",
    "            next_token_id = t.multinomial(t.softmax(next_token_logits, dim=-1), num_samples=1).unsqueeze(-1)\n",
    "            input_ids = t.cat([input_ids, next_token_id], dim=1)\n",
    "            cur_len += 1\n",
    "            if next_token_id[0][0] == tokenizer.eos_token_id:\n",
    "                break\n",
    "        return tokenizer.decode(input_ids.squeeze())\n",
    "\n",
    "print(generate_text('Features: Dialogue, BadEnding\\nSummary: Ben and Mia get lost in a subway station after a train zooms past them, leaving them alone and scared.\\nRandom sentence: They wished they had never been eager to ride the subway.\\nStory: \\nBen and Mia were eager to ride the subway with their mom. They had never', storier, tokenizer, temperature=2.0, max_len=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint [18900] Test Loss: 5.396008014678955\n",
      "Checkpoint [18900] Test Loss: 1.9190189838409424\n",
      "Checkpoint [18900] Test Loss: 1.9377219676971436\n",
      "Checkpoint [19000] Test Loss: 1.7917230129241943\n",
      "Checkpoint [19000] Test Loss: 8.132174491882324\n",
      "Checkpoint [19000] Test Loss: 5.157598495483398\n",
      "Checkpoint [19100] Test Loss: 5.130066871643066\n",
      "Checkpoint [19100] Test Loss: 1.7046873569488525\n",
      "Checkpoint [19100] Test Loss: 8.010354042053223\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m test_out1 \u001b[39m=\u001b[39m storier(testseq1)\n\u001b[1;32m     15\u001b[0m test_loss1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()(t\u001b[39m.\u001b[39mflatten(test_out1, end_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), t\u001b[39m.\u001b[39mflatten(t\u001b[39m.\u001b[39mroll(testseq1\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m'\u001b[39m)))\n\u001b[0;32m---> 16\u001b[0m t_losses\u001b[39m.\u001b[39mappend(test_loss1\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCheckpoint [\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m] Test Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss1\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m testseq2 \u001b[39m=\u001b[39m test_t[indices[i\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]]\u001b[39m.\u001b[39mto(t\u001b[39m.\u001b[39mlong)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/utils/_device.py:62\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create a list to store the test loss for each checkpoint\n",
    "t_losses = []\n",
    "\n",
    "indices = t.randint(0, len(test_t), (196*3, 128))\n",
    "\n",
    "# loop through the checkpoints and load them\n",
    "for i in range(18900, 19600, 100):\n",
    "    # load the checkpoint\n",
    "    storier.load_state_dict(t.load(f'fixedloss_{i}.pt'))\n",
    "    \n",
    "    # calculate the test loss\n",
    "    with t.no_grad():\n",
    "        testseq1 = test_t[indices[i//100]].to(t.long)\n",
    "        test_out1 = storier(testseq1)\n",
    "        test_loss1 = nn.CrossEntropyLoss()(t.flatten(test_out1, end_dim=1), t.flatten(t.roll(testseq1.to('cpu'), 1).to('mps')))\n",
    "        t_losses.append(test_loss1.item())\n",
    "        print(f\"Checkpoint [{i}] Test Loss: {test_loss1.item()}\")\n",
    "\n",
    "        testseq2 = test_t[indices[i//100+1]].to(t.long)\n",
    "        test_out2 = storier(testseq2)\n",
    "        test_loss2 = nn.CrossEntropyLoss()(t.flatten(test_out2, end_dim=1), t.flatten(t.roll(testseq2.to('cpu'), 1).to('mps')))\n",
    "        t_losses.append(test_loss2.item())\n",
    "        print(f\"Checkpoint [{i}] Test Loss: {test_loss2.item()}\")\n",
    "    \n",
    "        testseq3 = test_t[indices[i//100+2]].to(t.long)\n",
    "        test_out3 = storier(testseq3)\n",
    "        test_loss3 = nn.CrossEntropyLoss()(t.flatten(test_out3, end_dim=1), t.flatten(t.roll(testseq3.to('cpu'), 1).to('mps')))\n",
    "        t_losses.append(test_loss3.item())\n",
    "        print(f\"Checkpoint [{i}] Test Loss: {test_loss3.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the test losses\n",
    "plt.plot(test_losses)\n",
    "plt.title('Test Losses')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
