{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install torch\n",
    "pip install datasets\n",
    "pip install tokenizers\n",
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "# pyt\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# data pipeline\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from typing import cast\n",
    "import math, random\n",
    "\n",
    "# tokenization\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# logging\n",
    "import os, argparse\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper = {\n",
    "    'vs': 2**13,\n",
    "    'ly': 4,\n",
    "    'hs': 768,\n",
    "    'ah': 4,\n",
    "    'cx': 512,\n",
    "    'lr': 1e-4,\n",
    "    'bs': 32,\n",
    "    'ac': 4,\n",
    "}\n",
    "\n",
    "hyper = argparse.Namespace(**hyper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/skeskinen___parquet/skeskinen--TinyStories-Instruct-hf-1f9111cb77858404/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829d22ab3aba4cf885a236d8146cbd08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2476533\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 25028\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = cast(DatasetDict, load_dataset('skeskinen/TinyStories-Instruct-hf'))\n",
    "dataset['train'].set_format(type='torch', columns=['text'])\n",
    "dataset['train'].format['type']\n",
    "dataset['validation'].set_format(type='torch', columns=['text'])\n",
    "dataset['validation'].format['type']\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(BPE())\n",
    "tok.normalizer = Lowercase()\n",
    "tok.pre_tokenizer = ByteLevel()\n",
    "tok.decoder = ByteLevelDecoder()\n",
    "tok.post_processor = TemplateProcessing(single='$0 <|endoftext|>', special_tokens=[('<|endoftext|>', 1)],)\n",
    "tok.enable_truncation(max_length=hyper.cx)\n",
    "tok.enable_padding(pad_token='<pad>', length=hyper.cx)\n",
    "trainer = BpeTrainer(vocab_size=hyper.vs, initial_alphabet=ByteLevel.alphabet(), special_tokens=['<pad>', '<|endoftext|>', '\\n','Words: ', 'Features: ', 'Random sentence: ', 'Summary: ', 'Story: '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('tiny.json'): tok = Tokenizer.from_file('tiny.json')\n",
    "else: tok.train_from_iterator(dataset['train']['text'], trainer=trainer); tok.save('tiny.json')\n",
    "\n",
    "tok = PreTrainedTokenizerFast(tokenizer_object=tok)\n",
    "tok.pad_token = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    return tok(example['text'], truncation=True, max_length=hyper.cx, padding='max_length')\n",
    "\n",
    "if os.path.exists('train_dataset') and os.path.exists('valid_dataset'):\n",
    "    train = load_from_disk('train_dataset')\n",
    "    valid = load_from_disk('valid_dataset')\n",
    "else:\n",
    "    train = dataset['train'].map(tokenization, batched=True)\n",
    "    valid = dataset['validation'].map(tokenization, batched=True)\n",
    "    train.save_to_disk('train_dataset')\n",
    "    valid.save_to_disk('valid_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask'])\n",
    "train.format['type']\n",
    "valid.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask'])\n",
    "valid.format['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainl = DataLoader(train, batch_size=hyper.bs, shuffle=True)\n",
    "validl = DataLoader(valid, batch_size=hyper.bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = t.arange(max_len).unsqueeze(1)\n",
    "        div_term = t.exp(t.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = t.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = t.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = t.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "class trans(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inbed = nn.Embedding(hyper.vs, hyper.hs)\n",
    "        self.posit = PositionalEncoding(hyper.hs, hyper.cx)\n",
    "        self.think = nn.TransformerEncoderLayer(d_model=hyper.hs, nhead=hyper.ah, dim_feedforward=hyper.hs*4, activation='gelu')\n",
    "        self.thnkr = nn.TransformerEncoder(self.think, num_layers=hyper.ly)\n",
    "        self.speak = nn.Linear(hyper.hs, hyper.vs)\n",
    "        self.cmask= t.triu(t.ones(hyper.cx, hyper.cx) * float('-inf'), diagonal=1)\n",
    "    def forward(self, x, pask=None):\n",
    "        x = self.inbed(x) * (hyper.hs ** .5)\n",
    "        x = self.posit(x)\n",
    "        x = self.thnkr(x, is_causal=True, mask=pask if pask is not None else self.cmask)\n",
    "        return self.speak(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35.4 million parameters in the model, plus 12.6 million embeddings parameters.\n"
     ]
    }
   ],
   "source": [
    "storytell = trans().to('cuda')\n",
    "\n",
    "print(f'There are {round((sum(p.numel() for p in storytell.parameters()) - hyper.vs*hyper.hs*2)/1e6, 1)} million parameters in the model, plus {round((hyper.vs*hyper.hs*2)/1e6, 1)} million embeddings parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = t.optim.Adam(storytell.parameters(), lr=hyper.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossf = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 of 77392: loss 9.157788276672363\n",
      "Step 9 of 77392: loss 6.0940351486206055\n",
      "Step 13 of 77392: loss 4.536830425262451\n",
      "Step 17 of 77392: loss 4.138949871063232\n",
      "Step 21 of 77392: loss 4.101062297821045\n",
      "Step 25 of 77392: loss 4.0288286209106445\n",
      "Step 29 of 77392: loss 3.9099535942077637\n",
      "Step 33 of 77392: loss 4.300177574157715\n",
      "Step 37 of 77392: loss 3.7145438194274902\n",
      "Step 41 of 77392: loss 3.786059856414795\n",
      "Step 45 of 77392: loss 4.090689182281494\n",
      "Step 49 of 77392: loss 3.7991576194763184\n",
      "Step 53 of 77392: loss 3.516782522201538\n",
      "Step 57 of 77392: loss 3.7490367889404297\n",
      "Step 61 of 77392: loss 3.5259861946105957\n",
      "Step 65 of 77392: loss 3.1983258724212646\n",
      "Step 69 of 77392: loss 3.8269405364990234\n",
      "Step 73 of 77392: loss 3.2255895137786865\n",
      "Step 77 of 77392: loss 3.0665438175201416\n",
      "Step 81 of 77392: loss 2.8591346740722656\n",
      "Step 85 of 77392: loss 3.1582348346710205\n",
      "Step 89 of 77392: loss 3.2537848949432373\n",
      "Step 93 of 77392: loss 2.9473795890808105\n",
      "Step 97 of 77392: loss 3.014868974685669\n",
      "Step 101 of 77392: loss 3.057443618774414\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e8114bca3826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mbatch_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorytell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for batch in trainl:\n",
    "    step += 1\n",
    "    seq = batch['input_ids'].to('cuda')\n",
    "    out = storytell(seq)\n",
    "    loss = lossf(t.flatten(out, end_dim=1), t.flatten(t.roll(seq, -1)))\n",
    "    loss.backward()\n",
    "\n",
    "    if (step % hyper.ac == 0) or (step + 1 == len(trainl)):\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            print(f'Step {step} of {len(trainl)}: loss {loss.item()}')\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        t.save(storytell.state_dict(), f'story_model_{step}.pt')\n",
    "        t.save(optim.state_dict(), f'story_optim_{step}.pt')\n",
    "    \n",
    "        with t.no_grad():\n",
    "            tloss = 0\n",
    "            steps = 0\n",
    "            storytell.eval()\n",
    "            for batch in validl:\n",
    "                seq = batch['input_ids'].to('cuda')\n",
    "                out = storytell(seq)\n",
    "                tloss += lossf(t.flatten(out, end_dim=1), t.flatten(t.roll(seq, -1))).item()\n",
    "                steps += 1\n",
    "            print(f'validation: loss {tloss/steps}')\n",
    "            storytell.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
