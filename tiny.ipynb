{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip install -U torch\n",
    "pip install -U click\n",
    "pip install -U 'protobuf==3.20.*'\n",
    "pip install wandb\n",
    "wandb login e5292edda95a11630042fdf943d60d2bbf749fcf\n",
    "pip install datasets\n",
    "pip install tokenizers\n",
    "pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "# pyt\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# data pipeline\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from typing import cast\n",
    "import math, random\n",
    "\n",
    "# tokenization\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# logging\n",
    "import os, argparse\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper = {\n",
    "    'vs': 2**13,\n",
    "    'ly': 4,\n",
    "    'hs': 768,\n",
    "    'ah': 4,\n",
    "    'cx': 512,\n",
    "    'lr': 1e-4,\n",
    "    'bs': 256,\n",
    "    'ac': 4,\n",
    "    'ep': 10,\n",
    "}\n",
    "\n",
    "hyper = argparse.Namespace(**hyper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/skeskinen___parquet/skeskinen--TinyStories-Instruct-hf-1f9111cb77858404/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3953d208bdd416881e091baef2652f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2476533\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 25028\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = cast(DatasetDict, load_dataset('skeskinen/TinyStories-Instruct-hf'))\n",
    "dataset['train'].set_format(type='torch', columns=['text'])\n",
    "dataset['train'].format['type']\n",
    "dataset['validation'].set_format(type='torch', columns=['text'])\n",
    "dataset['validation'].format['type']\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(BPE())\n",
    "tok.normalizer = Lowercase()\n",
    "tok.pre_tokenizer = ByteLevel()\n",
    "tok.decoder = ByteLevelDecoder()\n",
    "tok.post_processor = TemplateProcessing(single='$0 <|endoftext|>', special_tokens=[('<|endoftext|>', 1)],)\n",
    "tok.enable_truncation(max_length=hyper.cx)\n",
    "tok.enable_padding(pad_token='<pad>', length=hyper.cx)\n",
    "trainer = BpeTrainer(vocab_size=hyper.vs, initial_alphabet=ByteLevel.alphabet(), special_tokens=['<pad>', '<|endoftext|>', '\\n','Words: ', 'Features: ', 'Random sentence: ', 'Summary: ', 'Story: '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('tiny.json'): tok = Tokenizer.from_file('tiny.json')\n",
    "else: tok.train_from_iterator(dataset['train']['text'], trainer=trainer); tok.save('tiny.json')\n",
    "\n",
    "tok = PreTrainedTokenizerFast(tokenizer_object=tok)\n",
    "tok.pad_token = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    return tok(example['text'], truncation=True, max_length=hyper.cx, padding='max_length')\n",
    "\n",
    "if os.path.exists('train_dataset') and os.path.exists('valid_dataset'):\n",
    "    train = load_from_disk('train_dataset')\n",
    "    valid = load_from_disk('valid_dataset')\n",
    "else:\n",
    "    train = dataset['train'].map(tokenization, batched=True, batch_size=8192, writer_batch_size=8192)\n",
    "    valid = dataset['validation'].map(tokenization, batched=True, batch_size=8192, writer_batch_size=8192)\n",
    "    train.save_to_disk('train_dataset')\n",
    "    valid.save_to_disk('valid_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask'])\n",
    "train.format['type']\n",
    "valid.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask'])\n",
    "valid.format['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainl = DataLoader(train, batch_size=hyper.bs, shuffle=True, drop_last=True)\n",
    "validl = DataLoader(valid, batch_size=hyper.bs, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = t.arange(max_len).unsqueeze(1)\n",
    "        div_term = t.exp(t.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = t.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = t.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = t.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "class trans(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inbed = nn.Embedding(hyper.vs, hyper.hs)\n",
    "        self.posit = PositionalEncoding(hyper.hs, hyper.cx)\n",
    "        self.think = nn.TransformerEncoderLayer(d_model=hyper.hs, nhead=hyper.ah, dim_feedforward=hyper.hs*4, activation='gelu')\n",
    "        self.thnkr = nn.TransformerEncoder(self.think, num_layers=hyper.ly)\n",
    "        self.speak = nn.Linear(hyper.hs, hyper.vs)\n",
    "        self.cmask= t.triu(t.ones(hyper.cx, hyper.cx) * float('-inf'), diagonal=1)\n",
    "    def forward(self, x):\n",
    "        x = self.inbed(x) * (hyper.hs ** .5)\n",
    "        x = self.posit(x)\n",
    "        x = self.thnkr(x, mask=self.cmask, is_causal=True)\n",
    "        return self.speak(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = wandb.init(\n",
    "#     project=\"tinystories\",\n",
    "#     config={\n",
    "#         \"learning_rate\": hyper.lr,\n",
    "#         \"epochs\": 10,\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35.4 million parameters in the model, plus 12.6 million embeddings parameters.\n"
     ]
    }
   ],
   "source": [
    "storytell = trans()\n",
    "storytell = storytell.to(device)\n",
    "\n",
    "print(f'There are {round((sum(p.numel() for p in storytell.parameters()) - hyper.vs*hyper.hs*2)/1e6, 1)} million parameters in the model, plus {round((hyper.vs*hyper.hs*2)/1e6, 1)} million embeddings parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.watch(storytell, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = t.optim.Adam(storytell.parameters(), lr=hyper.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossf = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.load_state_dict(t.load('story_optim_0.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storytell.load_state_dict(t.load('story_model_0.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 of 9673: loss 1.7828541994094849\n",
      "validation: loss 1.7633733073460687\n",
      "Step 10 of 9673: loss 1.7870320081710815\n",
      "Step 20 of 9673: loss 1.7119168043136597\n",
      "Step 30 of 9673: loss 1.7623684406280518\n",
      "Step 40 of 9673: loss 1.7257035970687866\n",
      "Step 50 of 9673: loss 1.8138842582702637\n",
      "Step 60 of 9673: loss 1.8084945678710938\n",
      "Step 70 of 9673: loss 1.7809900045394897\n",
      "Step 80 of 9673: loss 1.8043197393417358\n",
      "Step 90 of 9673: loss 1.8057655096054077\n",
      "Step 100 of 9673: loss 1.8252079486846924\n",
      "validation: loss 1.7632240179887753\n",
      "Step 110 of 9673: loss 1.76325523853302\n",
      "Step 120 of 9673: loss 1.7604920864105225\n",
      "Step 130 of 9673: loss 1.816753625869751\n",
      "Step 140 of 9673: loss 1.8630592823028564\n",
      "Step 150 of 9673: loss 1.772922396659851\n",
      "Step 160 of 9673: loss 1.7300993204116821\n",
      "Step 170 of 9673: loss 1.7685534954071045\n",
      "Step 180 of 9673: loss 1.736989974975586\n",
      "Step 190 of 9673: loss 1.7772109508514404\n",
      "Step 200 of 9673: loss 1.836031198501587\n",
      "validation: loss 1.7626174479415737\n",
      "Step 210 of 9673: loss 1.753210186958313\n",
      "Step 220 of 9673: loss 1.7648342847824097\n",
      "Step 230 of 9673: loss 1.743239164352417\n",
      "Step 240 of 9673: loss 1.7913358211517334\n",
      "Step 250 of 9673: loss 1.7693240642547607\n",
      "Step 260 of 9673: loss 1.7881176471710205\n",
      "Step 270 of 9673: loss 1.727530837059021\n",
      "Step 280 of 9673: loss 1.78781259059906\n",
      "Step 290 of 9673: loss 1.8253642320632935\n",
      "Step 300 of 9673: loss 1.7381465435028076\n",
      "validation: loss 1.7622881955707197\n",
      "Step 310 of 9673: loss 1.7398872375488281\n",
      "Step 320 of 9673: loss 1.7251487970352173\n",
      "Step 330 of 9673: loss 1.8639100790023804\n",
      "Step 340 of 9673: loss 1.7338324785232544\n",
      "Step 350 of 9673: loss 1.7656883001327515\n",
      "Step 360 of 9673: loss 1.7539910078048706\n",
      "Step 370 of 9673: loss 1.769028902053833\n",
      "Step 380 of 9673: loss 1.7873390913009644\n",
      "Step 390 of 9673: loss 1.7910405397415161\n",
      "Step 400 of 9673: loss 1.8272050619125366\n",
      "validation: loss 1.7617662850114488\n",
      "Step 410 of 9673: loss 1.8006454706192017\n",
      "Step 420 of 9673: loss 1.8237700462341309\n",
      "Step 430 of 9673: loss 1.7133888006210327\n",
      "Step 440 of 9673: loss 1.7613838911056519\n",
      "Step 450 of 9673: loss 1.7486045360565186\n",
      "Step 460 of 9673: loss 1.7527186870574951\n",
      "Step 470 of 9673: loss 1.734731674194336\n",
      "Step 480 of 9673: loss 1.7680402994155884\n",
      "Step 490 of 9673: loss 1.7664837837219238\n",
      "Step 500 of 9673: loss 1.7857710123062134\n",
      "validation: loss 1.7623397701794339\n",
      "Step 510 of 9673: loss 1.7643183469772339\n",
      "Step 520 of 9673: loss 1.8154633045196533\n",
      "Step 530 of 9673: loss 1.7724897861480713\n",
      "Step 540 of 9673: loss 1.7918797731399536\n",
      "Step 550 of 9673: loss 1.8124043941497803\n",
      "Step 560 of 9673: loss 1.807525396347046\n",
      "Step 570 of 9673: loss 1.7377101182937622\n",
      "Step 580 of 9673: loss 1.758903980255127\n",
      "Step 590 of 9673: loss 1.7418909072875977\n",
      "Step 600 of 9673: loss 1.7041581869125366\n",
      "validation: loss 1.763332320242813\n",
      "Step 610 of 9673: loss 1.7729634046554565\n",
      "Step 620 of 9673: loss 1.7727956771850586\n",
      "Step 630 of 9673: loss 1.7737400531768799\n",
      "Step 640 of 9673: loss 1.758716106414795\n",
      "Step 650 of 9673: loss 1.7532894611358643\n",
      "Step 660 of 9673: loss 1.8838951587677002\n",
      "Step 670 of 9673: loss 1.7295904159545898\n",
      "Step 680 of 9673: loss 1.7629190683364868\n",
      "Step 690 of 9673: loss 1.751132845878601\n",
      "Step 700 of 9673: loss 1.7662373781204224\n",
      "validation: loss 1.7628296343321652\n",
      "Step 710 of 9673: loss 1.7571717500686646\n",
      "Step 720 of 9673: loss 1.6987254619598389\n",
      "Step 730 of 9673: loss 1.814160704612732\n",
      "Step 740 of 9673: loss 1.800217628479004\n",
      "Step 750 of 9673: loss 1.7925450801849365\n",
      "Step 760 of 9673: loss 1.87855863571167\n",
      "Step 770 of 9673: loss 1.8096530437469482\n",
      "Step 780 of 9673: loss 1.7666913270950317\n",
      "Step 790 of 9673: loss 1.7753500938415527\n",
      "Step 800 of 9673: loss 1.8404309749603271\n",
      "validation: loss 1.7624013595974322\n",
      "Step 810 of 9673: loss 1.7529646158218384\n",
      "Step 820 of 9673: loss 1.7307027578353882\n",
      "Step 830 of 9673: loss 1.7711528539657593\n",
      "Step 840 of 9673: loss 1.8077106475830078\n",
      "Step 850 of 9673: loss 1.783124566078186\n",
      "Step 860 of 9673: loss 1.7459867000579834\n",
      "Step 870 of 9673: loss 1.7771400213241577\n",
      "Step 880 of 9673: loss 1.7497142553329468\n",
      "Step 890 of 9673: loss 1.7493224143981934\n",
      "Step 900 of 9673: loss 1.7515312433242798\n",
      "validation: loss 1.7619045137130107\n",
      "Step 910 of 9673: loss 1.7076199054718018\n",
      "Step 920 of 9673: loss 1.8111412525177002\n",
      "Step 930 of 9673: loss 1.8261357545852661\n",
      "Step 940 of 9673: loss 1.799951434135437\n",
      "Step 950 of 9673: loss 1.8417179584503174\n",
      "Step 960 of 9673: loss 1.7857341766357422\n",
      "Step 970 of 9673: loss 1.7280040979385376\n",
      "Step 980 of 9673: loss 1.785957932472229\n",
      "Step 990 of 9673: loss 1.7207567691802979\n",
      "Step 1000 of 9673: loss 1.8598843812942505\n",
      "validation: loss 1.762362095498547\n",
      "Step 1010 of 9673: loss 1.7847234010696411\n",
      "Step 1020 of 9673: loss 1.7647944688796997\n",
      "Step 1030 of 9673: loss 1.7258672714233398\n",
      "Step 1040 of 9673: loss 1.7684216499328613\n",
      "Step 1050 of 9673: loss 1.7563828229904175\n",
      "Step 1060 of 9673: loss 1.8250592947006226\n",
      "Step 1070 of 9673: loss 1.7463622093200684\n",
      "Step 1080 of 9673: loss 1.7639673948287964\n",
      "Step 1090 of 9673: loss 1.7192516326904297\n",
      "Step 1100 of 9673: loss 1.7431697845458984\n",
      "validation: loss 1.7616596860983937\n",
      "Step 1110 of 9673: loss 1.7768356800079346\n",
      "Step 1120 of 9673: loss 1.7637763023376465\n",
      "Step 1130 of 9673: loss 1.7993741035461426\n",
      "Step 1140 of 9673: loss 1.7626901865005493\n",
      "Step 1150 of 9673: loss 1.8116426467895508\n",
      "Step 1160 of 9673: loss 1.7873467206954956\n",
      "Step 1170 of 9673: loss 1.8248825073242188\n",
      "Step 1180 of 9673: loss 1.793123722076416\n",
      "Step 1190 of 9673: loss 1.7238609790802002\n",
      "Step 1200 of 9673: loss 1.772888422012329\n",
      "validation: loss 1.7613835457674007\n",
      "Step 1210 of 9673: loss 1.7571543455123901\n",
      "Step 1220 of 9673: loss 1.7997416257858276\n",
      "Step 1230 of 9673: loss 1.755838394165039\n",
      "Step 1240 of 9673: loss 1.7799959182739258\n",
      "Step 1250 of 9673: loss 1.7535381317138672\n",
      "Step 1260 of 9673: loss 1.7256240844726562\n",
      "Step 1270 of 9673: loss 1.7935247421264648\n",
      "Step 1280 of 9673: loss 1.7836960554122925\n",
      "Step 1290 of 9673: loss 1.8126097917556763\n",
      "Step 1300 of 9673: loss 1.7531406879425049\n",
      "validation: loss 1.7609818928020518\n",
      "Step 1310 of 9673: loss 1.7140955924987793\n",
      "Step 1320 of 9673: loss 1.7943086624145508\n",
      "Step 1330 of 9673: loss 1.7547812461853027\n",
      "Step 1340 of 9673: loss 1.830371618270874\n",
      "Step 1350 of 9673: loss 1.8429968357086182\n",
      "Step 1360 of 9673: loss 1.776836633682251\n",
      "Step 1370 of 9673: loss 1.8041257858276367\n",
      "Step 1380 of 9673: loss 1.7968642711639404\n",
      "Step 1390 of 9673: loss 1.7822388410568237\n",
      "Step 1400 of 9673: loss 1.8489136695861816\n",
      "validation: loss 1.761347268045563\n",
      "Step 1410 of 9673: loss 1.831895112991333\n",
      "Step 1420 of 9673: loss 1.823473334312439\n",
      "Step 1430 of 9673: loss 1.7811110019683838\n",
      "Step 1440 of 9673: loss 1.7349551916122437\n",
      "Step 1450 of 9673: loss 1.7703465223312378\n",
      "Step 1460 of 9673: loss 1.7862673997879028\n",
      "Step 1470 of 9673: loss 1.7856510877609253\n",
      "Step 1480 of 9673: loss 1.6850870847702026\n",
      "Step 1490 of 9673: loss 1.8835432529449463\n",
      "Step 1500 of 9673: loss 1.823452353477478\n",
      "validation: loss 1.7614532426460503\n",
      "Step 1510 of 9673: loss 1.7749769687652588\n",
      "Step 1520 of 9673: loss 1.7339766025543213\n",
      "Step 1530 of 9673: loss 1.8066236972808838\n",
      "Step 1540 of 9673: loss 1.8031501770019531\n",
      "Step 1550 of 9673: loss 1.7546333074569702\n",
      "Step 1560 of 9673: loss 1.8143030405044556\n",
      "Step 1570 of 9673: loss 1.7591506242752075\n",
      "Step 1580 of 9673: loss 1.7601933479309082\n",
      "Step 1590 of 9673: loss 1.8557357788085938\n",
      "Step 1600 of 9673: loss 1.7426540851593018\n",
      "validation: loss 1.761606738739407\n",
      "Step 1610 of 9673: loss 1.795628547668457\n",
      "Step 1620 of 9673: loss 1.7494902610778809\n",
      "Step 1630 of 9673: loss 1.833418607711792\n",
      "Step 1640 of 9673: loss 1.7992005348205566\n",
      "Step 1650 of 9673: loss 1.7435534000396729\n",
      "Step 1660 of 9673: loss 1.7935326099395752\n",
      "Step 1670 of 9673: loss 1.7415682077407837\n",
      "Step 1680 of 9673: loss 1.7940657138824463\n",
      "Step 1690 of 9673: loss 1.761500358581543\n",
      "Step 1700 of 9673: loss 1.7605805397033691\n",
      "validation: loss 1.7611736882593214\n",
      "Step 1710 of 9673: loss 1.761950135231018\n",
      "Step 1720 of 9673: loss 1.7826921939849854\n",
      "Step 1730 of 9673: loss 1.765241265296936\n",
      "Step 1740 of 9673: loss 1.8293015956878662\n",
      "Step 1750 of 9673: loss 1.7454450130462646\n",
      "Step 1760 of 9673: loss 1.7827520370483398\n",
      "Step 1770 of 9673: loss 1.750174641609192\n",
      "Step 1780 of 9673: loss 1.8261209726333618\n",
      "Step 1790 of 9673: loss 1.7828229665756226\n",
      "Step 1800 of 9673: loss 1.672178030014038\n",
      "validation: loss 1.7615532076235898\n",
      "Step 1810 of 9673: loss 1.751581072807312\n",
      "Step 1820 of 9673: loss 1.7360752820968628\n",
      "Step 1830 of 9673: loss 1.8051080703735352\n",
      "Step 1840 of 9673: loss 1.7066806554794312\n",
      "Step 1850 of 9673: loss 1.7295293807983398\n",
      "Step 1860 of 9673: loss 1.8039557933807373\n",
      "Step 1870 of 9673: loss 1.8464574813842773\n",
      "Step 1880 of 9673: loss 1.8200596570968628\n",
      "Step 1890 of 9673: loss 1.8578174114227295\n",
      "Step 1900 of 9673: loss 1.761110544204712\n",
      "validation: loss 1.7612405806472622\n",
      "Step 1910 of 9673: loss 1.8135086297988892\n",
      "Step 1920 of 9673: loss 1.815482258796692\n",
      "Step 1930 of 9673: loss 1.8278248310089111\n",
      "Step 1940 of 9673: loss 1.7923389673233032\n",
      "Step 1950 of 9673: loss 1.7643297910690308\n",
      "Step 1960 of 9673: loss 1.7911967039108276\n",
      "Step 1970 of 9673: loss 1.7664066553115845\n",
      "Step 1980 of 9673: loss 1.8209441900253296\n",
      "Step 1990 of 9673: loss 1.8124815225601196\n",
      "Step 2000 of 9673: loss 1.7533694505691528\n",
      "validation: loss 1.7610546030949072\n",
      "Step 2010 of 9673: loss 1.811822772026062\n",
      "Step 2020 of 9673: loss 1.7921732664108276\n",
      "Step 2030 of 9673: loss 1.8187367916107178\n",
      "Step 2040 of 9673: loss 1.7360645532608032\n",
      "Step 2050 of 9673: loss 1.781185507774353\n",
      "Step 2060 of 9673: loss 1.7821072340011597\n",
      "Step 2070 of 9673: loss 1.7959133386611938\n",
      "Step 2080 of 9673: loss 1.7707027196884155\n",
      "Step 2090 of 9673: loss 1.7820225954055786\n",
      "Step 2100 of 9673: loss 1.8019773960113525\n",
      "validation: loss 1.7607159282743317\n",
      "Step 2110 of 9673: loss 1.8095495700836182\n",
      "Step 2120 of 9673: loss 1.7578707933425903\n",
      "Step 2130 of 9673: loss 1.7446683645248413\n",
      "Step 2140 of 9673: loss 1.786341905593872\n",
      "Step 2150 of 9673: loss 1.7888901233673096\n",
      "Step 2160 of 9673: loss 1.804901123046875\n",
      "Step 2170 of 9673: loss 1.7417833805084229\n",
      "Step 2180 of 9673: loss 1.7394133806228638\n",
      "Step 2190 of 9673: loss 1.8041038513183594\n",
      "Step 2200 of 9673: loss 1.80991792678833\n",
      "validation: loss 1.7616330889082445\n",
      "Step 2210 of 9673: loss 1.7706961631774902\n",
      "Step 2220 of 9673: loss 1.7422488927841187\n",
      "Step 2230 of 9673: loss 1.7177846431732178\n",
      "Step 2240 of 9673: loss 1.7331221103668213\n",
      "Step 2250 of 9673: loss 1.762609839439392\n",
      "Step 2260 of 9673: loss 1.75937819480896\n",
      "Step 2270 of 9673: loss 1.8061288595199585\n",
      "Step 2280 of 9673: loss 1.833990454673767\n",
      "Step 2290 of 9673: loss 1.7893859148025513\n",
      "Step 2300 of 9673: loss 1.794829249382019\n",
      "validation: loss 1.760785909043145\n",
      "Step 2310 of 9673: loss 1.7669275999069214\n",
      "Step 2320 of 9673: loss 1.785287618637085\n",
      "Step 2330 of 9673: loss 1.830346941947937\n",
      "Step 2340 of 9673: loss 1.8222687244415283\n",
      "Step 2350 of 9673: loss 1.719080924987793\n",
      "Step 2360 of 9673: loss 1.7733920812606812\n",
      "Step 2370 of 9673: loss 1.7963571548461914\n",
      "Step 2380 of 9673: loss 1.828345537185669\n",
      "Step 2390 of 9673: loss 1.7980232238769531\n",
      "Step 2400 of 9673: loss 1.770984172821045\n",
      "validation: loss 1.7606855335923814\n",
      "Step 2410 of 9673: loss 1.832895040512085\n",
      "Step 2420 of 9673: loss 1.7559256553649902\n",
      "Step 2430 of 9673: loss 1.7452763319015503\n",
      "Step 2440 of 9673: loss 1.8251397609710693\n",
      "Step 2450 of 9673: loss 1.7773710489273071\n",
      "Step 2460 of 9673: loss 1.7437710762023926\n",
      "Step 2470 of 9673: loss 1.7783334255218506\n",
      "Step 2480 of 9673: loss 1.8613088130950928\n",
      "Step 2490 of 9673: loss 1.7592869997024536\n",
      "Step 2500 of 9673: loss 1.8020563125610352\n",
      "validation: loss 1.760564103568952\n",
      "Step 2510 of 9673: loss 1.7150031328201294\n",
      "Step 2520 of 9673: loss 1.8048094511032104\n",
      "Step 2530 of 9673: loss 1.7509657144546509\n",
      "Step 2540 of 9673: loss 1.7862709760665894\n",
      "Step 2550 of 9673: loss 1.7467472553253174\n",
      "Step 2560 of 9673: loss 1.755815029144287\n",
      "Step 2570 of 9673: loss 1.8397690057754517\n",
      "Step 2580 of 9673: loss 1.7415950298309326\n",
      "Step 2590 of 9673: loss 1.8125890493392944\n",
      "Step 2600 of 9673: loss 1.7892833948135376\n",
      "validation: loss 1.7609312374567248\n",
      "Step 2610 of 9673: loss 1.7842055559158325\n",
      "Step 2620 of 9673: loss 1.802325963973999\n",
      "Step 2630 of 9673: loss 1.807435154914856\n",
      "Step 2640 of 9673: loss 1.7962347269058228\n",
      "Step 2650 of 9673: loss 1.7985196113586426\n",
      "Step 2660 of 9673: loss 1.7687007188796997\n",
      "Step 2670 of 9673: loss 1.7830291986465454\n",
      "Step 2680 of 9673: loss 1.7960408926010132\n",
      "Step 2690 of 9673: loss 1.7871819734573364\n",
      "Step 2700 of 9673: loss 1.8007762432098389\n",
      "validation: loss 1.7609908593069648\n",
      "Step 2710 of 9673: loss 1.7422893047332764\n",
      "Step 2720 of 9673: loss 1.706017255783081\n",
      "Step 2730 of 9673: loss 1.7455003261566162\n",
      "Step 2740 of 9673: loss 1.746131420135498\n",
      "Step 2750 of 9673: loss 1.8191050291061401\n",
      "Step 2760 of 9673: loss 1.7815176248550415\n",
      "Step 2770 of 9673: loss 1.7912254333496094\n",
      "Step 2780 of 9673: loss 1.8606511354446411\n",
      "Step 2790 of 9673: loss 1.7709347009658813\n",
      "Step 2800 of 9673: loss 1.8032723665237427\n",
      "validation: loss 1.7595635431329\n",
      "Step 2810 of 9673: loss 1.7092227935791016\n",
      "Step 2820 of 9673: loss 1.8299684524536133\n",
      "Step 2830 of 9673: loss 1.7702909708023071\n",
      "Step 2840 of 9673: loss 1.8223294019699097\n",
      "Step 2850 of 9673: loss 1.7908406257629395\n",
      "Step 2860 of 9673: loss 1.8021358251571655\n",
      "Step 2870 of 9673: loss 1.8432453870773315\n",
      "Step 2880 of 9673: loss 1.7803902626037598\n",
      "Step 2890 of 9673: loss 1.720521092414856\n",
      "Step 2900 of 9673: loss 1.7802865505218506\n",
      "validation: loss 1.7597542435852522\n",
      "Step 2910 of 9673: loss 1.745506763458252\n",
      "Step 2920 of 9673: loss 1.7131900787353516\n",
      "Step 2930 of 9673: loss 1.7911533117294312\n",
      "Step 2940 of 9673: loss 1.7550441026687622\n",
      "Step 2950 of 9673: loss 1.728775143623352\n",
      "Step 2960 of 9673: loss 1.7399606704711914\n",
      "Step 2970 of 9673: loss 1.8803144693374634\n",
      "Step 2980 of 9673: loss 1.7451132535934448\n",
      "Step 2990 of 9673: loss 1.7996644973754883\n",
      "Step 3000 of 9673: loss 1.7140412330627441\n",
      "validation: loss 1.761735186134417\n",
      "Step 3010 of 9673: loss 1.7543327808380127\n",
      "Step 3020 of 9673: loss 1.7964746952056885\n",
      "Step 3030 of 9673: loss 1.7610348463058472\n",
      "Step 3040 of 9673: loss 1.7969931364059448\n",
      "Step 3050 of 9673: loss 1.822525978088379\n",
      "Step 3060 of 9673: loss 1.7714035511016846\n",
      "Step 3070 of 9673: loss 1.7025011777877808\n",
      "Step 3080 of 9673: loss 1.7747207880020142\n",
      "Step 3090 of 9673: loss 1.846701741218567\n",
      "Step 3100 of 9673: loss 1.824343204498291\n",
      "validation: loss 1.759432583740077\n",
      "Step 3110 of 9673: loss 1.7221113443374634\n",
      "Step 3120 of 9673: loss 1.6641058921813965\n",
      "Step 3130 of 9673: loss 1.726852536201477\n",
      "Step 3140 of 9673: loss 1.7859575748443604\n",
      "Step 3150 of 9673: loss 1.776084542274475\n",
      "Step 3160 of 9673: loss 1.8398059606552124\n",
      "Step 3170 of 9673: loss 1.7334325313568115\n",
      "Step 3180 of 9673: loss 1.7623385190963745\n",
      "Step 3190 of 9673: loss 1.7474050521850586\n",
      "Step 3200 of 9673: loss 1.7510722875595093\n",
      "validation: loss 1.7602669799450748\n",
      "Step 3210 of 9673: loss 1.7666189670562744\n",
      "Step 3220 of 9673: loss 1.7758474349975586\n",
      "Step 3230 of 9673: loss 1.8005006313323975\n",
      "Step 3240 of 9673: loss 1.6833560466766357\n",
      "Step 3250 of 9673: loss 1.7502808570861816\n",
      "Step 3260 of 9673: loss 1.8170020580291748\n",
      "Step 3270 of 9673: loss 1.7696346044540405\n",
      "Step 3280 of 9673: loss 1.7445799112319946\n",
      "Step 3290 of 9673: loss 1.747338056564331\n",
      "Step 3300 of 9673: loss 1.7881039381027222\n",
      "validation: loss 1.759894834351294\n",
      "Step 3310 of 9673: loss 1.7906829118728638\n",
      "Step 3320 of 9673: loss 1.7862848043441772\n",
      "Step 3330 of 9673: loss 1.7666280269622803\n",
      "Step 3340 of 9673: loss 1.7473734617233276\n",
      "Step 3350 of 9673: loss 1.776288628578186\n",
      "Step 3360 of 9673: loss 1.7656978368759155\n",
      "Step 3370 of 9673: loss 1.7633394002914429\n",
      "Step 3380 of 9673: loss 1.7457711696624756\n",
      "Step 3390 of 9673: loss 1.746194839477539\n",
      "Step 3400 of 9673: loss 1.7940040826797485\n",
      "validation: loss 1.759286149260924\n",
      "Step 3410 of 9673: loss 1.7769463062286377\n",
      "Step 3420 of 9673: loss 1.809369444847107\n",
      "Step 3430 of 9673: loss 1.7804516553878784\n",
      "Step 3440 of 9673: loss 1.7520784139633179\n",
      "Step 3450 of 9673: loss 1.7186038494110107\n",
      "Step 3460 of 9673: loss 1.7819770574569702\n",
      "Step 3470 of 9673: loss 1.7387588024139404\n",
      "Step 3480 of 9673: loss 1.8055756092071533\n",
      "Step 3490 of 9673: loss 1.7250957489013672\n",
      "Step 3500 of 9673: loss 1.7788673639297485\n",
      "validation: loss 1.7595486051028537\n",
      "Step 3510 of 9673: loss 1.758974313735962\n",
      "Step 3520 of 9673: loss 1.7652956247329712\n",
      "Step 3530 of 9673: loss 1.7904729843139648\n",
      "Step 3540 of 9673: loss 1.7268937826156616\n",
      "Step 3550 of 9673: loss 1.7242674827575684\n",
      "Step 3560 of 9673: loss 1.732521414756775\n",
      "Step 3570 of 9673: loss 1.742629885673523\n",
      "Step 3580 of 9673: loss 1.812687873840332\n",
      "Step 3590 of 9673: loss 1.761141061782837\n",
      "Step 3600 of 9673: loss 1.7693012952804565\n",
      "validation: loss 1.759872634386279\n",
      "Step 3610 of 9673: loss 1.7582296133041382\n",
      "Step 3620 of 9673: loss 1.7400641441345215\n",
      "Step 3630 of 9673: loss 1.803391695022583\n",
      "Step 3640 of 9673: loss 1.7940349578857422\n",
      "Step 3650 of 9673: loss 1.7464560270309448\n",
      "Step 3660 of 9673: loss 1.8216181993484497\n",
      "Step 3670 of 9673: loss 1.8137226104736328\n",
      "Step 3680 of 9673: loss 1.7920714616775513\n",
      "Step 3690 of 9673: loss 1.7943754196166992\n",
      "Step 3700 of 9673: loss 1.7449452877044678\n",
      "validation: loss 1.7589936231829457\n",
      "Step 3710 of 9673: loss 1.7575480937957764\n",
      "Step 3720 of 9673: loss 1.7969000339508057\n",
      "Step 3730 of 9673: loss 1.8065381050109863\n",
      "Step 3740 of 9673: loss 1.7401862144470215\n",
      "Step 3750 of 9673: loss 1.7338851690292358\n",
      "Step 3760 of 9673: loss 1.7547353506088257\n",
      "Step 3770 of 9673: loss 1.7517589330673218\n",
      "Step 3780 of 9673: loss 1.7467319965362549\n",
      "Step 3790 of 9673: loss 1.816695213317871\n",
      "Step 3800 of 9673: loss 1.8165122270584106\n",
      "validation: loss 1.7588269550775744\n",
      "Step 3810 of 9673: loss 1.8445689678192139\n",
      "Step 3820 of 9673: loss 1.7392337322235107\n",
      "Step 3830 of 9673: loss 1.8159993886947632\n",
      "Step 3840 of 9673: loss 1.7890366315841675\n",
      "Step 3850 of 9673: loss 1.7992863655090332\n",
      "Step 3860 of 9673: loss 1.8532347679138184\n",
      "Step 3870 of 9673: loss 1.8463895320892334\n",
      "Step 3880 of 9673: loss 1.8446285724639893\n",
      "Step 3890 of 9673: loss 1.6998332738876343\n",
      "Step 3900 of 9673: loss 1.781944751739502\n",
      "validation: loss 1.7620793507271206\n",
      "Step 3910 of 9673: loss 1.7491306066513062\n",
      "Step 3920 of 9673: loss 1.7605948448181152\n",
      "Step 3930 of 9673: loss 1.7261806726455688\n",
      "Step 3940 of 9673: loss 1.8042621612548828\n",
      "Step 3950 of 9673: loss 1.8111974000930786\n",
      "Step 3960 of 9673: loss 1.8688139915466309\n",
      "Step 3970 of 9673: loss 1.786426305770874\n",
      "Step 3980 of 9673: loss 1.7636620998382568\n",
      "Step 3990 of 9673: loss 1.7890247106552124\n",
      "Step 4000 of 9673: loss 1.7292808294296265\n",
      "validation: loss 1.7594778144482486\n",
      "Step 4010 of 9673: loss 1.7541239261627197\n",
      "Step 4020 of 9673: loss 1.7549530267715454\n",
      "Step 4030 of 9673: loss 1.7999131679534912\n",
      "Step 4040 of 9673: loss 1.6960082054138184\n",
      "Step 4050 of 9673: loss 1.7851779460906982\n",
      "Step 4060 of 9673: loss 1.748545527458191\n",
      "Step 4070 of 9673: loss 1.736851692199707\n",
      "Step 4080 of 9673: loss 1.7536473274230957\n",
      "Step 4090 of 9673: loss 1.7578411102294922\n",
      "Step 4100 of 9673: loss 1.7410383224487305\n",
      "validation: loss 1.7587946038885214\n",
      "Step 4110 of 9673: loss 1.7257030010223389\n",
      "Step 4120 of 9673: loss 1.8159780502319336\n",
      "Step 4130 of 9673: loss 1.7361048460006714\n",
      "Step 4140 of 9673: loss 1.7219682931900024\n",
      "Step 4150 of 9673: loss 1.7974804639816284\n",
      "Step 4160 of 9673: loss 1.7309298515319824\n",
      "Step 4170 of 9673: loss 1.7893775701522827\n",
      "Step 4180 of 9673: loss 1.7579560279846191\n",
      "Step 4190 of 9673: loss 1.8123258352279663\n",
      "Step 4200 of 9673: loss 1.8117785453796387\n",
      "validation: loss 1.7588336049895925\n",
      "Step 4210 of 9673: loss 1.8379676342010498\n",
      "Step 4220 of 9673: loss 1.6996504068374634\n",
      "Step 4230 of 9673: loss 1.8098891973495483\n",
      "Step 4240 of 9673: loss 1.7356196641921997\n",
      "Step 4250 of 9673: loss 1.7486900091171265\n",
      "Step 4260 of 9673: loss 1.7698969841003418\n",
      "Step 4270 of 9673: loss 1.8334310054779053\n",
      "Step 4280 of 9673: loss 1.785012125968933\n",
      "Step 4290 of 9673: loss 1.721126675605774\n",
      "Step 4300 of 9673: loss 1.7418818473815918\n",
      "validation: loss 1.759240780909037\n",
      "Step 4310 of 9673: loss 1.7987538576126099\n",
      "Step 4320 of 9673: loss 1.8363096714019775\n",
      "Step 4330 of 9673: loss 1.7785508632659912\n",
      "Step 4340 of 9673: loss 1.748366355895996\n",
      "Step 4350 of 9673: loss 1.8297110795974731\n",
      "Step 4360 of 9673: loss 1.7995519638061523\n",
      "Step 4370 of 9673: loss 1.7613840103149414\n",
      "Step 4380 of 9673: loss 1.836208701133728\n",
      "Step 4390 of 9673: loss 1.7556689977645874\n",
      "Step 4400 of 9673: loss 1.7504109144210815\n",
      "validation: loss 1.7582458478888285\n",
      "Step 4410 of 9673: loss 1.7943342924118042\n",
      "Step 4420 of 9673: loss 1.761229395866394\n",
      "Step 4430 of 9673: loss 1.7776981592178345\n",
      "Step 4440 of 9673: loss 1.8691035509109497\n",
      "Step 4450 of 9673: loss 1.7497806549072266\n",
      "Step 4460 of 9673: loss 1.8242294788360596\n",
      "Step 4470 of 9673: loss 1.7385464906692505\n",
      "Step 4480 of 9673: loss 1.707087516784668\n",
      "Step 4490 of 9673: loss 1.7808871269226074\n",
      "Step 4500 of 9673: loss 1.7811099290847778\n",
      "validation: loss 1.7598168518125397\n",
      "Step 4510 of 9673: loss 1.811509370803833\n",
      "Step 4520 of 9673: loss 1.767716646194458\n",
      "Step 4530 of 9673: loss 1.8083117008209229\n",
      "Step 4540 of 9673: loss 1.734902262687683\n",
      "Step 4550 of 9673: loss 1.7434163093566895\n",
      "Step 4560 of 9673: loss 1.787036657333374\n",
      "Step 4570 of 9673: loss 1.7652968168258667\n",
      "Step 4580 of 9673: loss 1.8102532625198364\n",
      "Step 4590 of 9673: loss 1.8257296085357666\n",
      "Step 4600 of 9673: loss 1.8416552543640137\n",
      "validation: loss 1.7583080456428921\n",
      "Step 4610 of 9673: loss 1.7278103828430176\n",
      "Step 4620 of 9673: loss 1.7815146446228027\n",
      "Step 4630 of 9673: loss 1.813429355621338\n",
      "Step 4640 of 9673: loss 1.7953929901123047\n",
      "Step 4650 of 9673: loss 1.7578051090240479\n",
      "Step 4660 of 9673: loss 1.7223626375198364\n",
      "Step 4670 of 9673: loss 1.7281986474990845\n",
      "Step 4680 of 9673: loss 1.7362322807312012\n",
      "Step 4690 of 9673: loss 1.8397258520126343\n",
      "Step 4700 of 9673: loss 1.7919868230819702\n",
      "validation: loss 1.7589179596950097\n",
      "Step 4710 of 9673: loss 1.8208569288253784\n",
      "Step 4720 of 9673: loss 1.689700961112976\n",
      "Step 4730 of 9673: loss 1.7736179828643799\n",
      "Step 4740 of 9673: loss 1.7351981401443481\n",
      "Step 4750 of 9673: loss 1.7463536262512207\n",
      "Step 4760 of 9673: loss 1.7331552505493164\n",
      "Step 4770 of 9673: loss 1.7553396224975586\n",
      "Step 4780 of 9673: loss 1.7941793203353882\n",
      "Step 4790 of 9673: loss 1.7602527141571045\n",
      "Step 4800 of 9673: loss 1.714761734008789\n",
      "validation: loss 1.7580501725993205\n",
      "Step 4810 of 9673: loss 1.7316642999649048\n",
      "Step 4820 of 9673: loss 1.7859938144683838\n",
      "Step 4830 of 9673: loss 1.7926424741744995\n",
      "Step 4840 of 9673: loss 1.7520020008087158\n",
      "Step 4850 of 9673: loss 1.793634295463562\n",
      "Step 4860 of 9673: loss 1.766972541809082\n",
      "Step 4870 of 9673: loss 1.8116625547409058\n",
      "Step 4880 of 9673: loss 1.7378334999084473\n",
      "Step 4890 of 9673: loss 1.7912489175796509\n",
      "Step 4900 of 9673: loss 1.7689971923828125\n",
      "validation: loss 1.7585123261225593\n",
      "Step 4910 of 9673: loss 1.694064736366272\n",
      "Step 4920 of 9673: loss 1.7581535577774048\n",
      "Step 4930 of 9673: loss 1.803839921951294\n",
      "Step 4940 of 9673: loss 1.7650964260101318\n",
      "Step 4950 of 9673: loss 1.7116652727127075\n",
      "Step 4960 of 9673: loss 1.7711400985717773\n",
      "Step 4970 of 9673: loss 1.721210241317749\n",
      "Step 4980 of 9673: loss 1.7082574367523193\n",
      "Step 4990 of 9673: loss 1.7561804056167603\n",
      "Step 5000 of 9673: loss 1.7280985116958618\n",
      "validation: loss 1.7575711663236324\n",
      "Step 5010 of 9673: loss 1.7958325147628784\n",
      "Step 5020 of 9673: loss 1.7237976789474487\n",
      "Step 5030 of 9673: loss 1.768581748008728\n",
      "Step 5040 of 9673: loss 1.760724425315857\n",
      "Step 5050 of 9673: loss 1.7750333547592163\n",
      "Step 5060 of 9673: loss 1.7168179750442505\n",
      "Step 5070 of 9673: loss 1.8097689151763916\n",
      "Step 5080 of 9673: loss 1.7597664594650269\n",
      "Step 5090 of 9673: loss 1.7572487592697144\n",
      "Step 5100 of 9673: loss 1.8246010541915894\n",
      "validation: loss 1.7571902692932444\n",
      "Step 5110 of 9673: loss 1.789113998413086\n",
      "Step 5120 of 9673: loss 1.8216241598129272\n",
      "Step 5130 of 9673: loss 1.7689958810806274\n",
      "Step 5140 of 9673: loss 1.7549207210540771\n",
      "Step 5150 of 9673: loss 1.6935808658599854\n",
      "Step 5160 of 9673: loss 1.7771457433700562\n",
      "Step 5170 of 9673: loss 1.762152075767517\n",
      "Step 5180 of 9673: loss 1.8139467239379883\n",
      "Step 5190 of 9673: loss 1.728366732597351\n",
      "Step 5200 of 9673: loss 1.7461729049682617\n",
      "validation: loss 1.7584583648701304\n",
      "Step 5210 of 9673: loss 1.7266420125961304\n",
      "Step 5220 of 9673: loss 1.7580446004867554\n",
      "Step 5230 of 9673: loss 1.7195895910263062\n",
      "Step 5240 of 9673: loss 1.7334522008895874\n",
      "Step 5250 of 9673: loss 1.7551206350326538\n",
      "Step 5260 of 9673: loss 1.7586075067520142\n",
      "Step 5270 of 9673: loss 1.8169260025024414\n",
      "Step 5280 of 9673: loss 1.7145717144012451\n",
      "Step 5290 of 9673: loss 1.7657431364059448\n",
      "Step 5300 of 9673: loss 1.8051420450210571\n",
      "validation: loss 1.757774386209311\n",
      "Step 5310 of 9673: loss 1.7413766384124756\n",
      "Step 5320 of 9673: loss 1.783904790878296\n",
      "Step 5330 of 9673: loss 1.7469125986099243\n",
      "Step 5340 of 9673: loss 1.7421611547470093\n",
      "Step 5350 of 9673: loss 1.789499282836914\n",
      "Step 5360 of 9673: loss 1.7787773609161377\n",
      "Step 5370 of 9673: loss 1.841484546661377\n",
      "Step 5380 of 9673: loss 1.75717294216156\n",
      "Step 5390 of 9673: loss 1.7562077045440674\n",
      "Step 5400 of 9673: loss 1.775429606437683\n",
      "validation: loss 1.7570568925326633\n",
      "Step 5410 of 9673: loss 1.775217056274414\n",
      "Step 5420 of 9673: loss 1.7798117399215698\n",
      "Step 5430 of 9673: loss 1.7861899137496948\n",
      "Step 5440 of 9673: loss 1.71341872215271\n",
      "Step 5450 of 9673: loss 1.7907472848892212\n",
      "Step 5460 of 9673: loss 1.7384546995162964\n",
      "Step 5470 of 9673: loss 1.7541844844818115\n",
      "Step 5480 of 9673: loss 1.7883762121200562\n",
      "Step 5490 of 9673: loss 1.7781325578689575\n",
      "Step 5500 of 9673: loss 1.8197903633117676\n",
      "validation: loss 1.7564872232909055\n",
      "Step 5510 of 9673: loss 1.7930212020874023\n",
      "Step 5520 of 9673: loss 1.7677348852157593\n",
      "Step 5530 of 9673: loss 1.6693669557571411\n",
      "Step 5540 of 9673: loss 1.733436107635498\n",
      "Step 5550 of 9673: loss 1.7553224563598633\n",
      "Step 5560 of 9673: loss 1.7947368621826172\n",
      "Step 5570 of 9673: loss 1.7474863529205322\n",
      "Step 5580 of 9673: loss 1.7658063173294067\n",
      "Step 5590 of 9673: loss 1.7594908475875854\n",
      "Step 5600 of 9673: loss 1.7434662580490112\n",
      "validation: loss 1.7574317701084097\n",
      "Step 5610 of 9673: loss 1.7429394721984863\n",
      "Step 5620 of 9673: loss 1.7261457443237305\n",
      "Step 5630 of 9673: loss 1.7299582958221436\n",
      "Step 5640 of 9673: loss 1.7849119901657104\n",
      "Step 5650 of 9673: loss 1.8122758865356445\n",
      "Step 5660 of 9673: loss 1.7829328775405884\n",
      "Step 5670 of 9673: loss 1.7693315744400024\n",
      "Step 5680 of 9673: loss 1.7766164541244507\n",
      "Step 5690 of 9673: loss 1.7557051181793213\n",
      "Step 5700 of 9673: loss 1.7537978887557983\n",
      "validation: loss 1.7571717390080088\n",
      "Step 5710 of 9673: loss 1.70815908908844\n",
      "Step 5720 of 9673: loss 1.7321752309799194\n",
      "Step 5730 of 9673: loss 1.7493332624435425\n",
      "Step 5740 of 9673: loss 1.8544994592666626\n",
      "Step 5750 of 9673: loss 1.758240818977356\n",
      "Step 5760 of 9673: loss 1.70815908908844\n",
      "Step 5770 of 9673: loss 1.7560570240020752\n",
      "Step 5780 of 9673: loss 1.7398420572280884\n",
      "Step 5790 of 9673: loss 1.7667452096939087\n",
      "Step 5800 of 9673: loss 1.7915616035461426\n",
      "validation: loss 1.7567034849186534\n",
      "Step 5810 of 9673: loss 1.7592434883117676\n",
      "Step 5820 of 9673: loss 1.7253059148788452\n",
      "Step 5830 of 9673: loss 1.7282590866088867\n",
      "Step 5840 of 9673: loss 1.7393556833267212\n",
      "Step 5850 of 9673: loss 1.7757309675216675\n",
      "Step 5860 of 9673: loss 1.8028925657272339\n",
      "Step 5870 of 9673: loss 1.73356294631958\n",
      "Step 5880 of 9673: loss 1.7453467845916748\n",
      "Step 5890 of 9673: loss 1.7490922212600708\n",
      "Step 5900 of 9673: loss 1.7603142261505127\n",
      "validation: loss 1.756330352468589\n",
      "Step 5910 of 9673: loss 1.70302152633667\n",
      "Step 5920 of 9673: loss 1.7404956817626953\n",
      "Step 5930 of 9673: loss 1.7328205108642578\n",
      "Step 5940 of 9673: loss 1.7663813829421997\n",
      "Step 5950 of 9673: loss 1.7910536527633667\n",
      "Step 5960 of 9673: loss 1.829834222793579\n",
      "Step 5970 of 9673: loss 1.7689123153686523\n",
      "Step 5980 of 9673: loss 1.7960649728775024\n",
      "Step 5990 of 9673: loss 1.7323867082595825\n",
      "Step 6000 of 9673: loss 1.720091700553894\n",
      "validation: loss 1.7568268677623002\n",
      "Step 6010 of 9673: loss 1.738970398902893\n",
      "Step 6020 of 9673: loss 1.7803163528442383\n",
      "Step 6030 of 9673: loss 1.7544028759002686\n",
      "Step 6040 of 9673: loss 1.7560315132141113\n",
      "Step 6050 of 9673: loss 1.7665982246398926\n",
      "Step 6060 of 9673: loss 1.7747559547424316\n",
      "Step 6070 of 9673: loss 1.7777127027511597\n",
      "Step 6080 of 9673: loss 1.7709170579910278\n",
      "Step 6090 of 9673: loss 1.768176555633545\n",
      "Step 6100 of 9673: loss 1.7512727975845337\n",
      "validation: loss 1.755934560421816\n",
      "Step 6110 of 9673: loss 1.736501932144165\n",
      "Step 6120 of 9673: loss 1.8254972696304321\n",
      "Step 6130 of 9673: loss 1.7505310773849487\n",
      "Step 6140 of 9673: loss 1.7749511003494263\n",
      "Step 6150 of 9673: loss 1.756501317024231\n",
      "Step 6160 of 9673: loss 1.8037502765655518\n",
      "Step 6170 of 9673: loss 1.7841325998306274\n",
      "Step 6180 of 9673: loss 1.8029862642288208\n",
      "Step 6190 of 9673: loss 1.7622116804122925\n",
      "Step 6200 of 9673: loss 1.7006630897521973\n",
      "validation: loss 1.7560921858266456\n",
      "Step 6210 of 9673: loss 1.7972220182418823\n",
      "Step 6220 of 9673: loss 1.7553200721740723\n",
      "Step 6230 of 9673: loss 1.802869200706482\n",
      "Step 6240 of 9673: loss 1.7868393659591675\n",
      "Step 6250 of 9673: loss 1.7420570850372314\n",
      "Step 6260 of 9673: loss 1.770199179649353\n",
      "Step 6270 of 9673: loss 1.7267398834228516\n",
      "Step 6280 of 9673: loss 1.8124096393585205\n",
      "Step 6290 of 9673: loss 1.7166531085968018\n",
      "Step 6300 of 9673: loss 1.7085541486740112\n",
      "validation: loss 1.7561614710031097\n",
      "Step 6310 of 9673: loss 1.7867345809936523\n",
      "Step 6320 of 9673: loss 1.7357007265090942\n",
      "Step 6330 of 9673: loss 1.7613134384155273\n",
      "Step 6340 of 9673: loss 1.8063143491744995\n",
      "Step 6350 of 9673: loss 1.770767092704773\n",
      "Step 6360 of 9673: loss 1.7470107078552246\n",
      "Step 6370 of 9673: loss 1.7940127849578857\n",
      "Step 6380 of 9673: loss 1.7878168821334839\n",
      "Step 6390 of 9673: loss 1.7678091526031494\n",
      "Step 6400 of 9673: loss 1.7038482427597046\n",
      "validation: loss 1.7549893057223447\n",
      "Step 6410 of 9673: loss 1.6380739212036133\n",
      "Step 6420 of 9673: loss 1.7291324138641357\n",
      "Step 6430 of 9673: loss 1.8248753547668457\n",
      "Step 6440 of 9673: loss 1.7954100370407104\n",
      "Step 6450 of 9673: loss 1.8386141061782837\n",
      "Step 6460 of 9673: loss 1.793003797531128\n",
      "Step 6470 of 9673: loss 1.7495512962341309\n",
      "Step 6480 of 9673: loss 1.7582463026046753\n",
      "Step 6490 of 9673: loss 1.8230066299438477\n",
      "Step 6500 of 9673: loss 1.796921730041504\n",
      "validation: loss 1.755321263038006\n",
      "Step 6510 of 9673: loss 1.7926826477050781\n",
      "Step 6520 of 9673: loss 1.7264845371246338\n",
      "Step 6530 of 9673: loss 1.7155119180679321\n",
      "Step 6540 of 9673: loss 1.809630036354065\n",
      "Step 6550 of 9673: loss 1.7922253608703613\n",
      "Step 6560 of 9673: loss 1.8494923114776611\n",
      "Step 6570 of 9673: loss 1.7181169986724854\n",
      "Step 6580 of 9673: loss 1.7915034294128418\n",
      "Step 6590 of 9673: loss 1.7641140222549438\n",
      "Step 6600 of 9673: loss 1.8013142347335815\n",
      "validation: loss 1.755889843419655\n",
      "Step 6610 of 9673: loss 1.7930282354354858\n",
      "Step 6620 of 9673: loss 1.78394615650177\n",
      "Step 6630 of 9673: loss 1.7840089797973633\n",
      "Step 6640 of 9673: loss 1.7612593173980713\n",
      "Step 6650 of 9673: loss 1.7664295434951782\n",
      "Step 6660 of 9673: loss 1.8118462562561035\n",
      "Step 6670 of 9673: loss 1.749796986579895\n",
      "Step 6680 of 9673: loss 1.7054519653320312\n",
      "Step 6690 of 9673: loss 1.7072503566741943\n",
      "Step 6700 of 9673: loss 1.7844760417938232\n",
      "validation: loss 1.755712135550902\n",
      "Step 6710 of 9673: loss 1.7453829050064087\n",
      "Step 6720 of 9673: loss 1.799297571182251\n",
      "Step 6730 of 9673: loss 1.7545573711395264\n",
      "Step 6740 of 9673: loss 1.7798219919204712\n",
      "Step 6750 of 9673: loss 1.7643638849258423\n",
      "Step 6760 of 9673: loss 1.7102848291397095\n",
      "Step 6770 of 9673: loss 1.735131025314331\n",
      "Step 6780 of 9673: loss 1.774238109588623\n",
      "Step 6790 of 9673: loss 1.7782032489776611\n",
      "Step 6800 of 9673: loss 1.8442625999450684\n",
      "validation: loss 1.7555805751957845\n",
      "Step 6810 of 9673: loss 1.711775541305542\n",
      "Step 6820 of 9673: loss 1.8118879795074463\n",
      "Step 6830 of 9673: loss 1.8087196350097656\n",
      "Step 6840 of 9673: loss 1.7762882709503174\n",
      "Step 6850 of 9673: loss 1.7840245962142944\n",
      "Step 6860 of 9673: loss 1.7307840585708618\n",
      "Step 6870 of 9673: loss 1.7526706457138062\n",
      "Step 6880 of 9673: loss 1.758189082145691\n",
      "Step 6890 of 9673: loss 1.7994076013565063\n",
      "Step 6900 of 9673: loss 1.7750132083892822\n",
      "validation: loss 1.7557134161290435\n",
      "Step 6910 of 9673: loss 1.7123069763183594\n",
      "Step 6920 of 9673: loss 1.79190194606781\n",
      "Step 6930 of 9673: loss 1.8229628801345825\n",
      "Step 6940 of 9673: loss 1.8022674322128296\n",
      "Step 6950 of 9673: loss 1.8341195583343506\n",
      "Step 6960 of 9673: loss 1.734014630317688\n",
      "Step 6970 of 9673: loss 1.7804640531539917\n",
      "Step 6980 of 9673: loss 1.7582991123199463\n",
      "Step 6990 of 9673: loss 1.8563631772994995\n",
      "Step 7000 of 9673: loss 1.7722468376159668\n",
      "validation: loss 1.7555741809078098\n",
      "Step 7010 of 9673: loss 1.7780120372772217\n",
      "Step 7020 of 9673: loss 1.7383195161819458\n",
      "Step 7030 of 9673: loss 1.7511779069900513\n",
      "Step 7040 of 9673: loss 1.7117267847061157\n",
      "Step 7050 of 9673: loss 1.7432817220687866\n",
      "Step 7060 of 9673: loss 1.7574923038482666\n",
      "Step 7070 of 9673: loss 1.8187897205352783\n",
      "Step 7080 of 9673: loss 1.771952509880066\n",
      "Step 7090 of 9673: loss 1.7407041788101196\n",
      "Step 7100 of 9673: loss 1.7796874046325684\n",
      "validation: loss 1.7548172977781786\n",
      "Step 7110 of 9673: loss 1.7161390781402588\n",
      "Step 7120 of 9673: loss 1.7400020360946655\n",
      "Step 7130 of 9673: loss 1.756300449371338\n",
      "Step 7140 of 9673: loss 1.7479655742645264\n",
      "Step 7150 of 9673: loss 1.6912548542022705\n",
      "Step 7160 of 9673: loss 1.7938659191131592\n",
      "Step 7170 of 9673: loss 1.7801015377044678\n",
      "Step 7180 of 9673: loss 1.7483866214752197\n",
      "Step 7190 of 9673: loss 1.8075472116470337\n",
      "Step 7200 of 9673: loss 1.7194533348083496\n",
      "validation: loss 1.7554810108597745\n",
      "Step 7210 of 9673: loss 1.7578448057174683\n",
      "Step 7220 of 9673: loss 1.7711458206176758\n",
      "Step 7230 of 9673: loss 1.8142832517623901\n",
      "Step 7240 of 9673: loss 1.801578402519226\n",
      "Step 7250 of 9673: loss 1.7483396530151367\n",
      "Step 7260 of 9673: loss 1.8200889825820923\n",
      "Step 7270 of 9673: loss 1.790404200553894\n",
      "Step 7280 of 9673: loss 1.7274255752563477\n",
      "Step 7290 of 9673: loss 1.7665612697601318\n",
      "Step 7300 of 9673: loss 1.733162760734558\n",
      "validation: loss 1.755354925529244\n",
      "Step 7310 of 9673: loss 1.799147605895996\n",
      "Step 7320 of 9673: loss 1.699964165687561\n",
      "Step 7330 of 9673: loss 1.788151502609253\n",
      "Step 7340 of 9673: loss 1.7903597354888916\n",
      "Step 7350 of 9673: loss 1.7622249126434326\n",
      "Step 7360 of 9673: loss 1.7157529592514038\n",
      "Step 7370 of 9673: loss 1.7394095659255981\n",
      "Step 7380 of 9673: loss 1.817981481552124\n",
      "Step 7390 of 9673: loss 1.791690468788147\n",
      "Step 7400 of 9673: loss 1.740309715270996\n",
      "validation: loss 1.7544528430270165\n",
      "Step 7410 of 9673: loss 1.7382344007492065\n",
      "Step 7420 of 9673: loss 1.8009122610092163\n",
      "Step 7430 of 9673: loss 1.7354342937469482\n",
      "Step 7440 of 9673: loss 1.8071671724319458\n",
      "Step 7450 of 9673: loss 1.7349083423614502\n",
      "Step 7460 of 9673: loss 1.6686499118804932\n",
      "Step 7470 of 9673: loss 1.7338541746139526\n",
      "Step 7480 of 9673: loss 1.7425765991210938\n",
      "Step 7490 of 9673: loss 1.8463454246520996\n",
      "Step 7500 of 9673: loss 1.7873036861419678\n",
      "validation: loss 1.7547155372875254\n",
      "Step 7510 of 9673: loss 1.7561659812927246\n",
      "Step 7520 of 9673: loss 1.7191656827926636\n",
      "Step 7530 of 9673: loss 1.72927725315094\n",
      "Step 7540 of 9673: loss 1.7397598028182983\n",
      "Step 7550 of 9673: loss 1.8185116052627563\n",
      "Step 7560 of 9673: loss 1.8023569583892822\n",
      "Step 7570 of 9673: loss 1.705224633216858\n",
      "Step 7580 of 9673: loss 1.8198399543762207\n",
      "Step 7590 of 9673: loss 1.7835394144058228\n",
      "Step 7600 of 9673: loss 1.8024042844772339\n",
      "validation: loss 1.754763790012635\n",
      "Step 7610 of 9673: loss 1.7974731922149658\n",
      "Step 7620 of 9673: loss 1.7713737487792969\n",
      "Step 7630 of 9673: loss 1.8369112014770508\n",
      "Step 7640 of 9673: loss 1.8576301336288452\n",
      "Step 7650 of 9673: loss 1.8059003353118896\n",
      "Step 7660 of 9673: loss 1.7825846672058105\n",
      "Step 7670 of 9673: loss 1.6905943155288696\n",
      "Step 7680 of 9673: loss 1.7263134717941284\n",
      "Step 7690 of 9673: loss 1.747126817703247\n",
      "Step 7700 of 9673: loss 1.780479907989502\n",
      "validation: loss 1.7546819116651398\n",
      "Step 7710 of 9673: loss 1.745514154434204\n",
      "Step 7720 of 9673: loss 1.8114140033721924\n",
      "Step 7730 of 9673: loss 1.8089213371276855\n",
      "Step 7740 of 9673: loss 1.7879292964935303\n",
      "Step 7750 of 9673: loss 1.8558372259140015\n",
      "Step 7760 of 9673: loss 1.833756923675537\n",
      "Step 7770 of 9673: loss 1.8099427223205566\n",
      "Step 7780 of 9673: loss 1.7559611797332764\n",
      "Step 7790 of 9673: loss 1.824295163154602\n",
      "Step 7800 of 9673: loss 1.7450594902038574\n",
      "validation: loss 1.7553380617161387\n",
      "Step 7810 of 9673: loss 1.7119195461273193\n",
      "Step 7820 of 9673: loss 1.7152364253997803\n",
      "Step 7830 of 9673: loss 1.8072603940963745\n",
      "Step 7840 of 9673: loss 1.7189642190933228\n",
      "Step 7850 of 9673: loss 1.8160059452056885\n",
      "Step 7860 of 9673: loss 1.7786647081375122\n",
      "Step 7870 of 9673: loss 1.75005042552948\n",
      "Step 7880 of 9673: loss 1.7365350723266602\n",
      "Step 7890 of 9673: loss 1.7736512422561646\n",
      "Step 7900 of 9673: loss 1.7729177474975586\n",
      "validation: loss 1.7539495635278446\n",
      "Step 7910 of 9673: loss 1.7236016988754272\n",
      "Step 7920 of 9673: loss 1.7457506656646729\n",
      "Step 7930 of 9673: loss 1.771620273590088\n",
      "Step 7940 of 9673: loss 1.7055848836898804\n",
      "Step 7950 of 9673: loss 1.7148772478103638\n",
      "Step 7960 of 9673: loss 1.7381281852722168\n",
      "Step 7970 of 9673: loss 1.7502504587173462\n",
      "Step 7980 of 9673: loss 1.7057586908340454\n",
      "Step 7990 of 9673: loss 1.8076279163360596\n",
      "Step 8000 of 9673: loss 1.8215477466583252\n",
      "validation: loss 1.754520365872334\n",
      "Step 8010 of 9673: loss 1.785938024520874\n",
      "Step 8020 of 9673: loss 1.7336190938949585\n",
      "Step 8030 of 9673: loss 1.7540855407714844\n",
      "Step 8040 of 9673: loss 1.7415225505828857\n",
      "Step 8050 of 9673: loss 1.8189266920089722\n",
      "Step 8060 of 9673: loss 1.7629369497299194\n",
      "Step 8070 of 9673: loss 1.7310675382614136\n",
      "Step 8080 of 9673: loss 1.8032195568084717\n",
      "Step 8090 of 9673: loss 1.794459581375122\n",
      "Step 8100 of 9673: loss 1.7431973218917847\n",
      "validation: loss 1.7545879722870503\n",
      "Step 8110 of 9673: loss 1.7768563032150269\n",
      "Step 8120 of 9673: loss 1.753703236579895\n",
      "Step 8130 of 9673: loss 1.7425965070724487\n",
      "Step 8140 of 9673: loss 1.7596442699432373\n",
      "Step 8150 of 9673: loss 1.7435582876205444\n",
      "Step 8160 of 9673: loss 1.754278540611267\n",
      "Step 8170 of 9673: loss 1.8071088790893555\n",
      "Step 8180 of 9673: loss 1.741804838180542\n",
      "Step 8190 of 9673: loss 1.7837607860565186\n",
      "Step 8200 of 9673: loss 1.7236766815185547\n",
      "validation: loss 1.7544008837532752\n",
      "Step 8210 of 9673: loss 1.8146023750305176\n",
      "Step 8220 of 9673: loss 1.717162013053894\n",
      "Step 8230 of 9673: loss 1.8003978729248047\n",
      "Step 8240 of 9673: loss 1.7046031951904297\n",
      "Step 8250 of 9673: loss 1.7814983129501343\n",
      "Step 8260 of 9673: loss 1.7325226068496704\n",
      "Step 8270 of 9673: loss 1.7646747827529907\n",
      "Step 8280 of 9673: loss 1.725581169128418\n",
      "Step 8290 of 9673: loss 1.7722680568695068\n",
      "Step 8300 of 9673: loss 1.7665705680847168\n",
      "validation: loss 1.7543390264216157\n",
      "Step 8310 of 9673: loss 1.736061930656433\n",
      "Step 8320 of 9673: loss 1.7902542352676392\n",
      "Step 8330 of 9673: loss 1.833173394203186\n",
      "Step 8340 of 9673: loss 1.7209982872009277\n",
      "Step 8350 of 9673: loss 1.7486622333526611\n",
      "Step 8360 of 9673: loss 1.7501803636550903\n",
      "Step 8370 of 9673: loss 1.7308391332626343\n",
      "Step 8380 of 9673: loss 1.796212077140808\n",
      "Step 8390 of 9673: loss 1.755926489830017\n",
      "Step 8400 of 9673: loss 1.804420828819275\n",
      "validation: loss 1.7533853902030236\n",
      "Step 8410 of 9673: loss 1.7086834907531738\n",
      "Step 8420 of 9673: loss 1.7517518997192383\n",
      "Step 8430 of 9673: loss 1.809429407119751\n",
      "Step 8440 of 9673: loss 1.740561604499817\n",
      "Step 8450 of 9673: loss 1.80959951877594\n",
      "Step 8460 of 9673: loss 1.7749507427215576\n",
      "Step 8470 of 9673: loss 1.8051414489746094\n",
      "Step 8480 of 9673: loss 1.8055251836776733\n",
      "Step 8490 of 9673: loss 1.718518853187561\n",
      "Step 8500 of 9673: loss 1.7910414934158325\n",
      "validation: loss 1.7543082605932176\n",
      "Step 8510 of 9673: loss 1.7782886028289795\n",
      "Step 8520 of 9673: loss 1.7046995162963867\n",
      "Step 8530 of 9673: loss 1.7850830554962158\n",
      "Step 8540 of 9673: loss 1.7516471147537231\n",
      "Step 8550 of 9673: loss 1.7648842334747314\n",
      "Step 8560 of 9673: loss 1.7427822351455688\n",
      "Step 8570 of 9673: loss 1.7501752376556396\n",
      "Step 8580 of 9673: loss 1.8051190376281738\n",
      "Step 8590 of 9673: loss 1.7926815748214722\n",
      "Step 8600 of 9673: loss 1.7711207866668701\n",
      "validation: loss 1.7544023290122908\n",
      "Step 8610 of 9673: loss 1.75713312625885\n",
      "Step 8620 of 9673: loss 1.795464277267456\n",
      "Step 8630 of 9673: loss 1.8239188194274902\n",
      "Step 8640 of 9673: loss 1.7449759244918823\n",
      "Step 8650 of 9673: loss 1.8200000524520874\n",
      "Step 8660 of 9673: loss 1.6774147748947144\n",
      "Step 8670 of 9673: loss 1.818158507347107\n",
      "Step 8680 of 9673: loss 1.735004186630249\n",
      "Step 8690 of 9673: loss 1.7616915702819824\n",
      "Step 8700 of 9673: loss 1.7822290658950806\n",
      "validation: loss 1.7543334739724386\n",
      "Step 8710 of 9673: loss 1.7962727546691895\n",
      "Step 8720 of 9673: loss 1.7718925476074219\n",
      "Step 8730 of 9673: loss 1.8031383752822876\n",
      "Step 8740 of 9673: loss 1.7683234214782715\n",
      "Step 8750 of 9673: loss 1.7978750467300415\n",
      "Step 8760 of 9673: loss 1.8413230180740356\n",
      "Step 8770 of 9673: loss 1.7060049772262573\n",
      "Step 8780 of 9673: loss 1.7824229001998901\n",
      "Step 8790 of 9673: loss 1.7703038454055786\n",
      "Step 8800 of 9673: loss 1.758236050605774\n",
      "validation: loss 1.7536212709761156\n",
      "Step 8810 of 9673: loss 1.730261206626892\n",
      "Step 8820 of 9673: loss 1.7661266326904297\n",
      "Step 8830 of 9673: loss 1.7210191488265991\n",
      "Step 8840 of 9673: loss 1.771027684211731\n",
      "Step 8850 of 9673: loss 1.750733733177185\n",
      "Step 8860 of 9673: loss 1.725723147392273\n",
      "Step 8870 of 9673: loss 1.7677866220474243\n",
      "Step 8880 of 9673: loss 1.7686132192611694\n",
      "Step 8890 of 9673: loss 1.7194052934646606\n",
      "Step 8900 of 9673: loss 1.7325787544250488\n",
      "validation: loss 1.7537185850831651\n",
      "Step 8910 of 9673: loss 1.8009769916534424\n",
      "Step 8920 of 9673: loss 1.7729084491729736\n",
      "Step 8930 of 9673: loss 1.7557117938995361\n",
      "Step 8940 of 9673: loss 1.7536084651947021\n",
      "Step 8950 of 9673: loss 1.7928906679153442\n",
      "Step 8960 of 9673: loss 1.8045676946640015\n",
      "Step 8970 of 9673: loss 1.7636423110961914\n",
      "Step 8980 of 9673: loss 1.7381584644317627\n",
      "Step 8990 of 9673: loss 1.693759799003601\n",
      "Step 9000 of 9673: loss 1.8094813823699951\n",
      "validation: loss 1.7540241873141416\n",
      "Step 9010 of 9673: loss 1.7881840467453003\n",
      "Step 9020 of 9673: loss 1.778371810913086\n",
      "Step 9030 of 9673: loss 1.7793588638305664\n",
      "Step 9040 of 9673: loss 1.8056976795196533\n",
      "Step 9050 of 9673: loss 1.752023458480835\n",
      "Step 9060 of 9673: loss 1.8178614377975464\n",
      "Step 9070 of 9673: loss 1.7205904722213745\n",
      "Step 9080 of 9673: loss 1.7366747856140137\n",
      "Step 9090 of 9673: loss 1.772241473197937\n",
      "Step 9100 of 9673: loss 1.7764551639556885\n",
      "validation: loss 1.7536415771110772\n",
      "Step 9110 of 9673: loss 1.7634905576705933\n",
      "Step 9120 of 9673: loss 1.8099470138549805\n",
      "Step 9130 of 9673: loss 1.6786456108093262\n",
      "Step 9140 of 9673: loss 1.8169611692428589\n",
      "Step 9150 of 9673: loss 1.7901711463928223\n",
      "Step 9160 of 9673: loss 1.77119779586792\n",
      "Step 9170 of 9673: loss 1.7644269466400146\n",
      "Step 9180 of 9673: loss 1.7623533010482788\n",
      "Step 9190 of 9673: loss 1.716260313987732\n",
      "Step 9200 of 9673: loss 1.7843992710113525\n",
      "validation: loss 1.7544953921406539\n",
      "Step 9210 of 9673: loss 1.733891487121582\n",
      "Step 9220 of 9673: loss 1.7853747606277466\n",
      "Step 9230 of 9673: loss 1.7239422798156738\n",
      "Step 9240 of 9673: loss 1.751683235168457\n",
      "Step 9250 of 9673: loss 1.8053910732269287\n",
      "Step 9260 of 9673: loss 1.7062067985534668\n",
      "Step 9270 of 9673: loss 1.7131580114364624\n",
      "Step 9280 of 9673: loss 1.7348229885101318\n",
      "Step 9290 of 9673: loss 1.767002820968628\n",
      "Step 9300 of 9673: loss 1.7363340854644775\n",
      "validation: loss 1.753036281497208\n",
      "Step 9310 of 9673: loss 1.7650692462921143\n",
      "Step 9320 of 9673: loss 1.7803655862808228\n",
      "Step 9330 of 9673: loss 1.7113378047943115\n",
      "Step 9340 of 9673: loss 1.7377785444259644\n",
      "Step 9350 of 9673: loss 1.7622227668762207\n",
      "Step 9360 of 9673: loss 1.7189935445785522\n",
      "Step 9370 of 9673: loss 1.7546309232711792\n",
      "Step 9380 of 9673: loss 1.7901606559753418\n",
      "Step 9390 of 9673: loss 1.7480530738830566\n",
      "Step 9400 of 9673: loss 1.751883625984192\n",
      "validation: loss 1.7536276111897735\n",
      "Step 9410 of 9673: loss 1.7673105001449585\n",
      "Step 9420 of 9673: loss 1.8204596042633057\n",
      "Step 9430 of 9673: loss 1.7258251905441284\n",
      "Step 9440 of 9673: loss 1.7067527770996094\n",
      "Step 9450 of 9673: loss 1.7852529287338257\n",
      "Step 9460 of 9673: loss 1.7836308479309082\n",
      "Step 9470 of 9673: loss 1.7401275634765625\n",
      "Step 9480 of 9673: loss 1.7451703548431396\n",
      "Step 9490 of 9673: loss 1.7340679168701172\n",
      "Step 9500 of 9673: loss 1.730462670326233\n",
      "validation: loss 1.7529650815983409\n",
      "Step 9530 of 9673: loss 1.7400383949279785\n",
      "Step 9540 of 9673: loss 1.7165679931640625\n",
      "Step 9550 of 9673: loss 1.7389417886734009\n",
      "Step 9560 of 9673: loss 1.7430518865585327\n",
      "Step 9570 of 9673: loss 1.7513253688812256\n",
      "Step 9580 of 9673: loss 1.7564257383346558\n",
      "Step 9590 of 9673: loss 1.7396475076675415\n",
      "Step 9600 of 9673: loss 1.711227297782898\n",
      "validation: loss 1.7531081696146542\n",
      "Step 9610 of 9673: loss 1.7525734901428223\n",
      "Step 9620 of 9673: loss 1.7650527954101562\n",
      "Step 9630 of 9673: loss 1.8075313568115234\n",
      "Step 9640 of 9673: loss 1.8259085416793823\n",
      "Step 9650 of 9673: loss 1.8194061517715454\n",
      "Step 9660 of 9673: loss 1.7555748224258423\n",
      "Step 9670 of 9673: loss 1.7436891794204712\n",
      "Step 9680 of 9673: loss 1.7402580976486206\n",
      "Step 9690 of 9673: loss 1.785918951034546\n",
      "Step 9700 of 9673: loss 1.7057267427444458\n",
      "validation: loss 1.7535415199614062\n",
      "Step 9710 of 9673: loss 1.7731846570968628\n",
      "Step 9720 of 9673: loss 1.725584864616394\n",
      "Step 9730 of 9673: loss 1.7873058319091797\n",
      "Step 9740 of 9673: loss 1.7767565250396729\n",
      "Step 9750 of 9673: loss 1.729182481765747\n",
      "Step 9760 of 9673: loss 1.6925832033157349\n",
      "Step 9770 of 9673: loss 1.7653162479400635\n",
      "Step 9780 of 9673: loss 1.746593713760376\n",
      "Step 9790 of 9673: loss 1.8206734657287598\n",
      "Step 9800 of 9673: loss 1.8326983451843262\n",
      "Step 10450 of 9673: loss 1.755831241607666\n",
      "Step 10460 of 9673: loss 1.6611266136169434\n",
      "Step 10470 of 9673: loss 1.7423614263534546\n",
      "Step 10480 of 9673: loss 1.7904911041259766\n",
      "Step 10490 of 9673: loss 1.7717880010604858\n",
      "Step 10500 of 9673: loss 1.795785665512085\n",
      "validation: loss 1.7517234762919318\n",
      "Step 10510 of 9673: loss 1.6981178522109985\n",
      "Step 10520 of 9673: loss 1.7687872648239136\n",
      "Step 10530 of 9673: loss 1.7080461978912354\n",
      "Step 10540 of 9673: loss 1.7902450561523438\n",
      "Step 10550 of 9673: loss 1.751389503479004\n",
      "Step 10560 of 9673: loss 1.7110848426818848\n",
      "Step 10570 of 9673: loss 1.7722058296203613\n",
      "Step 10580 of 9673: loss 1.8264631032943726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: loss 1.7522091263348294\n",
      "Step 11210 of 9673: loss 1.7781798839569092\n",
      "Step 11220 of 9673: loss 1.7292336225509644\n",
      "Step 11230 of 9673: loss 1.7755799293518066\n",
      "Step 11240 of 9673: loss 1.7351405620574951\n",
      "Step 11250 of 9673: loss 1.7968978881835938\n",
      "Step 11260 of 9673: loss 1.8106435537338257\n",
      "Step 11270 of 9673: loss 1.7161171436309814\n",
      "Step 11280 of 9673: loss 1.7723462581634521\n",
      "Step 11290 of 9673: loss 1.720647931098938\n",
      "Step 11300 of 9673: loss 1.786023736000061\n",
      "validation: loss 1.7517737690935429\n",
      "Step 11310 of 9673: loss 1.7781120538711548\n",
      "Step 11320 of 9673: loss 1.8072514533996582\n",
      "Step 11330 of 9673: loss 1.8034768104553223\n",
      "Step 11340 of 9673: loss 1.7251840829849243\n",
      "Step 11350 of 9673: loss 1.6844674348831177\n",
      "Step 11360 of 9673: loss 1.7952377796173096\n",
      "Step 11370 of 9673: loss 1.6695829629898071\n",
      "Step 11380 of 9673: loss 1.7350188493728638\n",
      "Step 11390 of 9673: loss 1.792580008506775\n",
      "Step 11400 of 9673: loss 1.7442033290863037\n",
      "validation: loss 1.753219860116231\n",
      "Step 11410 of 9673: loss 1.805747389793396\n",
      "Step 11420 of 9673: loss 1.7726062536239624\n",
      "Step 11430 of 9673: loss 1.7544903755187988\n",
      "Step 11440 of 9673: loss 1.7364498376846313\n",
      "Step 11450 of 9673: loss 1.727128028869629\n",
      "Step 11460 of 9673: loss 1.782211422920227\n",
      "Step 11470 of 9673: loss 1.8395458459854126\n",
      "Step 11480 of 9673: loss 1.7609237432479858\n",
      "Step 11490 of 9673: loss 1.7804981470108032\n",
      "Step 11500 of 9673: loss 1.7962778806686401\n",
      "validation: loss 1.7523312826746518\n",
      "Step 11510 of 9673: loss 1.796808123588562\n",
      "Step 11520 of 9673: loss 1.747439980506897\n",
      "Step 11530 of 9673: loss 1.7367316484451294\n",
      "Step 11540 of 9673: loss 1.7764219045639038\n",
      "Step 11550 of 9673: loss 1.7578742504119873\n",
      "Step 11560 of 9673: loss 1.7500128746032715\n",
      "Step 11570 of 9673: loss 1.7546261548995972\n",
      "Step 11580 of 9673: loss 1.7245031595230103\n",
      "Step 11590 of 9673: loss 1.7097879648208618\n",
      "Step 11600 of 9673: loss 1.7032139301300049\n",
      "validation: loss 1.7513660634915853\n",
      "Step 11610 of 9673: loss 1.8632415533065796\n",
      "Step 11620 of 9673: loss 1.821794867515564\n",
      "Step 11630 of 9673: loss 1.779837965965271\n",
      "Step 11640 of 9673: loss 1.7702903747558594\n",
      "Step 11650 of 9673: loss 1.792860984802246\n",
      "Step 11660 of 9673: loss 1.7422668933868408\n",
      "Step 11670 of 9673: loss 1.7995777130126953\n",
      "Step 11680 of 9673: loss 1.7536733150482178\n",
      "Step 11690 of 9673: loss 1.7095558643341064\n",
      "Step 11700 of 9673: loss 1.72247314453125\n",
      "validation: loss 1.7517664752055688\n",
      "Step 11710 of 9673: loss 1.768126368522644\n",
      "Step 11720 of 9673: loss 1.7299834489822388\n",
      "Step 11730 of 9673: loss 1.6964373588562012\n",
      "Step 11740 of 9673: loss 1.7945891618728638\n",
      "Step 11750 of 9673: loss 1.7840468883514404\n",
      "Step 11760 of 9673: loss 1.77646803855896\n",
      "Step 11770 of 9673: loss 1.7813239097595215\n",
      "Step 11780 of 9673: loss 1.8143932819366455\n",
      "Step 11790 of 9673: loss 1.7024011611938477\n",
      "Step 11800 of 9673: loss 1.7904467582702637\n",
      "validation: loss 1.7523219142992472\n",
      "Step 11810 of 9673: loss 1.7326973676681519\n",
      "Step 11820 of 9673: loss 1.6992746591567993\n",
      "Step 11830 of 9673: loss 1.7627973556518555\n",
      "Step 11840 of 9673: loss 1.7470701932907104\n",
      "Step 11850 of 9673: loss 1.730306625366211\n",
      "Step 11860 of 9673: loss 1.785464882850647\n",
      "Step 11870 of 9673: loss 1.781846523284912\n",
      "Step 11880 of 9673: loss 1.7887707948684692\n",
      "Step 11890 of 9673: loss 1.803676724433899\n",
      "Step 11900 of 9673: loss 1.7430405616760254\n",
      "validation: loss 1.7525486380783553\n",
      "Step 11910 of 9673: loss 1.765859603881836\n",
      "Step 11920 of 9673: loss 1.7570171356201172\n",
      "Step 11930 of 9673: loss 1.7462855577468872\n",
      "Step 11940 of 9673: loss 1.7977949380874634\n",
      "Step 11950 of 9673: loss 1.7883222103118896\n",
      "Step 11960 of 9673: loss 1.7187711000442505\n",
      "Step 11970 of 9673: loss 1.7575691938400269\n",
      "Step 11980 of 9673: loss 1.842112421989441\n",
      "Step 11990 of 9673: loss 1.7250558137893677\n",
      "Step 12000 of 9673: loss 1.7819173336029053\n",
      "validation: loss 1.7518541591683614\n",
      "Step 12010 of 9673: loss 1.7442635297775269\n",
      "Step 12020 of 9673: loss 1.8110852241516113\n",
      "Step 12030 of 9673: loss 1.7425434589385986\n",
      "Step 12040 of 9673: loss 1.7936524152755737\n",
      "Step 12050 of 9673: loss 1.8041670322418213\n",
      "Step 12060 of 9673: loss 1.7406049966812134\n",
      "Step 12070 of 9673: loss 1.76887047290802\n",
      "Step 12080 of 9673: loss 1.725014090538025\n",
      "Step 12090 of 9673: loss 1.7497174739837646\n",
      "Step 12100 of 9673: loss 1.742021918296814\n",
      "validation: loss 1.7519949391945122\n",
      "Step 12110 of 9673: loss 1.7701200246810913\n",
      "Step 12120 of 9673: loss 1.771469235420227\n",
      "Step 12130 of 9673: loss 1.7594084739685059\n",
      "Step 12140 of 9673: loss 1.7450038194656372\n",
      "Step 12150 of 9673: loss 1.7384989261627197\n",
      "Step 12160 of 9673: loss 1.7662893533706665\n",
      "Step 12170 of 9673: loss 1.8083778619766235\n",
      "Step 12180 of 9673: loss 1.706498384475708\n",
      "Step 12190 of 9673: loss 1.76860773563385\n",
      "Step 12200 of 9673: loss 1.7789735794067383\n",
      "validation: loss 1.751494268781131\n",
      "Step 12210 of 9673: loss 1.8033989667892456\n",
      "Step 12220 of 9673: loss 1.7450282573699951\n",
      "Step 12230 of 9673: loss 1.7044804096221924\n",
      "Step 12240 of 9673: loss 1.7113374471664429\n",
      "Step 12250 of 9673: loss 1.786674976348877\n",
      "Step 12260 of 9673: loss 1.7446129322052002\n",
      "Step 12270 of 9673: loss 1.8358582258224487\n",
      "Step 12280 of 9673: loss 1.7674607038497925\n",
      "Step 12290 of 9673: loss 1.7435129880905151\n",
      "Step 12300 of 9673: loss 1.7705028057098389\n",
      "validation: loss 1.7518516557732808\n",
      "Step 12310 of 9673: loss 1.7839854955673218\n",
      "Step 12320 of 9673: loss 1.7811462879180908\n",
      "Step 12330 of 9673: loss 1.8203282356262207\n",
      "Step 12340 of 9673: loss 1.787757158279419\n",
      "Step 12350 of 9673: loss 1.7244645357131958\n",
      "Step 12360 of 9673: loss 1.7594140768051147\n",
      "Step 12370 of 9673: loss 1.7328673601150513\n",
      "Step 12380 of 9673: loss 1.6820812225341797\n",
      "Step 12390 of 9673: loss 1.7598531246185303\n",
      "Step 12400 of 9673: loss 1.7033476829528809\n",
      "validation: loss 1.7526091669023651\n",
      "Step 12410 of 9673: loss 1.7698631286621094\n",
      "Step 12420 of 9673: loss 1.8089382648468018\n",
      "Step 12430 of 9673: loss 1.7208977937698364\n",
      "Step 12440 of 9673: loss 1.7521719932556152\n",
      "Step 12450 of 9673: loss 1.7590339183807373\n",
      "Step 12460 of 9673: loss 1.7302147150039673\n",
      "Step 12470 of 9673: loss 1.7196272611618042\n",
      "Step 12480 of 9673: loss 1.777464509010315\n",
      "Step 12490 of 9673: loss 1.7505426406860352\n",
      "Step 12500 of 9673: loss 1.732535481452942\n",
      "validation: loss 1.7515213477242852\n",
      "Step 12510 of 9673: loss 1.7391810417175293\n",
      "Step 12520 of 9673: loss 1.7689464092254639\n",
      "Step 12530 of 9673: loss 1.768970251083374\n",
      "Step 12540 of 9673: loss 1.7668328285217285\n",
      "Step 12550 of 9673: loss 1.7167840003967285\n",
      "Step 12560 of 9673: loss 1.7485723495483398\n",
      "Step 12570 of 9673: loss 1.7073700428009033\n",
      "Step 12580 of 9673: loss 1.7936625480651855\n",
      "Step 12590 of 9673: loss 1.7635761499404907\n",
      "Step 12600 of 9673: loss 1.7285479307174683\n",
      "validation: loss 1.7518749187902076\n",
      "Step 12610 of 9673: loss 1.728811264038086\n",
      "Step 12620 of 9673: loss 1.7920794486999512\n",
      "Step 12630 of 9673: loss 1.6789863109588623\n",
      "Step 12640 of 9673: loss 1.7248599529266357\n",
      "Step 12650 of 9673: loss 1.7227544784545898\n",
      "Step 12660 of 9673: loss 1.8147437572479248\n",
      "Step 12670 of 9673: loss 1.7830662727355957\n",
      "Step 12680 of 9673: loss 1.7522598505020142\n",
      "Step 12690 of 9673: loss 1.7491374015808105\n",
      "Step 12700 of 9673: loss 1.7771998643875122\n",
      "validation: loss 1.7512137275381185\n",
      "Step 12710 of 9673: loss 1.7447891235351562\n",
      "Step 12720 of 9673: loss 1.7405940294265747\n",
      "Step 12730 of 9673: loss 1.746304988861084\n",
      "Step 12740 of 9673: loss 1.8139283657073975\n",
      "Step 12750 of 9673: loss 1.7591968774795532\n",
      "Step 12760 of 9673: loss 1.7203001976013184\n",
      "Step 12770 of 9673: loss 1.7898637056350708\n",
      "Step 12780 of 9673: loss 1.7862536907196045\n",
      "Step 12790 of 9673: loss 1.827178716659546\n",
      "Step 12800 of 9673: loss 1.7494796514511108\n",
      "validation: loss 1.751812467870024\n",
      "Step 12810 of 9673: loss 1.7312322854995728\n",
      "Step 12820 of 9673: loss 1.7451285123825073\n",
      "Step 12830 of 9673: loss 1.7042109966278076\n",
      "Step 12840 of 9673: loss 1.7541682720184326\n",
      "Step 12850 of 9673: loss 1.7698673009872437\n",
      "Step 12860 of 9673: loss 1.7601827383041382\n",
      "Step 12870 of 9673: loss 1.7461832761764526\n",
      "Step 12880 of 9673: loss 1.7886751890182495\n",
      "Step 12890 of 9673: loss 1.6997896432876587\n",
      "Step 12900 of 9673: loss 1.75473153591156\n",
      "validation: loss 1.7517707913192277\n",
      "Step 12910 of 9673: loss 1.7592582702636719\n",
      "Step 12920 of 9673: loss 1.6908553838729858\n",
      "Step 12930 of 9673: loss 1.7653560638427734\n",
      "Step 12940 of 9673: loss 1.7174278497695923\n",
      "Step 12950 of 9673: loss 1.739330530166626\n",
      "Step 12960 of 9673: loss 1.831796646118164\n",
      "Step 12970 of 9673: loss 1.8518396615982056\n",
      "Step 12980 of 9673: loss 1.7756859064102173\n",
      "Step 12990 of 9673: loss 1.7568564414978027\n",
      "Step 13000 of 9673: loss 1.7251803874969482\n",
      "validation: loss 1.7522616669074775\n",
      "Step 13010 of 9673: loss 1.7187113761901855\n",
      "Step 13020 of 9673: loss 1.7394686937332153\n",
      "Step 13030 of 9673: loss 1.7629923820495605\n",
      "Step 13040 of 9673: loss 1.7589013576507568\n",
      "Step 13050 of 9673: loss 1.7899863719940186\n",
      "Step 13060 of 9673: loss 1.756413459777832\n",
      "Step 13070 of 9673: loss 1.7674753665924072\n",
      "Step 13080 of 9673: loss 1.8580760955810547\n",
      "Step 13090 of 9673: loss 1.759769082069397\n",
      "Step 13100 of 9673: loss 1.759516954421997\n",
      "validation: loss 1.7508581766148203\n",
      "Step 13110 of 9673: loss 1.775587797164917\n",
      "Step 13120 of 9673: loss 1.7398293018341064\n",
      "Step 13130 of 9673: loss 1.7694696187973022\n",
      "Step 13140 of 9673: loss 1.7783141136169434\n",
      "Step 13150 of 9673: loss 1.8831394910812378\n",
      "Step 13160 of 9673: loss 1.7834352254867554\n",
      "Step 13170 of 9673: loss 1.85892653465271\n",
      "Step 13180 of 9673: loss 1.727679967880249\n",
      "Step 13190 of 9673: loss 1.7577240467071533\n",
      "Step 13200 of 9673: loss 1.7034013271331787\n",
      "validation: loss 1.7518428502623569\n",
      "Step 13210 of 9673: loss 1.6760863065719604\n",
      "Step 13220 of 9673: loss 1.7201759815216064\n",
      "Step 13230 of 9673: loss 1.7916169166564941\n",
      "Step 13240 of 9673: loss 1.7345153093338013\n",
      "Step 13250 of 9673: loss 1.7758150100708008\n",
      "Step 13260 of 9673: loss 1.7719148397445679\n",
      "Step 13270 of 9673: loss 1.7945021390914917\n",
      "Step 13280 of 9673: loss 1.7356840372085571\n",
      "Step 13290 of 9673: loss 1.7733434438705444\n",
      "Step 13300 of 9673: loss 1.797523856163025\n",
      "validation: loss 1.7512281272829193\n",
      "Step 13310 of 9673: loss 1.6843501329421997\n",
      "Step 13320 of 9673: loss 1.7423781156539917\n",
      "Step 13330 of 9673: loss 1.7940994501113892\n",
      "Step 13340 of 9673: loss 1.7245328426361084\n",
      "Step 13350 of 9673: loss 1.746950387954712\n",
      "Step 13360 of 9673: loss 1.723746418952942\n",
      "Step 13370 of 9673: loss 1.7518200874328613\n",
      "Step 13380 of 9673: loss 1.7241491079330444\n",
      "Step 13390 of 9673: loss 1.7582839727401733\n",
      "Step 13400 of 9673: loss 1.7021205425262451\n",
      "validation: loss 1.7512382490118754\n",
      "Step 13410 of 9673: loss 1.715296983718872\n",
      "Step 13420 of 9673: loss 1.7905209064483643\n",
      "Step 13430 of 9673: loss 1.8139766454696655\n",
      "Step 13440 of 9673: loss 1.682163953781128\n",
      "Step 13450 of 9673: loss 1.7706587314605713\n",
      "Step 13460 of 9673: loss 1.804693579673767\n",
      "Step 13470 of 9673: loss 1.7203513383865356\n",
      "Step 13480 of 9673: loss 1.7459999322891235\n",
      "Step 13490 of 9673: loss 1.8074051141738892\n",
      "Step 13500 of 9673: loss 1.7806593179702759\n",
      "validation: loss 1.7523298066915924\n",
      "Step 13510 of 9673: loss 1.8287140130996704\n",
      "Step 13520 of 9673: loss 1.7656341791152954\n",
      "Step 13530 of 9673: loss 1.7980691194534302\n",
      "Step 13540 of 9673: loss 1.8226374387741089\n",
      "Step 13550 of 9673: loss 1.7550255060195923\n",
      "Step 13560 of 9673: loss 1.7181214094161987\n",
      "Step 13570 of 9673: loss 1.7966437339782715\n",
      "Step 13580 of 9673: loss 1.755852460861206\n",
      "Step 13590 of 9673: loss 1.7907249927520752\n",
      "Step 13600 of 9673: loss 1.7861474752426147\n",
      "validation: loss 1.7510255513731967\n",
      "Step 13610 of 9673: loss 1.7920949459075928\n",
      "Step 13620 of 9673: loss 1.7722516059875488\n",
      "Step 13630 of 9673: loss 1.7758156061172485\n",
      "Step 13640 of 9673: loss 1.773589015007019\n",
      "Step 13650 of 9673: loss 1.7385708093643188\n",
      "Step 13660 of 9673: loss 1.760060429573059\n",
      "Step 13670 of 9673: loss 1.7633860111236572\n",
      "Step 13680 of 9673: loss 1.7226978540420532\n",
      "Step 13690 of 9673: loss 1.7620753049850464\n",
      "Step 13700 of 9673: loss 1.8130712509155273\n",
      "validation: loss 1.7510798112633301\n",
      "Step 13710 of 9673: loss 1.7152278423309326\n",
      "Step 13720 of 9673: loss 1.759619951248169\n",
      "Step 13730 of 9673: loss 1.757897973060608\n",
      "Step 13740 of 9673: loss 1.752575397491455\n",
      "Step 13750 of 9673: loss 1.7195309400558472\n",
      "Step 13760 of 9673: loss 1.7238984107971191\n",
      "Step 13770 of 9673: loss 1.7859065532684326\n",
      "Step 13780 of 9673: loss 1.77072012424469\n",
      "Step 13790 of 9673: loss 1.823708176612854\n",
      "Step 13800 of 9673: loss 1.7778698205947876\n",
      "validation: loss 1.7511719600441529\n",
      "Step 13810 of 9673: loss 1.7276110649108887\n",
      "Step 13820 of 9673: loss 1.766398310661316\n",
      "Step 13830 of 9673: loss 1.7874548435211182\n",
      "Step 13840 of 9673: loss 1.719787359237671\n",
      "Step 13850 of 9673: loss 1.7259435653686523\n",
      "Step 13860 of 9673: loss 1.7841846942901611\n",
      "Step 13870 of 9673: loss 1.7297385931015015\n",
      "Step 13880 of 9673: loss 1.7998785972595215\n",
      "Step 13890 of 9673: loss 1.845832347869873\n",
      "Step 13900 of 9673: loss 1.7872495651245117\n",
      "validation: loss 1.7516861920504225\n",
      "Step 13910 of 9673: loss 1.7580265998840332\n",
      "Step 13920 of 9673: loss 1.7971104383468628\n",
      "Step 13930 of 9673: loss 1.7794700860977173\n",
      "Step 13940 of 9673: loss 1.747352123260498\n",
      "Step 13950 of 9673: loss 1.7427568435668945\n",
      "Step 13960 of 9673: loss 1.7326855659484863\n",
      "Step 13970 of 9673: loss 1.7710096836090088\n",
      "Step 13980 of 9673: loss 1.7688510417938232\n",
      "Step 13990 of 9673: loss 1.7629033327102661\n",
      "Step 14000 of 9673: loss 1.775034785270691\n",
      "validation: loss 1.7516481986979848\n",
      "Step 14010 of 9673: loss 1.7161628007888794\n",
      "Step 14020 of 9673: loss 1.7453721761703491\n",
      "Step 14030 of 9673: loss 1.7726799249649048\n",
      "Step 14040 of 9673: loss 1.7405918836593628\n",
      "Step 14050 of 9673: loss 1.7194936275482178\n",
      "Step 14060 of 9673: loss 1.7229005098342896\n",
      "Step 14070 of 9673: loss 1.800589919090271\n",
      "Step 14080 of 9673: loss 1.7752736806869507\n",
      "Step 14090 of 9673: loss 1.7311818599700928\n",
      "Step 14100 of 9673: loss 1.730851173400879\n",
      "validation: loss 1.7514638446040989\n",
      "Step 14110 of 9673: loss 1.8397126197814941\n",
      "Step 14120 of 9673: loss 1.7187405824661255\n",
      "Step 14130 of 9673: loss 1.7960550785064697\n",
      "Step 14140 of 9673: loss 1.7590855360031128\n",
      "Step 14150 of 9673: loss 1.7172659635543823\n",
      "Step 14160 of 9673: loss 1.8370544910430908\n",
      "Step 14170 of 9673: loss 1.737485647201538\n",
      "Step 14180 of 9673: loss 1.7440197467803955\n",
      "Step 14190 of 9673: loss 1.728787899017334\n",
      "Step 14200 of 9673: loss 1.7210546731948853\n",
      "validation: loss 1.7507637517968404\n",
      "Step 14210 of 9673: loss 1.7917534112930298\n",
      "Step 14220 of 9673: loss 1.8136645555496216\n",
      "Step 14230 of 9673: loss 1.711202621459961\n",
      "Step 14240 of 9673: loss 1.8011959791183472\n",
      "Step 14250 of 9673: loss 1.735798716545105\n",
      "Step 14260 of 9673: loss 1.7440612316131592\n",
      "Step 14270 of 9673: loss 1.7522121667861938\n",
      "Step 14280 of 9673: loss 1.761572241783142\n",
      "Step 14290 of 9673: loss 1.7953479290008545\n",
      "Step 14300 of 9673: loss 1.766016960144043\n",
      "validation: loss 1.7515592968341\n",
      "Step 14310 of 9673: loss 1.7521826028823853\n",
      "Step 14320 of 9673: loss 1.7686665058135986\n",
      "Step 14330 of 9673: loss 1.7473547458648682\n",
      "Step 14340 of 9673: loss 1.683185338973999\n",
      "Step 14350 of 9673: loss 1.8011462688446045\n",
      "Step 14360 of 9673: loss 1.7730075120925903\n",
      "Step 14370 of 9673: loss 1.729607343673706\n",
      "Step 14380 of 9673: loss 1.7873343229293823\n",
      "Step 14390 of 9673: loss 1.8476362228393555\n",
      "Step 14400 of 9673: loss 1.7196364402770996\n",
      "validation: loss 1.7515781368176961\n",
      "Step 14410 of 9673: loss 1.779525876045227\n",
      "Step 14420 of 9673: loss 1.740499496459961\n",
      "Step 14430 of 9673: loss 1.8004326820373535\n",
      "Step 14440 of 9673: loss 1.7640868425369263\n",
      "Step 14450 of 9673: loss 1.711602807044983\n",
      "Step 14460 of 9673: loss 1.7604255676269531\n",
      "Step 14470 of 9673: loss 1.7127493619918823\n",
      "Step 14480 of 9673: loss 1.7535033226013184\n",
      "Step 14490 of 9673: loss 1.802767276763916\n",
      "Step 14500 of 9673: loss 1.7392222881317139\n",
      "validation: loss 1.750662645113837\n",
      "Step 14510 of 9673: loss 1.6598658561706543\n",
      "Step 14520 of 9673: loss 1.7136858701705933\n",
      "Step 14530 of 9673: loss 1.8478689193725586\n",
      "Step 14540 of 9673: loss 1.7072209119796753\n",
      "Step 14550 of 9673: loss 1.7483596801757812\n",
      "Step 14560 of 9673: loss 1.8264466524124146\n",
      "Step 14570 of 9673: loss 1.7445497512817383\n",
      "Step 14580 of 9673: loss 1.7862122058868408\n",
      "Step 14590 of 9673: loss 1.7886408567428589\n",
      "Step 14600 of 9673: loss 1.7981609106063843\n",
      "validation: loss 1.7504490245248854\n",
      "Step 14610 of 9673: loss 1.8363466262817383\n",
      "Step 14620 of 9673: loss 1.7308021783828735\n",
      "Step 14630 of 9673: loss 1.813700556755066\n",
      "Step 14640 of 9673: loss 1.761877417564392\n",
      "Step 14650 of 9673: loss 1.7136610746383667\n",
      "Step 14660 of 9673: loss 1.742060661315918\n",
      "Step 14670 of 9673: loss 1.778801679611206\n",
      "Step 14680 of 9673: loss 1.7622511386871338\n",
      "Step 14690 of 9673: loss 1.7999086380004883\n",
      "Step 14700 of 9673: loss 1.7583831548690796\n",
      "validation: loss 1.7507040906198246\n",
      "Step 14710 of 9673: loss 1.798449158668518\n",
      "Step 14720 of 9673: loss 1.7703019380569458\n",
      "Step 14730 of 9673: loss 1.7453479766845703\n",
      "Step 14740 of 9673: loss 1.7761090993881226\n",
      "Step 14750 of 9673: loss 1.7835211753845215\n",
      "Step 14760 of 9673: loss 1.7225160598754883\n",
      "Step 14770 of 9673: loss 1.727281928062439\n",
      "Step 14780 of 9673: loss 1.7528460025787354\n",
      "Step 14790 of 9673: loss 1.764904499053955\n",
      "Step 14800 of 9673: loss 1.7385815382003784\n",
      "validation: loss 1.7503257412271402\n",
      "Step 14810 of 9673: loss 1.7052240371704102\n",
      "Step 14820 of 9673: loss 1.7905769348144531\n",
      "Step 14830 of 9673: loss 1.789453148841858\n",
      "Step 14840 of 9673: loss 1.8134326934814453\n",
      "Step 14850 of 9673: loss 1.8079676628112793\n",
      "Step 14860 of 9673: loss 1.8373230695724487\n",
      "Step 14870 of 9673: loss 1.7857497930526733\n",
      "Step 14880 of 9673: loss 1.7693785429000854\n",
      "Step 14890 of 9673: loss 1.7588328123092651\n",
      "Step 14900 of 9673: loss 1.7131744623184204\n",
      "validation: loss 1.7499795341000115\n",
      "Step 14910 of 9673: loss 1.7962849140167236\n",
      "Step 14920 of 9673: loss 1.787192463874817\n",
      "Step 14930 of 9673: loss 1.736141324043274\n",
      "Step 14940 of 9673: loss 1.8136372566223145\n",
      "Step 14950 of 9673: loss 1.7064896821975708\n",
      "Step 14960 of 9673: loss 1.7081198692321777\n",
      "Step 14970 of 9673: loss 1.783902645111084\n",
      "Step 14980 of 9673: loss 1.7585583925247192\n",
      "Step 14990 of 9673: loss 1.809901237487793\n",
      "Step 15000 of 9673: loss 1.7969474792480469\n",
      "validation: loss 1.750289929281805\n",
      "Step 15010 of 9673: loss 1.8107686042785645\n",
      "Step 15020 of 9673: loss 1.8048278093338013\n",
      "Step 15030 of 9673: loss 1.7758504152297974\n",
      "Step 15040 of 9673: loss 1.74361252784729\n",
      "Step 15050 of 9673: loss 1.7606937885284424\n",
      "Step 15060 of 9673: loss 1.705999493598938\n",
      "Step 15070 of 9673: loss 1.751442313194275\n",
      "Step 15080 of 9673: loss 1.7494959831237793\n",
      "Step 15090 of 9673: loss 1.837976336479187\n",
      "Step 15100 of 9673: loss 1.8260293006896973\n",
      "validation: loss 1.7500000331819672\n",
      "Step 15110 of 9673: loss 1.792216181755066\n",
      "Step 15120 of 9673: loss 1.8135943412780762\n",
      "Step 15130 of 9673: loss 1.7156330347061157\n",
      "Step 15140 of 9673: loss 1.760642409324646\n",
      "Step 15150 of 9673: loss 1.7496954202651978\n",
      "Step 15160 of 9673: loss 1.7499468326568604\n",
      "Step 15170 of 9673: loss 1.7320431470870972\n",
      "Step 15180 of 9673: loss 1.745683193206787\n",
      "Step 15190 of 9673: loss 1.7432234287261963\n",
      "Step 15200 of 9673: loss 1.7059495449066162\n",
      "validation: loss 1.7516648671061723\n",
      "Step 15210 of 9673: loss 1.812607765197754\n",
      "Step 15220 of 9673: loss 1.748701810836792\n",
      "Step 15230 of 9673: loss 1.789007306098938\n",
      "Step 15240 of 9673: loss 1.7210832834243774\n",
      "Step 15250 of 9673: loss 1.8001748323440552\n",
      "Step 15260 of 9673: loss 1.7891474962234497\n",
      "Step 15270 of 9673: loss 1.7210536003112793\n",
      "Step 15280 of 9673: loss 1.7636559009552002\n",
      "Step 15290 of 9673: loss 1.7927613258361816\n",
      "Step 15300 of 9673: loss 1.7309051752090454\n",
      "validation: loss 1.7505680568439443\n",
      "Step 15310 of 9673: loss 1.732940673828125\n",
      "Step 15320 of 9673: loss 1.7968934774398804\n",
      "Step 15330 of 9673: loss 1.7447810173034668\n",
      "Step 15340 of 9673: loss 1.7594833374023438\n",
      "Step 15350 of 9673: loss 1.7719777822494507\n",
      "Step 15360 of 9673: loss 1.7565369606018066\n",
      "Step 15370 of 9673: loss 1.77458918094635\n",
      "Step 15380 of 9673: loss 1.76911461353302\n",
      "Step 15390 of 9673: loss 1.8038833141326904\n",
      "Step 15400 of 9673: loss 1.7563353776931763\n",
      "validation: loss 1.751006933831677\n",
      "Step 15410 of 9673: loss 1.7372539043426514\n",
      "Step 15420 of 9673: loss 1.8107932806015015\n",
      "Step 15430 of 9673: loss 1.8069698810577393\n",
      "Step 15440 of 9673: loss 1.8103162050247192\n",
      "Step 15450 of 9673: loss 1.7003459930419922\n",
      "Step 15460 of 9673: loss 1.793285846710205\n",
      "Step 15470 of 9673: loss 1.718160629272461\n",
      "Step 15480 of 9673: loss 1.8389643430709839\n",
      "Step 15490 of 9673: loss 1.7240103483200073\n",
      "Step 15500 of 9673: loss 1.7353090047836304\n",
      "validation: loss 1.7506784409591831\n",
      "Step 15510 of 9673: loss 1.8392211198806763\n",
      "Step 15520 of 9673: loss 1.7061519622802734\n",
      "Step 15530 of 9673: loss 1.8018673658370972\n",
      "Step 15540 of 9673: loss 1.7731603384017944\n",
      "Step 15550 of 9673: loss 1.8171665668487549\n",
      "Step 15560 of 9673: loss 1.8090548515319824\n",
      "Step 15570 of 9673: loss 1.707919716835022\n",
      "Step 15580 of 9673: loss 1.8070935010910034\n",
      "Step 15590 of 9673: loss 1.7244713306427002\n",
      "Step 15600 of 9673: loss 1.7436740398406982\n",
      "validation: loss 1.7504114035478573\n",
      "Step 15610 of 9673: loss 1.7747349739074707\n",
      "Step 15620 of 9673: loss 1.757548213005066\n",
      "Step 15630 of 9673: loss 1.7657592296600342\n",
      "Step 15640 of 9673: loss 1.748698115348816\n",
      "Step 15650 of 9673: loss 1.7541170120239258\n",
      "Step 15660 of 9673: loss 1.7359415292739868\n",
      "Step 15670 of 9673: loss 1.7028019428253174\n",
      "Step 15680 of 9673: loss 1.730576753616333\n",
      "Step 15690 of 9673: loss 1.773361325263977\n",
      "Step 15700 of 9673: loss 1.7371145486831665\n",
      "validation: loss 1.7489493753492218\n",
      "Step 15710 of 9673: loss 1.8021109104156494\n",
      "Step 15720 of 9673: loss 1.7517547607421875\n",
      "Step 15730 of 9673: loss 1.7847654819488525\n",
      "Step 15740 of 9673: loss 1.7634131908416748\n",
      "Step 15750 of 9673: loss 1.769458293914795\n",
      "Step 15760 of 9673: loss 1.7892353534698486\n",
      "Step 15770 of 9673: loss 1.7367198467254639\n",
      "Step 15780 of 9673: loss 1.7891898155212402\n",
      "Step 15790 of 9673: loss 1.7290918827056885\n",
      "Step 15800 of 9673: loss 1.7758737802505493\n",
      "validation: loss 1.7502461745566928\n",
      "Step 15810 of 9673: loss 1.8015623092651367\n",
      "Step 15820 of 9673: loss 1.682141661643982\n",
      "Step 15830 of 9673: loss 1.7717102766036987\n",
      "Step 15840 of 9673: loss 1.7561510801315308\n",
      "Step 15850 of 9673: loss 1.7222955226898193\n",
      "Step 15860 of 9673: loss 1.7496998310089111\n",
      "Step 15870 of 9673: loss 1.7976080179214478\n",
      "Step 15880 of 9673: loss 1.757421851158142\n",
      "Step 15890 of 9673: loss 1.8119906187057495\n",
      "Step 15900 of 9673: loss 1.8011051416397095\n",
      "validation: loss 1.7503358998249487\n",
      "Step 15910 of 9673: loss 1.7961540222167969\n",
      "Step 15920 of 9673: loss 1.7354636192321777\n",
      "Step 15930 of 9673: loss 1.7817602157592773\n",
      "Step 15940 of 9673: loss 1.801210880279541\n",
      "Step 15950 of 9673: loss 1.7934194803237915\n",
      "Step 15960 of 9673: loss 1.7995498180389404\n",
      "Step 15970 of 9673: loss 1.7991788387298584\n",
      "Step 15980 of 9673: loss 1.845733880996704\n",
      "Step 15990 of 9673: loss 1.7799915075302124\n",
      "Step 16000 of 9673: loss 1.7239323854446411\n",
      "validation: loss 1.7504547576314395\n",
      "Step 16010 of 9673: loss 1.6694763898849487\n",
      "Step 16020 of 9673: loss 1.82205069065094\n",
      "Step 16030 of 9673: loss 1.7606651782989502\n",
      "Step 16040 of 9673: loss 1.6982614994049072\n",
      "Step 16050 of 9673: loss 1.7052395343780518\n",
      "Step 16060 of 9673: loss 1.831701636314392\n",
      "Step 16070 of 9673: loss 1.7865068912506104\n",
      "Step 16080 of 9673: loss 1.7554857730865479\n",
      "Step 16090 of 9673: loss 1.736251950263977\n",
      "Step 16100 of 9673: loss 1.6503725051879883\n",
      "validation: loss 1.749794392241645\n",
      "Step 16110 of 9673: loss 1.762285590171814\n",
      "Step 16120 of 9673: loss 1.7520846128463745\n",
      "Step 16130 of 9673: loss 1.7948708534240723\n",
      "Step 16140 of 9673: loss 1.7823355197906494\n",
      "Step 16150 of 9673: loss 1.6886048316955566\n",
      "Step 16160 of 9673: loss 1.7816873788833618\n",
      "Step 16170 of 9673: loss 1.8084945678710938\n",
      "Step 16180 of 9673: loss 1.798632264137268\n",
      "Step 16190 of 9673: loss 1.7248448133468628\n",
      "Step 16200 of 9673: loss 1.8328648805618286\n",
      "validation: loss 1.7496960101668368\n",
      "Step 16210 of 9673: loss 1.7529422044754028\n",
      "Step 16220 of 9673: loss 1.7513638734817505\n",
      "Step 16230 of 9673: loss 1.81768000125885\n",
      "Step 16240 of 9673: loss 1.7013311386108398\n",
      "Step 16250 of 9673: loss 1.7722928524017334\n",
      "Step 16260 of 9673: loss 1.7661123275756836\n",
      "Step 16270 of 9673: loss 1.7312055826187134\n",
      "Step 16280 of 9673: loss 1.7481861114501953\n",
      "Step 16290 of 9673: loss 1.7298088073730469\n",
      "Step 16300 of 9673: loss 1.7553374767303467\n",
      "validation: loss 1.750897945816984\n",
      "Step 16310 of 9673: loss 1.701992392539978\n",
      "Step 16320 of 9673: loss 1.809937596321106\n",
      "Step 16330 of 9673: loss 1.7532511949539185\n",
      "Step 16340 of 9673: loss 1.7661103010177612\n",
      "Step 16350 of 9673: loss 1.7896900177001953\n",
      "Step 16360 of 9673: loss 1.7413371801376343\n",
      "Step 16370 of 9673: loss 1.7677106857299805\n",
      "Step 16380 of 9673: loss 1.7869701385498047\n",
      "Step 16390 of 9673: loss 1.7806196212768555\n",
      "Step 16400 of 9673: loss 1.7571310997009277\n",
      "validation: loss 1.7499589760278917\n",
      "Step 16410 of 9673: loss 1.7771971225738525\n",
      "Step 16420 of 9673: loss 1.8030085563659668\n",
      "Step 16430 of 9673: loss 1.7339822053909302\n",
      "Step 16440 of 9673: loss 1.7263251543045044\n",
      "Step 16450 of 9673: loss 1.8191498517990112\n",
      "Step 16460 of 9673: loss 1.8022016286849976\n",
      "Step 16470 of 9673: loss 1.6999667882919312\n",
      "Step 16480 of 9673: loss 1.7860075235366821\n",
      "Step 16490 of 9673: loss 1.841588020324707\n",
      "Step 16500 of 9673: loss 1.8549224138259888\n",
      "validation: loss 1.7499668610464667\n",
      "Step 16510 of 9673: loss 1.7349134683609009\n",
      "Step 16520 of 9673: loss 1.726277470588684\n",
      "Step 16530 of 9673: loss 1.6566853523254395\n",
      "Step 16540 of 9673: loss 1.7898863554000854\n",
      "Step 16550 of 9673: loss 1.7868759632110596\n",
      "Step 16560 of 9673: loss 1.813099980354309\n",
      "Step 16570 of 9673: loss 1.7825140953063965\n",
      "Step 16580 of 9673: loss 1.7284753322601318\n",
      "Step 16590 of 9673: loss 1.704720377922058\n",
      "Step 16600 of 9673: loss 1.78327476978302\n",
      "validation: loss 1.7498305913099308\n",
      "Step 16610 of 9673: loss 1.8223475217819214\n",
      "Step 16620 of 9673: loss 1.7568516731262207\n",
      "Step 16630 of 9673: loss 1.7450475692749023\n",
      "Step 16640 of 9673: loss 1.7362875938415527\n",
      "Step 16650 of 9673: loss 1.8189804553985596\n",
      "Step 16660 of 9673: loss 1.7347112894058228\n",
      "Step 16670 of 9673: loss 1.7773915529251099\n",
      "Step 16680 of 9673: loss 1.784024953842163\n",
      "Step 16690 of 9673: loss 1.7374675273895264\n",
      "Step 16700 of 9673: loss 1.7648125886917114\n",
      "validation: loss 1.7499947511043745\n",
      "Step 16710 of 9673: loss 1.817638635635376\n",
      "Step 16720 of 9673: loss 1.728708028793335\n",
      "Step 16730 of 9673: loss 1.7876499891281128\n",
      "Step 16740 of 9673: loss 1.7308610677719116\n",
      "Step 16750 of 9673: loss 1.7905012369155884\n",
      "Step 16760 of 9673: loss 1.774530053138733\n",
      "Step 16770 of 9673: loss 1.7399624586105347\n",
      "Step 16780 of 9673: loss 1.7432001829147339\n",
      "Step 16790 of 9673: loss 1.7809573411941528\n",
      "Step 16800 of 9673: loss 1.6526325941085815\n",
      "validation: loss 1.7506157110646827\n",
      "Step 16810 of 9673: loss 1.7816816568374634\n",
      "Step 16820 of 9673: loss 1.8307090997695923\n",
      "Step 16830 of 9673: loss 1.790948510169983\n",
      "Step 16840 of 9673: loss 1.786173939704895\n",
      "Step 16850 of 9673: loss 1.7719122171401978\n",
      "Step 16860 of 9673: loss 1.7612082958221436\n",
      "Step 16870 of 9673: loss 1.730199933052063\n",
      "Step 16880 of 9673: loss 1.7608919143676758\n",
      "Step 16890 of 9673: loss 1.762168288230896\n",
      "Step 16900 of 9673: loss 1.7988396883010864\n",
      "validation: loss 1.7500899833502228\n",
      "Step 16910 of 9673: loss 1.771181583404541\n",
      "Step 16920 of 9673: loss 1.7523027658462524\n",
      "Step 16930 of 9673: loss 1.7845656871795654\n",
      "Step 16940 of 9673: loss 1.7789721488952637\n",
      "Step 16950 of 9673: loss 1.755179524421692\n",
      "Step 16960 of 9673: loss 1.8145148754119873\n",
      "Step 16970 of 9673: loss 1.7655274868011475\n",
      "Step 16980 of 9673: loss 1.7631654739379883\n",
      "Step 16990 of 9673: loss 1.7273935079574585\n",
      "Step 17000 of 9673: loss 1.7065318822860718\n",
      "validation: loss 1.7502822802238858\n",
      "Step 17010 of 9673: loss 1.8081468343734741\n",
      "Step 17020 of 9673: loss 1.7304983139038086\n",
      "Step 17030 of 9673: loss 1.7594610452651978\n",
      "Step 17040 of 9673: loss 1.7318871021270752\n",
      "Step 17050 of 9673: loss 1.8101403713226318\n",
      "Step 17060 of 9673: loss 1.7715461254119873\n",
      "Step 17070 of 9673: loss 1.6974587440490723\n",
      "Step 17080 of 9673: loss 1.7387135028839111\n",
      "Step 17090 of 9673: loss 1.7452998161315918\n",
      "Step 17100 of 9673: loss 1.7253568172454834\n",
      "validation: loss 1.7501394158786105\n",
      "Step 17110 of 9673: loss 1.6942445039749146\n",
      "Step 17120 of 9673: loss 1.7918789386749268\n",
      "Step 17130 of 9673: loss 1.7571287155151367\n",
      "Step 17140 of 9673: loss 1.750118374824524\n",
      "Step 17150 of 9673: loss 1.7030351161956787\n",
      "Step 17160 of 9673: loss 1.8449969291687012\n",
      "Step 17170 of 9673: loss 1.815292477607727\n",
      "Step 17180 of 9673: loss 1.7464356422424316\n",
      "Step 17190 of 9673: loss 1.731654167175293\n",
      "Step 17200 of 9673: loss 1.7977426052093506\n",
      "validation: loss 1.7504763431155805\n",
      "Step 17210 of 9673: loss 1.7671277523040771\n",
      "Step 17220 of 9673: loss 1.7914235591888428\n",
      "Step 17230 of 9673: loss 1.7548997402191162\n",
      "Step 17240 of 9673: loss 1.7596009969711304\n",
      "Step 17250 of 9673: loss 1.7438592910766602\n",
      "Step 17260 of 9673: loss 1.825326681137085\n",
      "Step 17270 of 9673: loss 1.7518806457519531\n",
      "Step 17280 of 9673: loss 1.7544465065002441\n",
      "Step 17290 of 9673: loss 1.785278081893921\n",
      "Step 17300 of 9673: loss 1.7484409809112549\n",
      "validation: loss 1.7493738329287656\n",
      "Step 17310 of 9673: loss 1.7816725969314575\n",
      "Step 17320 of 9673: loss 1.8226292133331299\n",
      "Step 17330 of 9673: loss 1.7496931552886963\n",
      "Step 17340 of 9673: loss 1.6916234493255615\n",
      "Step 17350 of 9673: loss 1.771898627281189\n",
      "Step 17360 of 9673: loss 1.768807291984558\n",
      "Step 17370 of 9673: loss 1.7684086561203003\n",
      "Step 17380 of 9673: loss 1.7219651937484741\n",
      "Step 17390 of 9673: loss 1.8381733894348145\n",
      "Step 17400 of 9673: loss 1.8076294660568237\n",
      "validation: loss 1.749375289248437\n",
      "Step 17410 of 9673: loss 1.7427244186401367\n",
      "Step 17420 of 9673: loss 1.741465449333191\n",
      "Step 17430 of 9673: loss 1.7406668663024902\n",
      "Step 17440 of 9673: loss 1.747323989868164\n",
      "Step 17450 of 9673: loss 1.7745578289031982\n",
      "Step 17460 of 9673: loss 1.738198161125183\n",
      "Step 17470 of 9673: loss 1.7505378723144531\n",
      "Step 17480 of 9673: loss 1.6968121528625488\n",
      "Step 17490 of 9673: loss 1.7908118963241577\n",
      "Step 17500 of 9673: loss 1.7251354455947876\n",
      "validation: loss 1.7500628994912217\n",
      "Step 17510 of 9673: loss 1.788568377494812\n",
      "Step 17520 of 9673: loss 1.7307426929473877\n",
      "Step 17530 of 9673: loss 1.7672858238220215\n",
      "Step 17540 of 9673: loss 1.686467170715332\n",
      "Step 17550 of 9673: loss 1.8759559392929077\n",
      "Step 17560 of 9673: loss 1.7870293855667114\n",
      "Step 17570 of 9673: loss 1.7090401649475098\n",
      "Step 17580 of 9673: loss 1.7342283725738525\n",
      "Step 17590 of 9673: loss 1.7672135829925537\n",
      "Step 17600 of 9673: loss 1.8095729351043701\n",
      "validation: loss 1.7498440754782294\n",
      "Step 17610 of 9673: loss 1.8533649444580078\n",
      "Step 17620 of 9673: loss 1.7630877494812012\n",
      "Step 17630 of 9673: loss 1.770432472229004\n",
      "Step 17640 of 9673: loss 1.7474148273468018\n",
      "Step 17650 of 9673: loss 1.7564969062805176\n",
      "Step 17660 of 9673: loss 1.7617998123168945\n",
      "Step 17670 of 9673: loss 1.7621114253997803\n",
      "Step 17680 of 9673: loss 1.7200926542282104\n",
      "Step 17690 of 9673: loss 1.785391926765442\n",
      "Step 17700 of 9673: loss 1.7545350790023804\n",
      "validation: loss 1.749265277508608\n",
      "Step 17710 of 9673: loss 1.8108267784118652\n",
      "Step 17720 of 9673: loss 1.8265496492385864\n",
      "Step 17730 of 9673: loss 1.7651989459991455\n",
      "Step 17740 of 9673: loss 1.753496766090393\n",
      "Step 17750 of 9673: loss 1.7632808685302734\n",
      "Step 17760 of 9673: loss 1.750283122062683\n",
      "Step 17770 of 9673: loss 1.7714582681655884\n",
      "Step 17780 of 9673: loss 1.736735224723816\n",
      "Step 17790 of 9673: loss 1.7275766134262085\n",
      "Step 17800 of 9673: loss 1.754926323890686\n",
      "validation: loss 1.74960407645432\n",
      "Step 17810 of 9673: loss 1.7464133501052856\n",
      "Step 17820 of 9673: loss 1.7476794719696045\n",
      "Step 17830 of 9673: loss 1.7963919639587402\n",
      "Step 17840 of 9673: loss 1.7860547304153442\n",
      "Step 17850 of 9673: loss 1.8656132221221924\n",
      "Step 17860 of 9673: loss 1.740362286567688\n",
      "Step 17870 of 9673: loss 1.7344286441802979\n",
      "Step 17880 of 9673: loss 1.7844786643981934\n",
      "Step 17890 of 9673: loss 1.7418293952941895\n",
      "Step 17900 of 9673: loss 1.812179684638977\n",
      "validation: loss 1.749129066762236\n",
      "Step 17910 of 9673: loss 1.7029147148132324\n",
      "Step 17920 of 9673: loss 1.7366509437561035\n",
      "Step 17930 of 9673: loss 1.7041864395141602\n",
      "Step 17940 of 9673: loss 1.7770947217941284\n",
      "Step 17950 of 9673: loss 1.8326321840286255\n",
      "Step 17960 of 9673: loss 1.7799397706985474\n",
      "Step 17970 of 9673: loss 1.7750009298324585\n",
      "Step 17980 of 9673: loss 1.798932671546936\n",
      "Step 17990 of 9673: loss 1.7469136714935303\n",
      "Step 18000 of 9673: loss 1.7691104412078857\n",
      "validation: loss 1.7493655583293168\n",
      "Step 18010 of 9673: loss 1.7783960103988647\n",
      "Step 18020 of 9673: loss 1.8001999855041504\n",
      "Step 18030 of 9673: loss 1.7766603231430054\n",
      "Step 18040 of 9673: loss 1.7790906429290771\n",
      "Step 18050 of 9673: loss 1.7471272945404053\n",
      "Step 18060 of 9673: loss 1.720621943473816\n",
      "Step 18070 of 9673: loss 1.7209267616271973\n",
      "Step 18080 of 9673: loss 1.7871776819229126\n",
      "Step 18090 of 9673: loss 1.812469244003296\n",
      "Step 18100 of 9673: loss 1.7398184537887573\n",
      "validation: loss 1.7495787672160827\n",
      "Step 18110 of 9673: loss 1.7876297235488892\n",
      "Step 18120 of 9673: loss 1.7778873443603516\n",
      "Step 18130 of 9673: loss 1.7455005645751953\n",
      "Step 18140 of 9673: loss 1.731330156326294\n",
      "Step 18150 of 9673: loss 1.8039745092391968\n",
      "Step 18160 of 9673: loss 1.757247805595398\n",
      "Step 18170 of 9673: loss 1.7782642841339111\n",
      "Step 18180 of 9673: loss 1.738642930984497\n",
      "Step 18190 of 9673: loss 1.7735881805419922\n",
      "Step 18200 of 9673: loss 1.7951819896697998\n",
      "validation: loss 1.7497086758466112\n",
      "Step 18210 of 9673: loss 1.7535682916641235\n",
      "Step 18220 of 9673: loss 1.781062364578247\n",
      "Step 18230 of 9673: loss 1.7797741889953613\n",
      "Step 18240 of 9673: loss 1.752303123474121\n",
      "Step 18250 of 9673: loss 1.705809235572815\n",
      "Step 18260 of 9673: loss 1.748778223991394\n",
      "Step 18270 of 9673: loss 1.7368968725204468\n",
      "Step 18280 of 9673: loss 1.7876476049423218\n",
      "Step 18290 of 9673: loss 1.7749592065811157\n",
      "Step 18300 of 9673: loss 1.7664381265640259\n",
      "validation: loss 1.7501766718539995\n",
      "Step 18310 of 9673: loss 1.7312917709350586\n",
      "Step 18320 of 9673: loss 1.7580924034118652\n",
      "Step 18330 of 9673: loss 1.736592411994934\n",
      "Step 18340 of 9673: loss 1.8021849393844604\n",
      "Step 18350 of 9673: loss 1.727158546447754\n",
      "Step 18360 of 9673: loss 1.8219730854034424\n",
      "Step 18370 of 9673: loss 1.7621278762817383\n",
      "Step 18380 of 9673: loss 1.746591329574585\n",
      "Step 18390 of 9673: loss 1.7674874067306519\n",
      "Step 18400 of 9673: loss 1.7587248086929321\n",
      "validation: loss 1.7500755872923075\n",
      "Step 18410 of 9673: loss 1.7479937076568604\n",
      "Step 18420 of 9673: loss 1.7183260917663574\n",
      "Step 18430 of 9673: loss 1.7490787506103516\n",
      "Step 18440 of 9673: loss 1.7860159873962402\n",
      "Step 18450 of 9673: loss 1.803992748260498\n",
      "Step 18460 of 9673: loss 1.7613229751586914\n",
      "Step 18470 of 9673: loss 1.7516695261001587\n",
      "Step 18480 of 9673: loss 1.8067080974578857\n",
      "Step 18490 of 9673: loss 1.6817001104354858\n",
      "Step 18500 of 9673: loss 1.7866994142532349\n",
      "validation: loss 1.7490680402087182\n",
      "Step 18510 of 9673: loss 1.8143402338027954\n",
      "Step 18520 of 9673: loss 1.7672526836395264\n",
      "Step 18530 of 9673: loss 1.7813990116119385\n",
      "Step 18540 of 9673: loss 1.7804560661315918\n",
      "Step 18550 of 9673: loss 1.7450660467147827\n",
      "Step 18560 of 9673: loss 1.7522510290145874\n",
      "Step 18570 of 9673: loss 1.6950085163116455\n",
      "Step 18580 of 9673: loss 1.7017638683319092\n",
      "Step 18590 of 9673: loss 1.813442349433899\n",
      "Step 18600 of 9673: loss 1.781209111213684\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8f22c9d8790a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorytell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                     \u001b[0mtloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlossf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                     \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'validation: loss {tloss/steps}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for epoch in range(hyper.ep):\n",
    "    for batch in trainl:\n",
    "        seq = batch['input_ids'].to(device)\n",
    "        out = storytell(seq)\n",
    "        loss = lossf(t.flatten(out, end_dim=1), t.flatten(t.roll(seq, -1)))\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f'Step {step} of {len(trainl)}: loss {loss.item()}')\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            t.save(storytell.state_dict(), f'story_model_{step}.pt')\n",
    "            t.save(optim.state_dict(), f'story_optim_{step}.pt')\n",
    "        \n",
    "            with t.no_grad():\n",
    "                tloss = 0\n",
    "                steps = 0\n",
    "                storytell.eval()\n",
    "                for batch in validl:\n",
    "                    seq = batch['input_ids'].to(device)\n",
    "                    out = storytell(seq)\n",
    "                    tloss += lossf(t.flatten(out, end_dim=1), t.flatten(t.roll(seq, -1))).item()\n",
    "                    steps += 1\n",
    "                print(f'validation: loss {tloss/steps}')\n",
    "                storytell.train()\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# idx = random.randint(0, len(valid) - 1)\n",
    "# print(idx)\n",
    "\n",
    "# print(tok.decode(valid['input_ids'][idx]))\n",
    "\n",
    "# print('model gen:')\n",
    "# print(tok.decode(storytell(valid['input_ids'][idx].unsqueeze(0).to(t.long)).argmax(dim=-1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  dialogue\n",
      "Words:  watch, pasta, nosy\n",
      "Story: \n",
      "\n",
      " abby was so excited to go to the park. she was on her way when she noticed the pasta shop. she wanted to get a snack before the park.\n",
      "\n",
      " abby's mom told her to be careful with the money. abby\n",
      "torch.Size([1, 60])\n",
      " with the money. abby ate it in the ladder was about him with water. one day, her mommy tried to the truck to balance. it. he encountered a little boy named timmy. as they saw a big pose! i can listen carefully this movie could, there was a big open the little seed felt good idea, \"because i could fly and lay down. you and dance in the balls and bob loved to buy a big box. she found a boy, \"the moon, the little girl was to the sun that she loved paper otter was so glad to play with a mean children and unique journey home and welcomed jack was told her tomatoes and kept flying high for the wet leaf! \n",
      "Story:  once upon\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, model, tokenizer, temperature=1.0, max_len=512):\n",
    "    model.eval()\n",
    "    with t.no_grad():\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)[:, :-1]\n",
    "        print(input_ids.shape)\n",
    "        cur_len = input_ids.shape[1]\n",
    "        while cur_len < max_len:\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs[0][-1, :] / temperature\n",
    "            next_token_logits[1] = -float('inf')\n",
    "            next_token_id = t.multinomial(t.softmax(next_token_logits, dim=-1), num_samples=1).unsqueeze(-1)\n",
    "            input_ids = t.cat([input_ids, next_token_id], dim=1)\n",
    "            cur_len += 1\n",
    "            if next_token_id[0][0] == tokenizer.eos_token_id:\n",
    "                break\n",
    "        return tokenizer.decode(input_ids.squeeze()[55:], skip_special_tokens=False)\n",
    "    \n",
    "idx = random.randint(0, len(valid) - 1)\n",
    "prompt = tok.decode(valid['input_ids'][idx][:60])\n",
    "print(prompt)\n",
    "print(generate_text(prompt, storytell, tok, temperature=1, max_len=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
