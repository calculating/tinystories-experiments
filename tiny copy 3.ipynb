{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip3 install -U torch --index-url https://download.pytorch.org/whl/cu118\n",
    "pip install wandb==0.14.0\n",
    "wandb login e5292edda95a11630042fdf943d60d2bbf749fcf\n",
    "pip install datasets\n",
    "pip install tokenizers\n",
    "pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyt\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# data pipeline\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from typing import cast\n",
    "import math, random\n",
    "\n",
    "# tokenization\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# logging\n",
    "import os, argparse\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.set_default_device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper = {\n",
    "    'vs': 2**13,\n",
    "    'ly': 4,\n",
    "    'hs': 768,\n",
    "    'ah': 4,\n",
    "    'cx': 512,\n",
    "    'lr': 1e-4,\n",
    "    'bs': 256,\n",
    "    'ac': 4,\n",
    "    'ep': 10,\n",
    "}\n",
    "\n",
    "hyper = argparse.Namespace(**hyper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/skeskinen___parquet/skeskinen--TinyStories-Instruct-hf-1f9111cb77858404/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3953d208bdd416881e091baef2652f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2476533\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 25028\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = cast(DatasetDict, load_dataset('skeskinen/TinyStories-Instruct-hf'))\n",
    "dataset['train'].set_format(type='torch', columns=['text'])\n",
    "dataset['train'].format['type']\n",
    "dataset['validation'].set_format(type='torch', columns=['text'])\n",
    "dataset['validation'].format['type']\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(BPE())\n",
    "tok.normalizer = Lowercase()\n",
    "tok.pre_tokenizer = ByteLevel()\n",
    "tok.decoder = ByteLevelDecoder()\n",
    "tok.post_processor = TemplateProcessing(single='$0 <|endoftext|>', special_tokens=[('<|endoftext|>', 1)],)\n",
    "tok.enable_truncation(max_length=hyper.cx)\n",
    "tok.enable_padding(pad_token='<pad>', length=hyper.cx)\n",
    "trainer = BpeTrainer(vocab_size=hyper.vs, initial_alphabet=ByteLevel.alphabet(), special_tokens=['<pad>', '<|endoftext|>', '\\n','Words: ', 'Features: ', 'Random sentence: ', 'Summary: ', 'Story: '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('tiny.json'): tok = Tokenizer.from_file('tiny.json')\n",
    "else: tok.train_from_iterator(dataset['train']['text'], trainer=trainer); tok.save('tiny.json')\n",
    "\n",
    "tok = PreTrainedTokenizerFast(tokenizer_object=tok)\n",
    "tok.pad_token = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    return tok(example['text'], truncation=True, max_length=hyper.cx, padding='max_length')\n",
    "\n",
    "if os.path.exists('train_dataset') and os.path.exists('valid_dataset'):\n",
    "    train = load_from_disk('train_dataset')\n",
    "    valid = load_from_disk('valid_dataset')\n",
    "else:\n",
    "    train = dataset['train'].map(tokenization, batched=True, batch_size=8192, writer_batch_size=8192)\n",
    "    valid = dataset['validation'].map(tokenization, batched=True, batch_size=8192, writer_batch_size=8192)\n",
    "    train.save_to_disk('train_dataset')\n",
    "    valid.save_to_disk('valid_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask'])\n",
    "train.format['type']\n",
    "valid.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask'])\n",
    "valid.format['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainl = DataLoader(train, batch_size=hyper.bs, shuffle=True, drop_last=True)\n",
    "validl = DataLoader(valid, batch_size=hyper.bs, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = t.arange(max_len).unsqueeze(1)\n",
    "        div_term = t.exp(t.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = t.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = t.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = t.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "class trans(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inbed = nn.Embedding(hyper.vs, hyper.hs)\n",
    "        self.posit = PositionalEncoding(hyper.hs, hyper.cx)\n",
    "        self.think = nn.TransformerEncoderLayer(d_model=hyper.hs, nhead=hyper.ah, dim_feedforward=hyper.hs*4, activation='gelu')\n",
    "        self.thnkr = nn.TransformerEncoder(self.think, num_layers=hyper.ly)\n",
    "        self.speak = nn.Linear(hyper.hs, hyper.vs)\n",
    "        self.cmask= t.triu(t.ones(hyper.cx, hyper.cx) * float('-inf'), diagonal=1)\n",
    "    def forward(self, x):\n",
    "        x = self.inbed(x) * (hyper.hs ** .5)\n",
    "        x = self.posit(x)\n",
    "        x = self.thnkr(x, mask=self.cmask, is_causal=True)\n",
    "        return self.speak(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = wandb.init(\n",
    "#     project=\"tinystories\",\n",
    "#     config={\n",
    "#         \"learning_rate\": hyper.lr,\n",
    "#         \"epochs\": 10,\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35.4 million parameters in the model, plus 12.6 million embeddings parameters.\n"
     ]
    }
   ],
   "source": [
    "storytell = trans()\n",
    "\n",
    "print(f'There are {round((sum(p.numel() for p in storytell.parameters()) - hyper.vs*hyper.hs*2)/1e6, 1)} million parameters in the model, plus {round((hyper.vs*hyper.hs*2)/1e6, 1)} million embeddings parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.watch(storytell, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = t.optim.Adam(storytell.parameters(), lr=hyper.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossf = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.load_state_dict(t.load('story_optim_6400.pt', map_location='mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storytell.load_state_dict(t.load('story_model_18600.pt', map_location='mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 of 9673: loss 1.7828541994094849\n",
      "validation: loss 1.7633733073460687\n",
      "Step 10 of 9673: loss 1.7870320081710815\n",
      "Step 20 of 9673: loss 1.7119168043136597\n",
      "Step 30 of 9673: loss 1.7623684406280518\n",
      "Step 40 of 9673: loss 1.7257035970687866\n",
      "Step 50 of 9673: loss 1.8138842582702637\n",
      "Step 60 of 9673: loss 1.8084945678710938\n",
      "Step 70 of 9673: loss 1.7809900045394897\n",
      "Step 80 of 9673: loss 1.8043197393417358\n",
      "Step 90 of 9673: loss 1.8057655096054077\n",
      "Step 100 of 9673: loss 1.8252079486846924\n",
      "validation: loss 1.7632240179887753\n",
      "Step 110 of 9673: loss 1.76325523853302\n",
      "Step 120 of 9673: loss 1.7604920864105225\n",
      "Step 130 of 9673: loss 1.816753625869751\n",
      "Step 140 of 9673: loss 1.8630592823028564\n",
      "Step 150 of 9673: loss 1.772922396659851\n",
      "Step 160 of 9673: loss 1.7300993204116821\n",
      "Step 170 of 9673: loss 1.7685534954071045\n",
      "Step 180 of 9673: loss 1.736989974975586\n",
      "Step 190 of 9673: loss 1.7772109508514404\n",
      "Step 200 of 9673: loss 1.836031198501587\n",
      "validation: loss 1.7626174479415737\n",
      "Step 210 of 9673: loss 1.753210186958313\n",
      "Step 220 of 9673: loss 1.7648342847824097\n",
      "Step 230 of 9673: loss 1.743239164352417\n",
      "Step 240 of 9673: loss 1.7913358211517334\n",
      "Step 250 of 9673: loss 1.7693240642547607\n",
      "Step 260 of 9673: loss 1.7881176471710205\n",
      "Step 270 of 9673: loss 1.727530837059021\n",
      "Step 280 of 9673: loss 1.78781259059906\n",
      "Step 290 of 9673: loss 1.8253642320632935\n",
      "Step 300 of 9673: loss 1.7381465435028076\n",
      "validation: loss 1.7622881955707197\n",
      "Step 310 of 9673: loss 1.7398872375488281\n",
      "Step 320 of 9673: loss 1.7251487970352173\n",
      "Step 330 of 9673: loss 1.8639100790023804\n",
      "Step 340 of 9673: loss 1.7338324785232544\n",
      "Step 350 of 9673: loss 1.7656883001327515\n",
      "Step 360 of 9673: loss 1.7539910078048706\n",
      "Step 370 of 9673: loss 1.769028902053833\n",
      "Step 380 of 9673: loss 1.7873390913009644\n",
      "Step 390 of 9673: loss 1.7910405397415161\n",
      "Step 400 of 9673: loss 1.8272050619125366\n",
      "validation: loss 1.7617662850114488\n",
      "Step 410 of 9673: loss 1.8006454706192017\n",
      "Step 420 of 9673: loss 1.8237700462341309\n",
      "Step 430 of 9673: loss 1.7133888006210327\n",
      "Step 440 of 9673: loss 1.7613838911056519\n",
      "Step 450 of 9673: loss 1.7486045360565186\n",
      "Step 460 of 9673: loss 1.7527186870574951\n",
      "Step 470 of 9673: loss 1.734731674194336\n",
      "Step 480 of 9673: loss 1.7680402994155884\n",
      "Step 490 of 9673: loss 1.7664837837219238\n",
      "Step 500 of 9673: loss 1.7857710123062134\n",
      "validation: loss 1.7623397701794339\n",
      "Step 510 of 9673: loss 1.7643183469772339\n",
      "Step 520 of 9673: loss 1.8154633045196533\n",
      "Step 530 of 9673: loss 1.7724897861480713\n",
      "Step 540 of 9673: loss 1.7918797731399536\n",
      "Step 550 of 9673: loss 1.8124043941497803\n",
      "Step 560 of 9673: loss 1.807525396347046\n",
      "Step 570 of 9673: loss 1.7377101182937622\n",
      "Step 580 of 9673: loss 1.758903980255127\n",
      "Step 590 of 9673: loss 1.7418909072875977\n",
      "Step 600 of 9673: loss 1.7041581869125366\n",
      "validation: loss 1.763332320242813\n",
      "Step 610 of 9673: loss 1.7729634046554565\n",
      "Step 620 of 9673: loss 1.7727956771850586\n",
      "Step 630 of 9673: loss 1.7737400531768799\n",
      "Step 640 of 9673: loss 1.758716106414795\n",
      "Step 650 of 9673: loss 1.7532894611358643\n",
      "Step 660 of 9673: loss 1.8838951587677002\n",
      "Step 670 of 9673: loss 1.7295904159545898\n",
      "Step 680 of 9673: loss 1.7629190683364868\n",
      "Step 690 of 9673: loss 1.751132845878601\n",
      "Step 700 of 9673: loss 1.7662373781204224\n",
      "validation: loss 1.7628296343321652\n",
      "Step 710 of 9673: loss 1.7571717500686646\n",
      "Step 720 of 9673: loss 1.6987254619598389\n",
      "Step 730 of 9673: loss 1.814160704612732\n",
      "Step 740 of 9673: loss 1.800217628479004\n",
      "Step 750 of 9673: loss 1.7925450801849365\n",
      "Step 760 of 9673: loss 1.87855863571167\n",
      "Step 770 of 9673: loss 1.8096530437469482\n",
      "Step 780 of 9673: loss 1.7666913270950317\n",
      "Step 790 of 9673: loss 1.7753500938415527\n",
      "Step 800 of 9673: loss 1.8404309749603271\n",
      "validation: loss 1.7624013595974322\n",
      "Step 810 of 9673: loss 1.7529646158218384\n",
      "Step 820 of 9673: loss 1.7307027578353882\n",
      "Step 830 of 9673: loss 1.7711528539657593\n",
      "Step 840 of 9673: loss 1.8077106475830078\n",
      "Step 850 of 9673: loss 1.783124566078186\n",
      "Step 860 of 9673: loss 1.7459867000579834\n",
      "Step 870 of 9673: loss 1.7771400213241577\n",
      "Step 880 of 9673: loss 1.7497142553329468\n",
      "Step 890 of 9673: loss 1.7493224143981934\n",
      "Step 900 of 9673: loss 1.7515312433242798\n",
      "validation: loss 1.7619045137130107\n",
      "Step 910 of 9673: loss 1.7076199054718018\n",
      "Step 920 of 9673: loss 1.8111412525177002\n",
      "Step 930 of 9673: loss 1.8261357545852661\n",
      "Step 940 of 9673: loss 1.799951434135437\n",
      "Step 950 of 9673: loss 1.8417179584503174\n",
      "Step 960 of 9673: loss 1.7857341766357422\n",
      "Step 970 of 9673: loss 1.7280040979385376\n",
      "Step 980 of 9673: loss 1.785957932472229\n",
      "Step 990 of 9673: loss 1.7207567691802979\n",
      "Step 1000 of 9673: loss 1.8598843812942505\n",
      "validation: loss 1.762362095498547\n",
      "Step 1010 of 9673: loss 1.7847234010696411\n",
      "Step 1020 of 9673: loss 1.7647944688796997\n",
      "Step 1030 of 9673: loss 1.7258672714233398\n",
      "Step 1040 of 9673: loss 1.7684216499328613\n",
      "Step 1050 of 9673: loss 1.7563828229904175\n",
      "Step 1060 of 9673: loss 1.8250592947006226\n",
      "Step 1070 of 9673: loss 1.7463622093200684\n",
      "Step 1080 of 9673: loss 1.7639673948287964\n",
      "Step 1090 of 9673: loss 1.7192516326904297\n",
      "Step 1100 of 9673: loss 1.7431697845458984\n",
      "validation: loss 1.7616596860983937\n",
      "Step 1110 of 9673: loss 1.7768356800079346\n",
      "Step 1120 of 9673: loss 1.7637763023376465\n",
      "Step 1130 of 9673: loss 1.7993741035461426\n",
      "Step 1140 of 9673: loss 1.7626901865005493\n",
      "Step 1150 of 9673: loss 1.8116426467895508\n",
      "Step 1160 of 9673: loss 1.7873467206954956\n",
      "Step 1170 of 9673: loss 1.8248825073242188\n",
      "Step 1180 of 9673: loss 1.793123722076416\n",
      "Step 1190 of 9673: loss 1.7238609790802002\n",
      "Step 1200 of 9673: loss 1.772888422012329\n",
      "validation: loss 1.7613835457674007\n",
      "Step 1210 of 9673: loss 1.7571543455123901\n",
      "Step 1220 of 9673: loss 1.7997416257858276\n",
      "Step 1230 of 9673: loss 1.755838394165039\n",
      "Step 1240 of 9673: loss 1.7799959182739258\n",
      "Step 1250 of 9673: loss 1.7535381317138672\n",
      "Step 1260 of 9673: loss 1.7256240844726562\n",
      "Step 1270 of 9673: loss 1.7935247421264648\n",
      "Step 1280 of 9673: loss 1.7836960554122925\n",
      "Step 1290 of 9673: loss 1.8126097917556763\n",
      "Step 1300 of 9673: loss 1.7531406879425049\n",
      "validation: loss 1.7609818928020518\n",
      "Step 1310 of 9673: loss 1.7140955924987793\n",
      "Step 1320 of 9673: loss 1.7943086624145508\n",
      "Step 1330 of 9673: loss 1.7547812461853027\n",
      "Step 1340 of 9673: loss 1.830371618270874\n",
      "Step 1350 of 9673: loss 1.8429968357086182\n",
      "Step 1360 of 9673: loss 1.776836633682251\n",
      "Step 1370 of 9673: loss 1.8041257858276367\n",
      "Step 1380 of 9673: loss 1.7968642711639404\n",
      "Step 1390 of 9673: loss 1.7822388410568237\n",
      "Step 1400 of 9673: loss 1.8489136695861816\n",
      "validation: loss 1.761347268045563\n",
      "Step 1410 of 9673: loss 1.831895112991333\n",
      "Step 1420 of 9673: loss 1.823473334312439\n",
      "Step 1430 of 9673: loss 1.7811110019683838\n",
      "Step 1440 of 9673: loss 1.7349551916122437\n",
      "Step 1450 of 9673: loss 1.7703465223312378\n",
      "Step 1460 of 9673: loss 1.7862673997879028\n",
      "Step 1470 of 9673: loss 1.7856510877609253\n",
      "Step 1480 of 9673: loss 1.6850870847702026\n",
      "Step 1490 of 9673: loss 1.8835432529449463\n",
      "Step 1500 of 9673: loss 1.823452353477478\n",
      "validation: loss 1.7614532426460503\n",
      "Step 1510 of 9673: loss 1.7749769687652588\n",
      "Step 1520 of 9673: loss 1.7339766025543213\n",
      "Step 1530 of 9673: loss 1.8066236972808838\n",
      "Step 1540 of 9673: loss 1.8031501770019531\n",
      "Step 1550 of 9673: loss 1.7546333074569702\n",
      "Step 1560 of 9673: loss 1.8143030405044556\n",
      "Step 1570 of 9673: loss 1.7591506242752075\n",
      "Step 1580 of 9673: loss 1.7601933479309082\n",
      "Step 1590 of 9673: loss 1.8557357788085938\n",
      "Step 1600 of 9673: loss 1.7426540851593018\n",
      "validation: loss 1.761606738739407\n",
      "Step 1610 of 9673: loss 1.795628547668457\n",
      "Step 1620 of 9673: loss 1.7494902610778809\n",
      "Step 1630 of 9673: loss 1.833418607711792\n",
      "Step 1640 of 9673: loss 1.7992005348205566\n",
      "Step 1650 of 9673: loss 1.7435534000396729\n",
      "Step 1660 of 9673: loss 1.7935326099395752\n",
      "Step 1670 of 9673: loss 1.7415682077407837\n",
      "Step 1680 of 9673: loss 1.7940657138824463\n",
      "Step 1690 of 9673: loss 1.761500358581543\n",
      "Step 1700 of 9673: loss 1.7605805397033691\n",
      "validation: loss 1.7611736882593214\n",
      "Step 1710 of 9673: loss 1.761950135231018\n",
      "Step 1720 of 9673: loss 1.7826921939849854\n",
      "Step 1730 of 9673: loss 1.765241265296936\n",
      "Step 1740 of 9673: loss 1.8293015956878662\n",
      "Step 1750 of 9673: loss 1.7454450130462646\n",
      "Step 1760 of 9673: loss 1.7827520370483398\n",
      "Step 1770 of 9673: loss 1.750174641609192\n",
      "Step 1780 of 9673: loss 1.8261209726333618\n",
      "Step 1790 of 9673: loss 1.7828229665756226\n",
      "Step 1800 of 9673: loss 1.672178030014038\n",
      "validation: loss 1.7615532076235898\n",
      "Step 1810 of 9673: loss 1.751581072807312\n",
      "Step 1820 of 9673: loss 1.7360752820968628\n",
      "Step 1830 of 9673: loss 1.8051080703735352\n",
      "Step 1840 of 9673: loss 1.7066806554794312\n",
      "Step 1850 of 9673: loss 1.7295293807983398\n",
      "Step 1860 of 9673: loss 1.8039557933807373\n",
      "Step 1870 of 9673: loss 1.8464574813842773\n",
      "Step 1880 of 9673: loss 1.8200596570968628\n",
      "Step 1890 of 9673: loss 1.8578174114227295\n",
      "Step 1900 of 9673: loss 1.761110544204712\n",
      "validation: loss 1.7612405806472622\n",
      "Step 1910 of 9673: loss 1.8135086297988892\n",
      "Step 1920 of 9673: loss 1.815482258796692\n",
      "Step 1930 of 9673: loss 1.8278248310089111\n",
      "Step 1940 of 9673: loss 1.7923389673233032\n",
      "Step 1950 of 9673: loss 1.7643297910690308\n",
      "Step 1960 of 9673: loss 1.7911967039108276\n",
      "Step 1970 of 9673: loss 1.7664066553115845\n",
      "Step 1980 of 9673: loss 1.8209441900253296\n",
      "Step 1990 of 9673: loss 1.8124815225601196\n",
      "Step 2000 of 9673: loss 1.7533694505691528\n",
      "validation: loss 1.7610546030949072\n",
      "Step 2010 of 9673: loss 1.811822772026062\n",
      "Step 2020 of 9673: loss 1.7921732664108276\n",
      "Step 2030 of 9673: loss 1.8187367916107178\n",
      "Step 2040 of 9673: loss 1.7360645532608032\n",
      "Step 2050 of 9673: loss 1.781185507774353\n",
      "Step 2060 of 9673: loss 1.7821072340011597\n",
      "Step 2070 of 9673: loss 1.7959133386611938\n",
      "Step 2080 of 9673: loss 1.7707027196884155\n",
      "Step 2090 of 9673: loss 1.7820225954055786\n",
      "Step 2100 of 9673: loss 1.8019773960113525\n",
      "validation: loss 1.7607159282743317\n",
      "Step 2110 of 9673: loss 1.8095495700836182\n",
      "Step 2120 of 9673: loss 1.7578707933425903\n",
      "Step 2130 of 9673: loss 1.7446683645248413\n",
      "Step 2140 of 9673: loss 1.786341905593872\n",
      "Step 2150 of 9673: loss 1.7888901233673096\n",
      "Step 2160 of 9673: loss 1.804901123046875\n",
      "Step 2170 of 9673: loss 1.7417833805084229\n",
      "Step 2180 of 9673: loss 1.7394133806228638\n",
      "Step 2190 of 9673: loss 1.8041038513183594\n",
      "Step 2200 of 9673: loss 1.80991792678833\n",
      "validation: loss 1.7616330889082445\n",
      "Step 2210 of 9673: loss 1.7706961631774902\n",
      "Step 2220 of 9673: loss 1.7422488927841187\n",
      "Step 2230 of 9673: loss 1.7177846431732178\n",
      "Step 2240 of 9673: loss 1.7331221103668213\n",
      "Step 2250 of 9673: loss 1.762609839439392\n",
      "Step 2260 of 9673: loss 1.75937819480896\n",
      "Step 2270 of 9673: loss 1.8061288595199585\n",
      "Step 2280 of 9673: loss 1.833990454673767\n",
      "Step 2290 of 9673: loss 1.7893859148025513\n",
      "Step 2300 of 9673: loss 1.794829249382019\n",
      "validation: loss 1.760785909043145\n",
      "Step 2310 of 9673: loss 1.7669275999069214\n",
      "Step 2320 of 9673: loss 1.785287618637085\n",
      "Step 2330 of 9673: loss 1.830346941947937\n",
      "Step 2340 of 9673: loss 1.8222687244415283\n",
      "Step 2350 of 9673: loss 1.719080924987793\n",
      "Step 2360 of 9673: loss 1.7733920812606812\n",
      "Step 2370 of 9673: loss 1.7963571548461914\n",
      "Step 2380 of 9673: loss 1.828345537185669\n",
      "Step 2390 of 9673: loss 1.7980232238769531\n",
      "Step 2400 of 9673: loss 1.770984172821045\n",
      "validation: loss 1.7606855335923814\n",
      "Step 2410 of 9673: loss 1.832895040512085\n",
      "Step 2420 of 9673: loss 1.7559256553649902\n",
      "Step 2430 of 9673: loss 1.7452763319015503\n",
      "Step 2440 of 9673: loss 1.8251397609710693\n",
      "Step 2450 of 9673: loss 1.7773710489273071\n",
      "Step 2460 of 9673: loss 1.7437710762023926\n",
      "Step 2470 of 9673: loss 1.7783334255218506\n",
      "Step 2480 of 9673: loss 1.8613088130950928\n",
      "Step 2490 of 9673: loss 1.7592869997024536\n",
      "Step 2500 of 9673: loss 1.8020563125610352\n",
      "validation: loss 1.760564103568952\n",
      "Step 2510 of 9673: loss 1.7150031328201294\n",
      "Step 2520 of 9673: loss 1.8048094511032104\n",
      "Step 2530 of 9673: loss 1.7509657144546509\n",
      "Step 2540 of 9673: loss 1.7862709760665894\n",
      "Step 2550 of 9673: loss 1.7467472553253174\n",
      "Step 2560 of 9673: loss 1.755815029144287\n",
      "Step 2570 of 9673: loss 1.8397690057754517\n",
      "Step 2580 of 9673: loss 1.7415950298309326\n",
      "Step 2590 of 9673: loss 1.8125890493392944\n",
      "Step 2600 of 9673: loss 1.7892833948135376\n",
      "validation: loss 1.7609312374567248\n",
      "Step 2610 of 9673: loss 1.7842055559158325\n",
      "Step 2620 of 9673: loss 1.802325963973999\n",
      "Step 2630 of 9673: loss 1.807435154914856\n",
      "Step 2640 of 9673: loss 1.7962347269058228\n",
      "Step 2650 of 9673: loss 1.7985196113586426\n",
      "Step 2660 of 9673: loss 1.7687007188796997\n",
      "Step 2670 of 9673: loss 1.7830291986465454\n",
      "Step 2680 of 9673: loss 1.7960408926010132\n",
      "Step 2690 of 9673: loss 1.7871819734573364\n",
      "Step 2700 of 9673: loss 1.8007762432098389\n",
      "validation: loss 1.7609908593069648\n",
      "Step 2710 of 9673: loss 1.7422893047332764\n",
      "Step 2720 of 9673: loss 1.706017255783081\n",
      "Step 2730 of 9673: loss 1.7455003261566162\n",
      "Step 2740 of 9673: loss 1.746131420135498\n",
      "Step 2750 of 9673: loss 1.8191050291061401\n",
      "Step 2760 of 9673: loss 1.7815176248550415\n",
      "Step 2770 of 9673: loss 1.7912254333496094\n",
      "Step 2780 of 9673: loss 1.8606511354446411\n",
      "Step 2790 of 9673: loss 1.7709347009658813\n",
      "Step 2800 of 9673: loss 1.8032723665237427\n",
      "validation: loss 1.7595635431329\n",
      "Step 2810 of 9673: loss 1.7092227935791016\n",
      "Step 2820 of 9673: loss 1.8299684524536133\n",
      "Step 2830 of 9673: loss 1.7702909708023071\n",
      "Step 2840 of 9673: loss 1.8223294019699097\n",
      "Step 2850 of 9673: loss 1.7908406257629395\n",
      "Step 2860 of 9673: loss 1.8021358251571655\n",
      "Step 2870 of 9673: loss 1.8432453870773315\n",
      "Step 2880 of 9673: loss 1.7803902626037598\n",
      "Step 2890 of 9673: loss 1.720521092414856\n",
      "Step 2900 of 9673: loss 1.7802865505218506\n",
      "validation: loss 1.7597542435852522\n",
      "Step 2910 of 9673: loss 1.745506763458252\n",
      "Step 2920 of 9673: loss 1.7131900787353516\n",
      "Step 2930 of 9673: loss 1.7911533117294312\n",
      "Step 2940 of 9673: loss 1.7550441026687622\n",
      "Step 2950 of 9673: loss 1.728775143623352\n",
      "Step 2960 of 9673: loss 1.7399606704711914\n",
      "Step 2970 of 9673: loss 1.8803144693374634\n",
      "Step 2980 of 9673: loss 1.7451132535934448\n",
      "Step 2990 of 9673: loss 1.7996644973754883\n",
      "Step 3000 of 9673: loss 1.7140412330627441\n",
      "validation: loss 1.761735186134417\n",
      "Step 3010 of 9673: loss 1.7543327808380127\n",
      "Step 3020 of 9673: loss 1.7964746952056885\n",
      "Step 3030 of 9673: loss 1.7610348463058472\n",
      "Step 3040 of 9673: loss 1.7969931364059448\n",
      "Step 3050 of 9673: loss 1.822525978088379\n",
      "Step 3060 of 9673: loss 1.7714035511016846\n",
      "Step 3070 of 9673: loss 1.7025011777877808\n",
      "Step 3080 of 9673: loss 1.7747207880020142\n",
      "Step 3090 of 9673: loss 1.846701741218567\n",
      "Step 3100 of 9673: loss 1.824343204498291\n",
      "validation: loss 1.759432583740077\n",
      "Step 3110 of 9673: loss 1.7221113443374634\n",
      "Step 3120 of 9673: loss 1.6641058921813965\n",
      "Step 3130 of 9673: loss 1.726852536201477\n",
      "Step 3140 of 9673: loss 1.7859575748443604\n",
      "Step 3150 of 9673: loss 1.776084542274475\n",
      "Step 3160 of 9673: loss 1.8398059606552124\n",
      "Step 3170 of 9673: loss 1.7334325313568115\n",
      "Step 3180 of 9673: loss 1.7623385190963745\n",
      "Step 3190 of 9673: loss 1.7474050521850586\n",
      "Step 3200 of 9673: loss 1.7510722875595093\n",
      "validation: loss 1.7602669799450748\n",
      "Step 3210 of 9673: loss 1.7666189670562744\n",
      "Step 3220 of 9673: loss 1.7758474349975586\n",
      "Step 3230 of 9673: loss 1.8005006313323975\n",
      "Step 3240 of 9673: loss 1.6833560466766357\n",
      "Step 3250 of 9673: loss 1.7502808570861816\n",
      "Step 3260 of 9673: loss 1.8170020580291748\n",
      "Step 3270 of 9673: loss 1.7696346044540405\n",
      "Step 3280 of 9673: loss 1.7445799112319946\n",
      "Step 3290 of 9673: loss 1.747338056564331\n",
      "Step 3300 of 9673: loss 1.7881039381027222\n",
      "validation: loss 1.759894834351294\n",
      "Step 3310 of 9673: loss 1.7906829118728638\n",
      "Step 3320 of 9673: loss 1.7862848043441772\n",
      "Step 3330 of 9673: loss 1.7666280269622803\n",
      "Step 3340 of 9673: loss 1.7473734617233276\n",
      "Step 3350 of 9673: loss 1.776288628578186\n",
      "Step 3360 of 9673: loss 1.7656978368759155\n",
      "Step 3370 of 9673: loss 1.7633394002914429\n",
      "Step 3380 of 9673: loss 1.7457711696624756\n",
      "Step 3390 of 9673: loss 1.746194839477539\n",
      "Step 3400 of 9673: loss 1.7940040826797485\n",
      "validation: loss 1.759286149260924\n",
      "Step 3410 of 9673: loss 1.7769463062286377\n",
      "Step 3420 of 9673: loss 1.809369444847107\n",
      "Step 3430 of 9673: loss 1.7804516553878784\n",
      "Step 3440 of 9673: loss 1.7520784139633179\n",
      "Step 3450 of 9673: loss 1.7186038494110107\n",
      "Step 3460 of 9673: loss 1.7819770574569702\n",
      "Step 3470 of 9673: loss 1.7387588024139404\n",
      "Step 3480 of 9673: loss 1.8055756092071533\n",
      "Step 3490 of 9673: loss 1.7250957489013672\n",
      "Step 3500 of 9673: loss 1.7788673639297485\n",
      "validation: loss 1.7595486051028537\n",
      "Step 3510 of 9673: loss 1.758974313735962\n",
      "Step 3520 of 9673: loss 1.7652956247329712\n",
      "Step 3530 of 9673: loss 1.7904729843139648\n",
      "Step 3540 of 9673: loss 1.7268937826156616\n",
      "Step 3550 of 9673: loss 1.7242674827575684\n",
      "Step 3560 of 9673: loss 1.732521414756775\n",
      "Step 3570 of 9673: loss 1.742629885673523\n",
      "Step 3580 of 9673: loss 1.812687873840332\n",
      "Step 3590 of 9673: loss 1.761141061782837\n",
      "Step 3600 of 9673: loss 1.7693012952804565\n",
      "validation: loss 1.759872634386279\n",
      "Step 3610 of 9673: loss 1.7582296133041382\n",
      "Step 3620 of 9673: loss 1.7400641441345215\n",
      "Step 3630 of 9673: loss 1.803391695022583\n",
      "Step 3640 of 9673: loss 1.7940349578857422\n",
      "Step 3650 of 9673: loss 1.7464560270309448\n",
      "Step 3660 of 9673: loss 1.8216181993484497\n",
      "Step 3670 of 9673: loss 1.8137226104736328\n",
      "Step 3680 of 9673: loss 1.7920714616775513\n",
      "Step 3690 of 9673: loss 1.7943754196166992\n",
      "Step 3700 of 9673: loss 1.7449452877044678\n",
      "validation: loss 1.7589936231829457\n",
      "Step 3710 of 9673: loss 1.7575480937957764\n",
      "Step 3720 of 9673: loss 1.7969000339508057\n",
      "Step 3730 of 9673: loss 1.8065381050109863\n",
      "Step 3740 of 9673: loss 1.7401862144470215\n",
      "Step 3750 of 9673: loss 1.7338851690292358\n",
      "Step 3760 of 9673: loss 1.7547353506088257\n",
      "Step 3770 of 9673: loss 1.7517589330673218\n",
      "Step 3780 of 9673: loss 1.7467319965362549\n",
      "Step 3790 of 9673: loss 1.816695213317871\n",
      "Step 3800 of 9673: loss 1.8165122270584106\n",
      "validation: loss 1.7588269550775744\n",
      "Step 3810 of 9673: loss 1.8445689678192139\n",
      "Step 3820 of 9673: loss 1.7392337322235107\n",
      "Step 3830 of 9673: loss 1.8159993886947632\n",
      "Step 3840 of 9673: loss 1.7890366315841675\n",
      "Step 3850 of 9673: loss 1.7992863655090332\n",
      "Step 3860 of 9673: loss 1.8532347679138184\n",
      "Step 3870 of 9673: loss 1.8463895320892334\n",
      "Step 3880 of 9673: loss 1.8446285724639893\n",
      "Step 3890 of 9673: loss 1.6998332738876343\n",
      "Step 3900 of 9673: loss 1.781944751739502\n",
      "validation: loss 1.7620793507271206\n",
      "Step 3910 of 9673: loss 1.7491306066513062\n",
      "Step 3920 of 9673: loss 1.7605948448181152\n",
      "Step 3930 of 9673: loss 1.7261806726455688\n",
      "Step 3940 of 9673: loss 1.8042621612548828\n",
      "Step 3950 of 9673: loss 1.8111974000930786\n",
      "Step 3960 of 9673: loss 1.8688139915466309\n",
      "Step 3970 of 9673: loss 1.786426305770874\n",
      "Step 3980 of 9673: loss 1.7636620998382568\n",
      "Step 3990 of 9673: loss 1.7890247106552124\n",
      "Step 4000 of 9673: loss 1.7292808294296265\n",
      "validation: loss 1.7594778144482486\n",
      "Step 4010 of 9673: loss 1.7541239261627197\n",
      "Step 4020 of 9673: loss 1.7549530267715454\n",
      "Step 4030 of 9673: loss 1.7999131679534912\n",
      "Step 4040 of 9673: loss 1.6960082054138184\n",
      "Step 4050 of 9673: loss 1.7851779460906982\n",
      "Step 4060 of 9673: loss 1.748545527458191\n",
      "Step 4070 of 9673: loss 1.736851692199707\n",
      "Step 4080 of 9673: loss 1.7536473274230957\n",
      "Step 4090 of 9673: loss 1.7578411102294922\n",
      "Step 4100 of 9673: loss 1.7410383224487305\n",
      "validation: loss 1.7587946038885214\n",
      "Step 4110 of 9673: loss 1.7257030010223389\n",
      "Step 4120 of 9673: loss 1.8159780502319336\n",
      "Step 4130 of 9673: loss 1.7361048460006714\n",
      "Step 4140 of 9673: loss 1.7219682931900024\n",
      "Step 4150 of 9673: loss 1.7974804639816284\n",
      "Step 4160 of 9673: loss 1.7309298515319824\n",
      "Step 4170 of 9673: loss 1.7893775701522827\n",
      "Step 4180 of 9673: loss 1.7579560279846191\n",
      "Step 4190 of 9673: loss 1.8123258352279663\n",
      "Step 4200 of 9673: loss 1.8117785453796387\n",
      "validation: loss 1.7588336049895925\n",
      "Step 4210 of 9673: loss 1.8379676342010498\n",
      "Step 4220 of 9673: loss 1.6996504068374634\n",
      "Step 4230 of 9673: loss 1.8098891973495483\n",
      "Step 4240 of 9673: loss 1.7356196641921997\n",
      "Step 4250 of 9673: loss 1.7486900091171265\n",
      "Step 4260 of 9673: loss 1.7698969841003418\n",
      "Step 4270 of 9673: loss 1.8334310054779053\n",
      "Step 4280 of 9673: loss 1.785012125968933\n",
      "Step 4290 of 9673: loss 1.721126675605774\n",
      "Step 4300 of 9673: loss 1.7418818473815918\n",
      "validation: loss 1.759240780909037\n",
      "Step 4310 of 9673: loss 1.7987538576126099\n",
      "Step 4320 of 9673: loss 1.8363096714019775\n",
      "Step 4330 of 9673: loss 1.7785508632659912\n",
      "Step 4340 of 9673: loss 1.748366355895996\n",
      "Step 4350 of 9673: loss 1.8297110795974731\n",
      "Step 4360 of 9673: loss 1.7995519638061523\n",
      "Step 4370 of 9673: loss 1.7613840103149414\n",
      "Step 4380 of 9673: loss 1.836208701133728\n",
      "Step 4390 of 9673: loss 1.7556689977645874\n",
      "Step 4400 of 9673: loss 1.7504109144210815\n",
      "validation: loss 1.7582458478888285\n",
      "Step 4410 of 9673: loss 1.7943342924118042\n",
      "Step 4420 of 9673: loss 1.761229395866394\n",
      "Step 4430 of 9673: loss 1.7776981592178345\n",
      "Step 4440 of 9673: loss 1.8691035509109497\n",
      "Step 4450 of 9673: loss 1.7497806549072266\n",
      "Step 4460 of 9673: loss 1.8242294788360596\n",
      "Step 4470 of 9673: loss 1.7385464906692505\n",
      "Step 4480 of 9673: loss 1.707087516784668\n",
      "Step 4490 of 9673: loss 1.7808871269226074\n",
      "Step 4500 of 9673: loss 1.7811099290847778\n",
      "validation: loss 1.7598168518125397\n",
      "Step 4510 of 9673: loss 1.811509370803833\n",
      "Step 4520 of 9673: loss 1.767716646194458\n",
      "Step 4530 of 9673: loss 1.8083117008209229\n",
      "Step 4540 of 9673: loss 1.734902262687683\n",
      "Step 4550 of 9673: loss 1.7434163093566895\n",
      "Step 4560 of 9673: loss 1.787036657333374\n",
      "Step 4570 of 9673: loss 1.7652968168258667\n",
      "Step 4580 of 9673: loss 1.8102532625198364\n",
      "Step 4590 of 9673: loss 1.8257296085357666\n",
      "Step 4600 of 9673: loss 1.8416552543640137\n",
      "validation: loss 1.7583080456428921\n",
      "Step 4610 of 9673: loss 1.7278103828430176\n",
      "Step 4620 of 9673: loss 1.7815146446228027\n",
      "Step 4630 of 9673: loss 1.813429355621338\n",
      "Step 4640 of 9673: loss 1.7953929901123047\n",
      "Step 4650 of 9673: loss 1.7578051090240479\n",
      "Step 4660 of 9673: loss 1.7223626375198364\n",
      "Step 4670 of 9673: loss 1.7281986474990845\n",
      "Step 4680 of 9673: loss 1.7362322807312012\n",
      "Step 4690 of 9673: loss 1.8397258520126343\n",
      "Step 4700 of 9673: loss 1.7919868230819702\n",
      "validation: loss 1.7589179596950097\n",
      "Step 4710 of 9673: loss 1.8208569288253784\n",
      "Step 4720 of 9673: loss 1.689700961112976\n",
      "Step 4730 of 9673: loss 1.7736179828643799\n",
      "Step 4740 of 9673: loss 1.7351981401443481\n",
      "Step 4750 of 9673: loss 1.7463536262512207\n",
      "Step 4760 of 9673: loss 1.7331552505493164\n",
      "Step 4770 of 9673: loss 1.7553396224975586\n",
      "Step 4780 of 9673: loss 1.7941793203353882\n",
      "Step 4790 of 9673: loss 1.7602527141571045\n",
      "Step 4800 of 9673: loss 1.714761734008789\n",
      "validation: loss 1.7580501725993205\n",
      "Step 4810 of 9673: loss 1.7316642999649048\n",
      "Step 4820 of 9673: loss 1.7859938144683838\n",
      "Step 4830 of 9673: loss 1.7926424741744995\n",
      "Step 4840 of 9673: loss 1.7520020008087158\n",
      "Step 4850 of 9673: loss 1.793634295463562\n",
      "Step 4860 of 9673: loss 1.766972541809082\n",
      "Step 4870 of 9673: loss 1.8116625547409058\n",
      "Step 4880 of 9673: loss 1.7378334999084473\n",
      "Step 4890 of 9673: loss 1.7912489175796509\n",
      "Step 4900 of 9673: loss 1.7689971923828125\n",
      "validation: loss 1.7585123261225593\n",
      "Step 4910 of 9673: loss 1.694064736366272\n",
      "Step 4920 of 9673: loss 1.7581535577774048\n",
      "Step 4930 of 9673: loss 1.803839921951294\n",
      "Step 4940 of 9673: loss 1.7650964260101318\n",
      "Step 4950 of 9673: loss 1.7116652727127075\n",
      "Step 4960 of 9673: loss 1.7711400985717773\n",
      "Step 4970 of 9673: loss 1.721210241317749\n",
      "Step 4980 of 9673: loss 1.7082574367523193\n",
      "Step 4990 of 9673: loss 1.7561804056167603\n",
      "Step 5000 of 9673: loss 1.7280985116958618\n",
      "validation: loss 1.7575711663236324\n",
      "Step 5010 of 9673: loss 1.7958325147628784\n",
      "Step 5020 of 9673: loss 1.7237976789474487\n",
      "Step 5030 of 9673: loss 1.768581748008728\n",
      "Step 5040 of 9673: loss 1.760724425315857\n",
      "Step 5050 of 9673: loss 1.7750333547592163\n",
      "Step 5060 of 9673: loss 1.7168179750442505\n",
      "Step 5070 of 9673: loss 1.8097689151763916\n",
      "Step 5080 of 9673: loss 1.7597664594650269\n",
      "Step 5090 of 9673: loss 1.7572487592697144\n",
      "Step 5100 of 9673: loss 1.8246010541915894\n",
      "validation: loss 1.7571902692932444\n",
      "Step 5110 of 9673: loss 1.789113998413086\n",
      "Step 5120 of 9673: loss 1.8216241598129272\n",
      "Step 5130 of 9673: loss 1.7689958810806274\n",
      "Step 5140 of 9673: loss 1.7549207210540771\n",
      "Step 5150 of 9673: loss 1.6935808658599854\n",
      "Step 5160 of 9673: loss 1.7771457433700562\n",
      "Step 5170 of 9673: loss 1.762152075767517\n",
      "Step 5180 of 9673: loss 1.8139467239379883\n",
      "Step 5190 of 9673: loss 1.728366732597351\n",
      "Step 5200 of 9673: loss 1.7461729049682617\n",
      "validation: loss 1.7584583648701304\n",
      "Step 5210 of 9673: loss 1.7266420125961304\n",
      "Step 5220 of 9673: loss 1.7580446004867554\n",
      "Step 5230 of 9673: loss 1.7195895910263062\n",
      "Step 5240 of 9673: loss 1.7334522008895874\n",
      "Step 5250 of 9673: loss 1.7551206350326538\n",
      "Step 5260 of 9673: loss 1.7586075067520142\n",
      "Step 5270 of 9673: loss 1.8169260025024414\n",
      "Step 5280 of 9673: loss 1.7145717144012451\n",
      "Step 5290 of 9673: loss 1.7657431364059448\n",
      "Step 5300 of 9673: loss 1.8051420450210571\n",
      "validation: loss 1.757774386209311\n",
      "Step 5310 of 9673: loss 1.7413766384124756\n",
      "Step 5320 of 9673: loss 1.783904790878296\n",
      "Step 5330 of 9673: loss 1.7469125986099243\n",
      "Step 5340 of 9673: loss 1.7421611547470093\n",
      "Step 5350 of 9673: loss 1.789499282836914\n",
      "Step 5360 of 9673: loss 1.7787773609161377\n",
      "Step 5370 of 9673: loss 1.841484546661377\n",
      "Step 5380 of 9673: loss 1.75717294216156\n",
      "Step 5390 of 9673: loss 1.7562077045440674\n",
      "Step 5400 of 9673: loss 1.775429606437683\n",
      "validation: loss 1.7570568925326633\n",
      "Step 5410 of 9673: loss 1.775217056274414\n",
      "Step 5420 of 9673: loss 1.7798117399215698\n",
      "Step 5430 of 9673: loss 1.7861899137496948\n",
      "Step 5440 of 9673: loss 1.71341872215271\n",
      "Step 5450 of 9673: loss 1.7907472848892212\n",
      "Step 5460 of 9673: loss 1.7384546995162964\n",
      "Step 5470 of 9673: loss 1.7541844844818115\n",
      "Step 5480 of 9673: loss 1.7883762121200562\n",
      "Step 5490 of 9673: loss 1.7781325578689575\n",
      "Step 5500 of 9673: loss 1.8197903633117676\n",
      "validation: loss 1.7564872232909055\n",
      "Step 5510 of 9673: loss 1.7930212020874023\n",
      "Step 5520 of 9673: loss 1.7677348852157593\n",
      "Step 5530 of 9673: loss 1.6693669557571411\n",
      "Step 5540 of 9673: loss 1.733436107635498\n",
      "Step 5550 of 9673: loss 1.7553224563598633\n",
      "Step 5560 of 9673: loss 1.7947368621826172\n",
      "Step 5570 of 9673: loss 1.7474863529205322\n",
      "Step 5580 of 9673: loss 1.7658063173294067\n",
      "Step 5590 of 9673: loss 1.7594908475875854\n",
      "Step 5600 of 9673: loss 1.7434662580490112\n",
      "validation: loss 1.7574317701084097\n",
      "Step 5610 of 9673: loss 1.7429394721984863\n",
      "Step 5620 of 9673: loss 1.7261457443237305\n",
      "Step 5630 of 9673: loss 1.7299582958221436\n",
      "Step 5640 of 9673: loss 1.7849119901657104\n",
      "Step 5650 of 9673: loss 1.8122758865356445\n",
      "Step 5660 of 9673: loss 1.7829328775405884\n",
      "Step 5670 of 9673: loss 1.7693315744400024\n",
      "Step 5680 of 9673: loss 1.7766164541244507\n",
      "Step 5690 of 9673: loss 1.7557051181793213\n",
      "Step 5700 of 9673: loss 1.7537978887557983\n",
      "validation: loss 1.7571717390080088\n",
      "Step 5710 of 9673: loss 1.70815908908844\n",
      "Step 5720 of 9673: loss 1.7321752309799194\n",
      "Step 5730 of 9673: loss 1.7493332624435425\n",
      "Step 5740 of 9673: loss 1.8544994592666626\n",
      "Step 5750 of 9673: loss 1.758240818977356\n",
      "Step 5760 of 9673: loss 1.70815908908844\n",
      "Step 5770 of 9673: loss 1.7560570240020752\n",
      "Step 5780 of 9673: loss 1.7398420572280884\n",
      "Step 5790 of 9673: loss 1.7667452096939087\n",
      "Step 5800 of 9673: loss 1.7915616035461426\n",
      "validation: loss 1.7567034849186534\n",
      "Step 5810 of 9673: loss 1.7592434883117676\n",
      "Step 5820 of 9673: loss 1.7253059148788452\n",
      "Step 5830 of 9673: loss 1.7282590866088867\n",
      "Step 5840 of 9673: loss 1.7393556833267212\n",
      "Step 5850 of 9673: loss 1.7757309675216675\n",
      "Step 5860 of 9673: loss 1.8028925657272339\n",
      "Step 5870 of 9673: loss 1.73356294631958\n",
      "Step 5880 of 9673: loss 1.7453467845916748\n",
      "Step 5890 of 9673: loss 1.7490922212600708\n",
      "Step 5900 of 9673: loss 1.7603142261505127\n",
      "validation: loss 1.756330352468589\n",
      "Step 5910 of 9673: loss 1.70302152633667\n",
      "Step 5920 of 9673: loss 1.7404956817626953\n",
      "Step 5930 of 9673: loss 1.7328205108642578\n",
      "Step 5940 of 9673: loss 1.7663813829421997\n",
      "Step 5950 of 9673: loss 1.7910536527633667\n",
      "Step 5960 of 9673: loss 1.829834222793579\n",
      "Step 5970 of 9673: loss 1.7689123153686523\n",
      "Step 5980 of 9673: loss 1.7960649728775024\n",
      "Step 5990 of 9673: loss 1.7323867082595825\n",
      "Step 6000 of 9673: loss 1.720091700553894\n",
      "validation: loss 1.7568268677623002\n",
      "Step 6010 of 9673: loss 1.738970398902893\n",
      "Step 6020 of 9673: loss 1.7803163528442383\n",
      "Step 6030 of 9673: loss 1.7544028759002686\n",
      "Step 6040 of 9673: loss 1.7560315132141113\n",
      "Step 6050 of 9673: loss 1.7665982246398926\n",
      "Step 6060 of 9673: loss 1.7747559547424316\n",
      "Step 6070 of 9673: loss 1.7777127027511597\n",
      "Step 6080 of 9673: loss 1.7709170579910278\n",
      "Step 6090 of 9673: loss 1.768176555633545\n",
      "Step 6100 of 9673: loss 1.7512727975845337\n",
      "validation: loss 1.755934560421816\n",
      "Step 6110 of 9673: loss 1.736501932144165\n",
      "Step 6120 of 9673: loss 1.8254972696304321\n",
      "Step 6130 of 9673: loss 1.7505310773849487\n",
      "Step 6140 of 9673: loss 1.7749511003494263\n",
      "Step 6150 of 9673: loss 1.756501317024231\n",
      "Step 6160 of 9673: loss 1.8037502765655518\n",
      "Step 6170 of 9673: loss 1.7841325998306274\n",
      "Step 6180 of 9673: loss 1.8029862642288208\n",
      "Step 6190 of 9673: loss 1.7622116804122925\n",
      "Step 6200 of 9673: loss 1.7006630897521973\n",
      "validation: loss 1.7560921858266456\n",
      "Step 6210 of 9673: loss 1.7972220182418823\n",
      "Step 6220 of 9673: loss 1.7553200721740723\n",
      "Step 6230 of 9673: loss 1.802869200706482\n",
      "Step 6240 of 9673: loss 1.7868393659591675\n",
      "Step 6250 of 9673: loss 1.7420570850372314\n",
      "Step 6260 of 9673: loss 1.770199179649353\n",
      "Step 6270 of 9673: loss 1.7267398834228516\n",
      "Step 6280 of 9673: loss 1.8124096393585205\n",
      "Step 6290 of 9673: loss 1.7166531085968018\n",
      "Step 6300 of 9673: loss 1.7085541486740112\n",
      "validation: loss 1.7561614710031097\n",
      "Step 6310 of 9673: loss 1.7867345809936523\n",
      "Step 6320 of 9673: loss 1.7357007265090942\n",
      "Step 6330 of 9673: loss 1.7613134384155273\n",
      "Step 6340 of 9673: loss 1.8063143491744995\n",
      "Step 6350 of 9673: loss 1.770767092704773\n",
      "Step 6360 of 9673: loss 1.7470107078552246\n",
      "Step 6370 of 9673: loss 1.7940127849578857\n",
      "Step 6380 of 9673: loss 1.7878168821334839\n",
      "Step 6390 of 9673: loss 1.7678091526031494\n",
      "Step 6400 of 9673: loss 1.7038482427597046\n",
      "validation: loss 1.7549893057223447\n",
      "Step 6410 of 9673: loss 1.6380739212036133\n",
      "Step 6420 of 9673: loss 1.7291324138641357\n",
      "Step 6430 of 9673: loss 1.8248753547668457\n",
      "Step 6440 of 9673: loss 1.7954100370407104\n",
      "Step 6450 of 9673: loss 1.8386141061782837\n",
      "Step 6460 of 9673: loss 1.793003797531128\n",
      "Step 6470 of 9673: loss 1.7495512962341309\n",
      "Step 6480 of 9673: loss 1.7582463026046753\n",
      "Step 6490 of 9673: loss 1.8230066299438477\n",
      "Step 6500 of 9673: loss 1.796921730041504\n",
      "validation: loss 1.755321263038006\n",
      "Step 6510 of 9673: loss 1.7926826477050781\n",
      "Step 6520 of 9673: loss 1.7264845371246338\n",
      "Step 6530 of 9673: loss 1.7155119180679321\n",
      "Step 6540 of 9673: loss 1.809630036354065\n",
      "Step 6550 of 9673: loss 1.7922253608703613\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for epoch in range(hyper.ep):\n",
    "    for batch in trainl:\n",
    "        seq = batch['input_ids'].to(device)\n",
    "        out = storytell(seq)\n",
    "        loss = lossf(t.flatten(out, end_dim=1), t.flatten(t.roll(seq, -1)))\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f'Step {step} of {len(trainl)}: loss {loss.item()}')\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            t.save(storytell.state_dict(), f'story_model_{step}.pt')\n",
    "            t.save(optim.state_dict(), f'story_optim_{step}.pt')\n",
    "        \n",
    "            with t.no_grad():\n",
    "                tloss = 0\n",
    "                steps = 0\n",
    "                storytell.eval()\n",
    "                for batch in validl:\n",
    "                    seq = batch['input_ids'].to(device)\n",
    "                    out = storytell(seq)\n",
    "                    tloss += lossf(t.flatten(out, end_dim=1), t.flatten(t.roll(seq, -1))).item()\n",
    "                    steps += 1\n",
    "                print(f'validation: loss {tloss/steps}')\n",
    "                storytell.train()\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# idx = random.randint(0, len(valid) - 1)\n",
    "# print(idx)\n",
    "\n",
    "# print(tok.decode(valid['input_ids'][idx]))\n",
    "\n",
    "# print('model gen:')\n",
    "# print(tok.decode(storytell(valid['input_ids'][idx].unsqueeze(0).to(t.long)).argmax(dim=-1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  dialogue\n",
      "Words:  prevent, worry, calm\n",
      "Story:  once upon a time, in a calm little town, there lived a boy named tom. tom had a big worry. he was scared of the dark. every night, when it was time to sleep, tom would cry.\n",
      "\n",
      " one day\n",
      "torch.Size([1, 60])\n",
      ".\n",
      "\n",
      " one day, \"wow!\" tom was afraid and have to his mom's okay, and said, \"ok, \"hello, there was a way, he would be careful with her friend, lily's mom and the butterfly inside the dog kept walking through the train arrived at the yard.\n",
      "\n",
      "Words:  attach, \"why are you!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " tom says, lived in the empty\n",
      "\n",
      "\n",
      "Story: \n",
      "\n",
      "Story:  once upon a little girl named lily finds a big hug their mouths.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Story:  once upon a time, but she got really fun. he couldn't worry, but then, the little girl named spot and a big dog was a\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, model, tokenizer, temperature=1.0, max_len=512):\n",
    "    model.eval()\n",
    "    with t.no_grad():\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt')[:, :-1]\n",
    "        print(input_ids.shape)\n",
    "        cur_len = input_ids.shape[1]\n",
    "        while cur_len < max_len:\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs[0][-1, :] / temperature\n",
    "            # next_token_logits[:5] = -float('inf')\n",
    "            next_token_id = t.multinomial(t.softmax(next_token_logits, dim=-1), num_samples=1).unsqueeze(-1)\n",
    "            input_ids = t.cat([input_ids, next_token_id], dim=1)\n",
    "            cur_len += 1\n",
    "            if next_token_id[0][0] == tokenizer.eos_token_id:\n",
    "                break\n",
    "        return tokenizer.decode(input_ids.squeeze()[55:], skip_special_tokens=False)\n",
    "    \n",
    "idx = random.randint(0, len(valid) - 1)\n",
    "prompt = tok.decode(valid['input_ids'][idx][:60])\n",
    "print(prompt)\n",
    "print(generate_text(prompt, storytell, tok, temperature=.7, max_len=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
